{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b983d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f714309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: [-0.04623526 -0.03684296  0.03551175 -0.00946995]\n",
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "state, _ = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51a7210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_features=4, num_actions=2, hidden_features=128) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=hidden_features)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_features, out_features=hidden_features * 2)\n",
    "        self.fc3 = nn.Linear(in_features=hidden_features * 2, out_features=num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e5696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_buffer_size=10000, batch_size=16):\n",
    "        self.buffer = deque(maxlen=max_buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add_sample(self, element: tuple):\n",
    "        self.buffer.append(element)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        return random.sample(self.buffer, k=self.batch_size) if len(self.buffer) > self.batch_size else list(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44d3d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BUFFER_SIZE = 1000\n",
    "N_TRAINING_STEPS = 20000\n",
    "N_START_LEARNING = 1000\n",
    "TARGET_UPDATE_FREQUENCY = 50\n",
    "LEARNING_RATE = 5e-4\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "EPSILON = 0.95\n",
    "DECAY = 0.9\n",
    "MIN_EPSILON = 0.01\n",
    "epsilon = 1.0\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "q_net = QNetwork(num_features=state_size, num_actions=action_size)\n",
    "target_q_net = QNetwork(num_features=state_size, num_actions=action_size)\n",
    "buffer = ReplayBuffer(batch_size=BATCH_SIZE, max_buffer_size=N_BUFFER_SIZE)\n",
    "\n",
    "target_q_net.load_state_dict(q_net.state_dict())\n",
    "optimizer = torch.optim.Adam(q_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25df0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, q_net, action_size, epsilon=0.5):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    else:\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = q_net(state_tensor)\n",
    "            return torch.argmax(logits, dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3787fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer filled so far: 1\n",
      "Buffer filled so far: 101\n",
      "Buffer filled so far: 201\n",
      "Buffer filled so far: 301\n",
      "Buffer filled so far: 401\n",
      "Buffer filled so far: 501\n",
      "Buffer filled so far: 601\n",
      "Buffer filled so far: 701\n",
      "Buffer filled so far: 801\n",
      "Buffer filled so far: 901\n",
      "[STEP]: 1050, [LOSS]: 0.0105, [EPISODE]: 46, [REWARD]: 13.0\n",
      "[STEP]: 1100, [LOSS]: 0.1207, [EPISODE]: 49, [REWARD]: 15.0\n",
      "[STEP]: 1150, [LOSS]: 0.2161, [EPISODE]: 54, [REWARD]: 12.0\n",
      "[STEP]: 1200, [LOSS]: 0.2840, [EPISODE]: 59, [REWARD]: 8.0\n",
      "[STEP]: 1250, [LOSS]: 1.0511, [EPISODE]: 63, [REWARD]: 20.0\n",
      "[STEP]: 1300, [LOSS]: 0.4367, [EPISODE]: 65, [REWARD]: 33.0\n",
      "[STEP]: 1350, [LOSS]: 0.1517, [EPISODE]: 68, [REWARD]: 5.0\n",
      "[STEP]: 1400, [LOSS]: 6.2937, [EPISODE]: 73, [REWARD]: 8.0\n",
      "[STEP]: 1450, [LOSS]: 3.6024, [EPISODE]: 77, [REWARD]: 10.0\n",
      "[STEP]: 1500, [LOSS]: 0.3372, [EPISODE]: 81, [REWARD]: 23.0\n",
      "[STEP]: 1550, [LOSS]: 4.6899, [EPISODE]: 82, [REWARD]: 45.0\n",
      "[STEP]: 1600, [LOSS]: 1.2906, [EPISODE]: 83, [REWARD]: 38.0\n",
      "[STEP]: 1650, [LOSS]: 0.3430, [EPISODE]: 83, [REWARD]: 88.0\n",
      "[STEP]: 1700, [LOSS]: 9.1776, [EPISODE]: 83, [REWARD]: 138.0\n",
      "[STEP]: 1750, [LOSS]: 0.9400, [EPISODE]: 84, [REWARD]: 33.0\n",
      "[STEP]: 1800, [LOSS]: 2.3602, [EPISODE]: 85, [REWARD]: 5.0\n",
      "[STEP]: 1850, [LOSS]: 0.3749, [EPISODE]: 85, [REWARD]: 55.0\n",
      "[STEP]: 1900, [LOSS]: 2.3964, [EPISODE]: 86, [REWARD]: 31.0\n",
      "[STEP]: 1950, [LOSS]: 0.8476, [EPISODE]: 86, [REWARD]: 81.0\n",
      "[STEP]: 2000, [LOSS]: 0.1700, [EPISODE]: 88, [REWARD]: 2.0\n",
      "[STEP]: 2050, [LOSS]: 1.3559, [EPISODE]: 88, [REWARD]: 52.0\n",
      "[STEP]: 2100, [LOSS]: 18.6791, [EPISODE]: 89, [REWARD]: 36.0\n",
      "[STEP]: 2150, [LOSS]: 0.4478, [EPISODE]: 90, [REWARD]: 47.0\n",
      "[STEP]: 2200, [LOSS]: 18.9520, [EPISODE]: 91, [REWARD]: 5.0\n",
      "[STEP]: 2250, [LOSS]: 5.9435, [EPISODE]: 92, [REWARD]: 6.0\n",
      "[STEP]: 2300, [LOSS]: 0.2502, [EPISODE]: 92, [REWARD]: 56.0\n",
      "[STEP]: 2350, [LOSS]: 21.9689, [EPISODE]: 92, [REWARD]: 106.0\n",
      "[STEP]: 2400, [LOSS]: 7.1221, [EPISODE]: 93, [REWARD]: 24.0\n",
      "[STEP]: 2450, [LOSS]: 0.5896, [EPISODE]: 94, [REWARD]: 5.0\n",
      "[STEP]: 2500, [LOSS]: 43.5544, [EPISODE]: 95, [REWARD]: 1.0\n",
      "[STEP]: 2550, [LOSS]: 34.5411, [EPISODE]: 95, [REWARD]: 51.0\n",
      "[STEP]: 2600, [LOSS]: 0.0416, [EPISODE]: 95, [REWARD]: 101.0\n",
      "[STEP]: 2650, [LOSS]: 44.7067, [EPISODE]: 96, [REWARD]: 30.0\n",
      "[STEP]: 2700, [LOSS]: 51.5620, [EPISODE]: 97, [REWARD]: 33.0\n",
      "[STEP]: 2750, [LOSS]: 33.9893, [EPISODE]: 98, [REWARD]: 25.0\n",
      "[STEP]: 2800, [LOSS]: 21.5529, [EPISODE]: 99, [REWARD]: 22.0\n",
      "[STEP]: 2850, [LOSS]: 0.0637, [EPISODE]: 100, [REWARD]: 10.0\n",
      "[STEP]: 2900, [LOSS]: 0.0363, [EPISODE]: 100, [REWARD]: 60.0\n",
      "[STEP]: 2950, [LOSS]: 26.2407, [EPISODE]: 101, [REWARD]: 41.0\n",
      "[STEP]: 3000, [LOSS]: 3.7377, [EPISODE]: 103, [REWARD]: 14.0\n",
      "[STEP]: 3050, [LOSS]: 0.2709, [EPISODE]: 104, [REWARD]: 11.0\n",
      "[STEP]: 3100, [LOSS]: 37.8080, [EPISODE]: 105, [REWARD]: 13.0\n",
      "[STEP]: 3150, [LOSS]: 0.1047, [EPISODE]: 105, [REWARD]: 63.0\n",
      "[STEP]: 3200, [LOSS]: 0.3896, [EPISODE]: 106, [REWARD]: 23.0\n",
      "[STEP]: 3250, [LOSS]: 0.2886, [EPISODE]: 106, [REWARD]: 73.0\n",
      "[STEP]: 3300, [LOSS]: 0.0910, [EPISODE]: 107, [REWARD]: 34.0\n",
      "[STEP]: 3350, [LOSS]: 0.2453, [EPISODE]: 107, [REWARD]: 84.0\n",
      "[STEP]: 3400, [LOSS]: 0.1846, [EPISODE]: 108, [REWARD]: 4.0\n",
      "[STEP]: 3450, [LOSS]: 0.2089, [EPISODE]: 108, [REWARD]: 54.0\n",
      "[STEP]: 3500, [LOSS]: 37.4745, [EPISODE]: 108, [REWARD]: 104.0\n",
      "[STEP]: 3550, [LOSS]: 0.3228, [EPISODE]: 108, [REWARD]: 154.0\n",
      "[STEP]: 3600, [LOSS]: 0.1776, [EPISODE]: 109, [REWARD]: 44.0\n",
      "[STEP]: 3650, [LOSS]: 0.1197, [EPISODE]: 109, [REWARD]: 94.0\n",
      "[STEP]: 3700, [LOSS]: 0.0458, [EPISODE]: 109, [REWARD]: 144.0\n",
      "[STEP]: 3750, [LOSS]: 0.4603, [EPISODE]: 110, [REWARD]: 31.0\n",
      "[STEP]: 3800, [LOSS]: 0.0622, [EPISODE]: 110, [REWARD]: 81.0\n",
      "[STEP]: 3850, [LOSS]: 0.1595, [EPISODE]: 111, [REWARD]: 6.0\n",
      "[STEP]: 3900, [LOSS]: 0.1352, [EPISODE]: 111, [REWARD]: 56.0\n",
      "[STEP]: 3950, [LOSS]: 0.2276, [EPISODE]: 111, [REWARD]: 106.0\n",
      "[STEP]: 4000, [LOSS]: 1.3394, [EPISODE]: 112, [REWARD]: 35.0\n",
      "[STEP]: 4050, [LOSS]: 0.1122, [EPISODE]: 112, [REWARD]: 85.0\n",
      "[STEP]: 4100, [LOSS]: 0.1572, [EPISODE]: 113, [REWARD]: 8.0\n",
      "[STEP]: 4150, [LOSS]: 1.4352, [EPISODE]: 113, [REWARD]: 58.0\n",
      "[STEP]: 4200, [LOSS]: 0.2839, [EPISODE]: 113, [REWARD]: 108.0\n",
      "[STEP]: 4250, [LOSS]: 2.3842, [EPISODE]: 114, [REWARD]: 38.0\n",
      "[STEP]: 4300, [LOSS]: 0.1201, [EPISODE]: 114, [REWARD]: 88.0\n",
      "[STEP]: 4350, [LOSS]: 0.1427, [EPISODE]: 115, [REWARD]: 13.0\n",
      "[STEP]: 4400, [LOSS]: 0.1769, [EPISODE]: 115, [REWARD]: 63.0\n",
      "[STEP]: 4450, [LOSS]: 0.0477, [EPISODE]: 115, [REWARD]: 113.0\n",
      "[STEP]: 4500, [LOSS]: 0.0699, [EPISODE]: 116, [REWARD]: 22.0\n",
      "[STEP]: 4550, [LOSS]: 0.0512, [EPISODE]: 116, [REWARD]: 72.0\n",
      "[STEP]: 4600, [LOSS]: 0.0735, [EPISODE]: 116, [REWARD]: 122.0\n",
      "[STEP]: 4650, [LOSS]: 0.1008, [EPISODE]: 116, [REWARD]: 172.0\n",
      "[STEP]: 4700, [LOSS]: 0.0700, [EPISODE]: 117, [REWARD]: 46.0\n",
      "[STEP]: 4750, [LOSS]: 0.0593, [EPISODE]: 117, [REWARD]: 96.0\n",
      "[STEP]: 4800, [LOSS]: 0.2204, [EPISODE]: 117, [REWARD]: 146.0\n",
      "[STEP]: 4850, [LOSS]: 0.0753, [EPISODE]: 118, [REWARD]: 9.0\n",
      "[STEP]: 4900, [LOSS]: 7.8736, [EPISODE]: 118, [REWARD]: 59.0\n",
      "[STEP]: 4950, [LOSS]: 0.0673, [EPISODE]: 118, [REWARD]: 109.0\n",
      "[STEP]: 5000, [LOSS]: 0.0379, [EPISODE]: 118, [REWARD]: 159.0\n",
      "[STEP]: 5050, [LOSS]: 0.2848, [EPISODE]: 119, [REWARD]: 42.0\n",
      "[STEP]: 5100, [LOSS]: 0.2527, [EPISODE]: 119, [REWARD]: 92.0\n",
      "[STEP]: 5150, [LOSS]: 0.0374, [EPISODE]: 119, [REWARD]: 142.0\n",
      "[STEP]: 5200, [LOSS]: 0.0529, [EPISODE]: 120, [REWARD]: 9.0\n",
      "[STEP]: 5250, [LOSS]: 0.0229, [EPISODE]: 120, [REWARD]: 59.0\n",
      "[STEP]: 5300, [LOSS]: 0.0488, [EPISODE]: 120, [REWARD]: 109.0\n",
      "[STEP]: 5350, [LOSS]: 0.0778, [EPISODE]: 120, [REWARD]: 159.0\n",
      "[STEP]: 5400, [LOSS]: 1.2398, [EPISODE]: 121, [REWARD]: 26.0\n",
      "[STEP]: 5450, [LOSS]: 0.0422, [EPISODE]: 121, [REWARD]: 76.0\n",
      "[STEP]: 5500, [LOSS]: 0.0248, [EPISODE]: 121, [REWARD]: 126.0\n",
      "[STEP]: 5550, [LOSS]: 0.0307, [EPISODE]: 121, [REWARD]: 176.0\n",
      "[STEP]: 5600, [LOSS]: 0.0148, [EPISODE]: 122, [REWARD]: 8.0\n",
      "[STEP]: 5650, [LOSS]: 0.0256, [EPISODE]: 122, [REWARD]: 58.0\n",
      "[STEP]: 5700, [LOSS]: 0.0875, [EPISODE]: 122, [REWARD]: 108.0\n",
      "[STEP]: 5750, [LOSS]: 0.0834, [EPISODE]: 122, [REWARD]: 158.0\n",
      "[STEP]: 5800, [LOSS]: 0.0183, [EPISODE]: 122, [REWARD]: 208.0\n",
      "[STEP]: 5850, [LOSS]: 0.0438, [EPISODE]: 123, [REWARD]: 33.0\n",
      "[STEP]: 5900, [LOSS]: 0.0335, [EPISODE]: 123, [REWARD]: 83.0\n",
      "[STEP]: 5950, [LOSS]: 17.1452, [EPISODE]: 123, [REWARD]: 133.0\n",
      "[STEP]: 6000, [LOSS]: 17.4084, [EPISODE]: 123, [REWARD]: 183.0\n",
      "[STEP]: 6050, [LOSS]: 0.0325, [EPISODE]: 123, [REWARD]: 233.0\n",
      "[STEP]: 6100, [LOSS]: 0.0401, [EPISODE]: 124, [REWARD]: 11.0\n",
      "[STEP]: 6150, [LOSS]: 0.0969, [EPISODE]: 124, [REWARD]: 61.0\n",
      "[STEP]: 6200, [LOSS]: 0.0385, [EPISODE]: 124, [REWARD]: 111.0\n",
      "[STEP]: 6250, [LOSS]: 0.0696, [EPISODE]: 124, [REWARD]: 161.0\n",
      "[STEP]: 6300, [LOSS]: 0.0288, [EPISODE]: 124, [REWARD]: 211.0\n",
      "[STEP]: 6350, [LOSS]: 0.0702, [EPISODE]: 124, [REWARD]: 261.0\n",
      "[STEP]: 6400, [LOSS]: 0.0311, [EPISODE]: 124, [REWARD]: 311.0\n",
      "[STEP]: 6450, [LOSS]: 0.0229, [EPISODE]: 124, [REWARD]: 361.0\n",
      "[STEP]: 6500, [LOSS]: 0.0232, [EPISODE]: 124, [REWARD]: 411.0\n",
      "[STEP]: 6550, [LOSS]: 0.0556, [EPISODE]: 125, [REWARD]: 39.0\n",
      "[STEP]: 6600, [LOSS]: 0.0130, [EPISODE]: 125, [REWARD]: 89.0\n",
      "[STEP]: 6650, [LOSS]: 0.0216, [EPISODE]: 125, [REWARD]: 139.0\n",
      "[STEP]: 6700, [LOSS]: 0.0220, [EPISODE]: 125, [REWARD]: 189.0\n",
      "[STEP]: 6750, [LOSS]: 0.0209, [EPISODE]: 125, [REWARD]: 239.0\n",
      "[STEP]: 6800, [LOSS]: 0.2398, [EPISODE]: 125, [REWARD]: 289.0\n",
      "[STEP]: 6850, [LOSS]: 0.0136, [EPISODE]: 125, [REWARD]: 339.0\n",
      "[STEP]: 6900, [LOSS]: 0.0285, [EPISODE]: 126, [REWARD]: 5.0\n",
      "[STEP]: 6950, [LOSS]: 18.2586, [EPISODE]: 126, [REWARD]: 55.0\n",
      "[STEP]: 7000, [LOSS]: 0.0148, [EPISODE]: 126, [REWARD]: 105.0\n",
      "[STEP]: 7050, [LOSS]: 0.0732, [EPISODE]: 126, [REWARD]: 155.0\n",
      "[STEP]: 7100, [LOSS]: 0.0453, [EPISODE]: 126, [REWARD]: 205.0\n",
      "[STEP]: 7150, [LOSS]: 0.0442, [EPISODE]: 126, [REWARD]: 255.0\n",
      "[STEP]: 7200, [LOSS]: 0.0184, [EPISODE]: 126, [REWARD]: 305.0\n",
      "[STEP]: 7250, [LOSS]: 0.0474, [EPISODE]: 126, [REWARD]: 355.0\n",
      "[STEP]: 7300, [LOSS]: 0.0969, [EPISODE]: 127, [REWARD]: 45.0\n",
      "[STEP]: 7350, [LOSS]: 0.0442, [EPISODE]: 127, [REWARD]: 95.0\n",
      "[STEP]: 7400, [LOSS]: 0.0455, [EPISODE]: 127, [REWARD]: 145.0\n",
      "[STEP]: 7450, [LOSS]: 20.6078, [EPISODE]: 128, [REWARD]: 24.0\n",
      "[STEP]: 7500, [LOSS]: 0.0655, [EPISODE]: 128, [REWARD]: 74.0\n",
      "[STEP]: 7550, [LOSS]: 19.1153, [EPISODE]: 129, [REWARD]: 5.0\n",
      "[STEP]: 7600, [LOSS]: 0.0091, [EPISODE]: 129, [REWARD]: 55.0\n",
      "[STEP]: 7650, [LOSS]: 0.0354, [EPISODE]: 129, [REWARD]: 105.0\n",
      "[STEP]: 7700, [LOSS]: 225.8317, [EPISODE]: 129, [REWARD]: 155.0\n",
      "[STEP]: 7750, [LOSS]: 0.0231, [EPISODE]: 130, [REWARD]: 48.0\n",
      "[STEP]: 7800, [LOSS]: 0.0184, [EPISODE]: 130, [REWARD]: 98.0\n",
      "[STEP]: 7850, [LOSS]: 0.0750, [EPISODE]: 130, [REWARD]: 148.0\n",
      "[STEP]: 7900, [LOSS]: 0.0227, [EPISODE]: 131, [REWARD]: 32.0\n",
      "[STEP]: 7950, [LOSS]: 187.2327, [EPISODE]: 131, [REWARD]: 82.0\n",
      "[STEP]: 8000, [LOSS]: 0.1338, [EPISODE]: 132, [REWARD]: 5.0\n",
      "[STEP]: 8050, [LOSS]: 181.5869, [EPISODE]: 132, [REWARD]: 55.0\n",
      "[STEP]: 8100, [LOSS]: 211.8277, [EPISODE]: 132, [REWARD]: 105.0\n",
      "[STEP]: 8150, [LOSS]: 2.0717, [EPISODE]: 133, [REWARD]: 19.0\n",
      "[STEP]: 8200, [LOSS]: 0.3785, [EPISODE]: 133, [REWARD]: 69.0\n",
      "[STEP]: 8250, [LOSS]: 0.0827, [EPISODE]: 134, [REWARD]: 32.0\n",
      "[STEP]: 8300, [LOSS]: 169.9085, [EPISODE]: 134, [REWARD]: 82.0\n",
      "[STEP]: 8350, [LOSS]: 1.8756, [EPISODE]: 134, [REWARD]: 132.0\n",
      "[STEP]: 8400, [LOSS]: 0.1727, [EPISODE]: 134, [REWARD]: 182.0\n",
      "[STEP]: 8450, [LOSS]: 0.1181, [EPISODE]: 134, [REWARD]: 232.0\n",
      "[STEP]: 8500, [LOSS]: 0.2077, [EPISODE]: 135, [REWARD]: 15.0\n",
      "[STEP]: 8550, [LOSS]: 0.2587, [EPISODE]: 135, [REWARD]: 65.0\n",
      "[STEP]: 8600, [LOSS]: 138.3817, [EPISODE]: 135, [REWARD]: 115.0\n",
      "[STEP]: 8650, [LOSS]: 0.1157, [EPISODE]: 135, [REWARD]: 165.0\n",
      "[STEP]: 8700, [LOSS]: 0.1242, [EPISODE]: 135, [REWARD]: 215.0\n",
      "[STEP]: 8750, [LOSS]: 0.2797, [EPISODE]: 135, [REWARD]: 265.0\n",
      "[STEP]: 8800, [LOSS]: 0.0951, [EPISODE]: 135, [REWARD]: 315.0\n",
      "[STEP]: 8850, [LOSS]: 0.1404, [EPISODE]: 136, [REWARD]: 5.0\n",
      "[STEP]: 8900, [LOSS]: 0.1051, [EPISODE]: 136, [REWARD]: 55.0\n",
      "[STEP]: 8950, [LOSS]: 0.0984, [EPISODE]: 136, [REWARD]: 105.0\n",
      "[STEP]: 9000, [LOSS]: 0.0552, [EPISODE]: 136, [REWARD]: 155.0\n",
      "[STEP]: 9050, [LOSS]: 168.5960, [EPISODE]: 137, [REWARD]: 23.0\n",
      "[STEP]: 9100, [LOSS]: 0.2736, [EPISODE]: 137, [REWARD]: 73.0\n",
      "[STEP]: 9150, [LOSS]: 172.8327, [EPISODE]: 137, [REWARD]: 123.0\n",
      "[STEP]: 9200, [LOSS]: 168.4150, [EPISODE]: 137, [REWARD]: 173.0\n",
      "[STEP]: 9250, [LOSS]: 0.0430, [EPISODE]: 138, [REWARD]: 15.0\n",
      "[STEP]: 9300, [LOSS]: 0.0244, [EPISODE]: 138, [REWARD]: 65.0\n",
      "[STEP]: 9350, [LOSS]: 0.1762, [EPISODE]: 138, [REWARD]: 115.0\n",
      "[STEP]: 9400, [LOSS]: 0.1419, [EPISODE]: 138, [REWARD]: 165.0\n",
      "[STEP]: 9450, [LOSS]: 144.1118, [EPISODE]: 139, [REWARD]: 25.0\n",
      "[STEP]: 9500, [LOSS]: 0.0189, [EPISODE]: 139, [REWARD]: 75.0\n",
      "[STEP]: 9550, [LOSS]: 0.0871, [EPISODE]: 139, [REWARD]: 125.0\n",
      "[STEP]: 9600, [LOSS]: 0.0466, [EPISODE]: 139, [REWARD]: 175.0\n",
      "[STEP]: 9650, [LOSS]: 0.0312, [EPISODE]: 140, [REWARD]: 22.0\n",
      "[STEP]: 9700, [LOSS]: 0.3145, [EPISODE]: 140, [REWARD]: 72.0\n",
      "[STEP]: 9750, [LOSS]: 0.2554, [EPISODE]: 140, [REWARD]: 122.0\n",
      "[STEP]: 9800, [LOSS]: 113.1393, [EPISODE]: 141, [REWARD]: 20.0\n",
      "[STEP]: 9850, [LOSS]: 0.0292, [EPISODE]: 141, [REWARD]: 70.0\n",
      "[STEP]: 9900, [LOSS]: 0.3655, [EPISODE]: 141, [REWARD]: 120.0\n",
      "[STEP]: 9950, [LOSS]: 0.1215, [EPISODE]: 141, [REWARD]: 170.0\n",
      "[STEP]: 10000, [LOSS]: 0.0580, [EPISODE]: 141, [REWARD]: 220.0\n",
      "[STEP]: 10050, [LOSS]: 0.5418, [EPISODE]: 142, [REWARD]: 35.0\n",
      "[STEP]: 10100, [LOSS]: 0.0247, [EPISODE]: 142, [REWARD]: 85.0\n",
      "[STEP]: 10150, [LOSS]: 0.0345, [EPISODE]: 142, [REWARD]: 135.0\n",
      "[STEP]: 10200, [LOSS]: 79.0609, [EPISODE]: 142, [REWARD]: 185.0\n",
      "[STEP]: 10250, [LOSS]: 0.2346, [EPISODE]: 143, [REWARD]: 39.0\n",
      "[STEP]: 10300, [LOSS]: 0.0293, [EPISODE]: 143, [REWARD]: 89.0\n",
      "[STEP]: 10350, [LOSS]: 0.1252, [EPISODE]: 143, [REWARD]: 139.0\n",
      "[STEP]: 10400, [LOSS]: 0.1203, [EPISODE]: 143, [REWARD]: 189.0\n",
      "[STEP]: 10450, [LOSS]: 0.0344, [EPISODE]: 143, [REWARD]: 239.0\n",
      "[STEP]: 10500, [LOSS]: 0.0845, [EPISODE]: 144, [REWARD]: 42.0\n",
      "[STEP]: 10550, [LOSS]: 0.0360, [EPISODE]: 144, [REWARD]: 92.0\n",
      "[STEP]: 10600, [LOSS]: 0.1246, [EPISODE]: 144, [REWARD]: 142.0\n",
      "[STEP]: 10650, [LOSS]: 0.0169, [EPISODE]: 144, [REWARD]: 192.0\n",
      "[STEP]: 10700, [LOSS]: 0.1490, [EPISODE]: 145, [REWARD]: 7.0\n",
      "[STEP]: 10750, [LOSS]: 0.1588, [EPISODE]: 145, [REWARD]: 57.0\n",
      "[STEP]: 10800, [LOSS]: 0.0736, [EPISODE]: 145, [REWARD]: 107.0\n",
      "[STEP]: 10850, [LOSS]: 0.0610, [EPISODE]: 145, [REWARD]: 157.0\n",
      "[STEP]: 10900, [LOSS]: 0.0241, [EPISODE]: 145, [REWARD]: 207.0\n",
      "[STEP]: 10950, [LOSS]: 11.6547, [EPISODE]: 145, [REWARD]: 257.0\n",
      "[STEP]: 11000, [LOSS]: 0.2510, [EPISODE]: 146, [REWARD]: 25.0\n",
      "[STEP]: 11050, [LOSS]: 0.0596, [EPISODE]: 146, [REWARD]: 75.0\n",
      "[STEP]: 11100, [LOSS]: 0.0415, [EPISODE]: 146, [REWARD]: 125.0\n",
      "[STEP]: 11150, [LOSS]: 0.0516, [EPISODE]: 146, [REWARD]: 175.0\n",
      "[STEP]: 11200, [LOSS]: 0.2469, [EPISODE]: 147, [REWARD]: 13.0\n",
      "[STEP]: 11250, [LOSS]: 0.1547, [EPISODE]: 147, [REWARD]: 63.0\n",
      "[STEP]: 11300, [LOSS]: 0.0229, [EPISODE]: 147, [REWARD]: 113.0\n",
      "[STEP]: 11350, [LOSS]: 0.3754, [EPISODE]: 147, [REWARD]: 163.0\n",
      "[STEP]: 11400, [LOSS]: 0.0869, [EPISODE]: 147, [REWARD]: 213.0\n",
      "[STEP]: 11450, [LOSS]: 0.0360, [EPISODE]: 147, [REWARD]: 263.0\n",
      "[STEP]: 11500, [LOSS]: 0.0202, [EPISODE]: 147, [REWARD]: 313.0\n",
      "[STEP]: 11550, [LOSS]: 0.0161, [EPISODE]: 147, [REWARD]: 363.0\n",
      "[STEP]: 11600, [LOSS]: 0.0611, [EPISODE]: 147, [REWARD]: 413.0\n",
      "[STEP]: 11650, [LOSS]: 0.0619, [EPISODE]: 147, [REWARD]: 463.0\n",
      "[STEP]: 11700, [LOSS]: 0.2030, [EPISODE]: 148, [REWARD]: 13.0\n",
      "[STEP]: 11750, [LOSS]: 0.0214, [EPISODE]: 148, [REWARD]: 63.0\n",
      "[STEP]: 11800, [LOSS]: 0.1287, [EPISODE]: 148, [REWARD]: 113.0\n",
      "[STEP]: 11850, [LOSS]: 1.6593, [EPISODE]: 148, [REWARD]: 163.0\n",
      "[STEP]: 11900, [LOSS]: 0.3118, [EPISODE]: 148, [REWARD]: 213.0\n",
      "[STEP]: 11950, [LOSS]: 0.0296, [EPISODE]: 148, [REWARD]: 263.0\n",
      "[STEP]: 12000, [LOSS]: 249.0926, [EPISODE]: 148, [REWARD]: 313.0\n",
      "[STEP]: 12050, [LOSS]: 0.0145, [EPISODE]: 148, [REWARD]: 363.0\n",
      "[STEP]: 12100, [LOSS]: 0.0287, [EPISODE]: 148, [REWARD]: 413.0\n",
      "[STEP]: 12150, [LOSS]: 0.0267, [EPISODE]: 148, [REWARD]: 463.0\n",
      "[STEP]: 12200, [LOSS]: 0.0522, [EPISODE]: 149, [REWARD]: 45.0\n",
      "[STEP]: 12250, [LOSS]: 0.0095, [EPISODE]: 149, [REWARD]: 95.0\n",
      "[STEP]: 12300, [LOSS]: 0.0239, [EPISODE]: 149, [REWARD]: 145.0\n",
      "[STEP]: 12350, [LOSS]: 0.0086, [EPISODE]: 149, [REWARD]: 195.0\n",
      "[STEP]: 12400, [LOSS]: 0.0148, [EPISODE]: 149, [REWARD]: 245.0\n",
      "[STEP]: 12450, [LOSS]: 0.0544, [EPISODE]: 149, [REWARD]: 295.0\n",
      "[STEP]: 12500, [LOSS]: 0.0157, [EPISODE]: 149, [REWARD]: 345.0\n",
      "[STEP]: 12550, [LOSS]: 0.0097, [EPISODE]: 149, [REWARD]: 395.0\n",
      "[STEP]: 12600, [LOSS]: 0.0883, [EPISODE]: 149, [REWARD]: 445.0\n",
      "[STEP]: 12650, [LOSS]: 0.0165, [EPISODE]: 149, [REWARD]: 495.0\n",
      "[STEP]: 12700, [LOSS]: 0.0235, [EPISODE]: 150, [REWARD]: 45.0\n",
      "[STEP]: 12750, [LOSS]: 0.0183, [EPISODE]: 150, [REWARD]: 95.0\n",
      "[STEP]: 12800, [LOSS]: 0.0131, [EPISODE]: 150, [REWARD]: 145.0\n",
      "[STEP]: 12850, [LOSS]: 0.1264, [EPISODE]: 150, [REWARD]: 195.0\n",
      "[STEP]: 12900, [LOSS]: 0.0379, [EPISODE]: 150, [REWARD]: 245.0\n",
      "[STEP]: 12950, [LOSS]: 0.0517, [EPISODE]: 150, [REWARD]: 295.0\n",
      "[STEP]: 13000, [LOSS]: 0.0198, [EPISODE]: 150, [REWARD]: 345.0\n",
      "[STEP]: 13050, [LOSS]: 0.0481, [EPISODE]: 150, [REWARD]: 395.0\n",
      "[STEP]: 13100, [LOSS]: 0.0840, [EPISODE]: 150, [REWARD]: 445.0\n",
      "[STEP]: 13150, [LOSS]: 0.0088, [EPISODE]: 150, [REWARD]: 495.0\n",
      "[STEP]: 13200, [LOSS]: 0.0350, [EPISODE]: 151, [REWARD]: 45.0\n",
      "[STEP]: 13250, [LOSS]: 0.0140, [EPISODE]: 151, [REWARD]: 95.0\n",
      "[STEP]: 13300, [LOSS]: 0.0253, [EPISODE]: 151, [REWARD]: 145.0\n",
      "[STEP]: 13350, [LOSS]: 0.0417, [EPISODE]: 151, [REWARD]: 195.0\n",
      "[STEP]: 13400, [LOSS]: 0.0087, [EPISODE]: 151, [REWARD]: 245.0\n",
      "[STEP]: 13450, [LOSS]: 89.3759, [EPISODE]: 151, [REWARD]: 295.0\n",
      "[STEP]: 13500, [LOSS]: 0.0234, [EPISODE]: 151, [REWARD]: 345.0\n",
      "[STEP]: 13550, [LOSS]: 0.0115, [EPISODE]: 151, [REWARD]: 395.0\n",
      "[STEP]: 13600, [LOSS]: 0.0367, [EPISODE]: 151, [REWARD]: 445.0\n",
      "[STEP]: 13650, [LOSS]: 0.0186, [EPISODE]: 151, [REWARD]: 495.0\n",
      "[STEP]: 13700, [LOSS]: 86.5182, [EPISODE]: 152, [REWARD]: 50.0\n",
      "[STEP]: 13750, [LOSS]: 0.0547, [EPISODE]: 152, [REWARD]: 100.0\n",
      "[STEP]: 13800, [LOSS]: 0.0148, [EPISODE]: 152, [REWARD]: 150.0\n",
      "[STEP]: 13850, [LOSS]: 0.0365, [EPISODE]: 152, [REWARD]: 200.0\n",
      "[STEP]: 13900, [LOSS]: 0.0350, [EPISODE]: 152, [REWARD]: 250.0\n",
      "[STEP]: 13950, [LOSS]: 0.0329, [EPISODE]: 152, [REWARD]: 300.0\n",
      "[STEP]: 14000, [LOSS]: 0.0162, [EPISODE]: 152, [REWARD]: 350.0\n",
      "[STEP]: 14050, [LOSS]: 0.0507, [EPISODE]: 152, [REWARD]: 400.0\n",
      "[STEP]: 14100, [LOSS]: 0.0162, [EPISODE]: 152, [REWARD]: 450.0\n",
      "[STEP]: 14150, [LOSS]: 285.3062, [EPISODE]: 152, [REWARD]: 500.0\n",
      "[STEP]: 14200, [LOSS]: 0.0247, [EPISODE]: 153, [REWARD]: 50.0\n",
      "[STEP]: 14250, [LOSS]: 0.0249, [EPISODE]: 153, [REWARD]: 100.0\n",
      "[STEP]: 14300, [LOSS]: 241.3968, [EPISODE]: 153, [REWARD]: 150.0\n",
      "[STEP]: 14350, [LOSS]: 0.0178, [EPISODE]: 153, [REWARD]: 200.0\n",
      "[STEP]: 14400, [LOSS]: 0.0198, [EPISODE]: 153, [REWARD]: 250.0\n",
      "[STEP]: 14450, [LOSS]: 0.0089, [EPISODE]: 153, [REWARD]: 300.0\n",
      "[STEP]: 14500, [LOSS]: 0.1366, [EPISODE]: 153, [REWARD]: 350.0\n",
      "[STEP]: 14550, [LOSS]: 0.0293, [EPISODE]: 153, [REWARD]: 400.0\n",
      "[STEP]: 14600, [LOSS]: 0.0193, [EPISODE]: 153, [REWARD]: 450.0\n",
      "[STEP]: 14650, [LOSS]: 0.0773, [EPISODE]: 153, [REWARD]: 500.0\n",
      "[STEP]: 14700, [LOSS]: 0.1894, [EPISODE]: 154, [REWARD]: 50.0\n",
      "[STEP]: 14750, [LOSS]: 0.0139, [EPISODE]: 154, [REWARD]: 100.0\n",
      "[STEP]: 14800, [LOSS]: 0.0393, [EPISODE]: 154, [REWARD]: 150.0\n",
      "[STEP]: 14850, [LOSS]: 293.5354, [EPISODE]: 154, [REWARD]: 200.0\n",
      "[STEP]: 14900, [LOSS]: 0.0106, [EPISODE]: 154, [REWARD]: 250.0\n",
      "[STEP]: 14950, [LOSS]: 0.0119, [EPISODE]: 154, [REWARD]: 300.0\n",
      "[STEP]: 15000, [LOSS]: 0.0110, [EPISODE]: 154, [REWARD]: 350.0\n",
      "[STEP]: 15050, [LOSS]: 0.0231, [EPISODE]: 155, [REWARD]: 12.0\n",
      "[STEP]: 15100, [LOSS]: 0.0267, [EPISODE]: 155, [REWARD]: 62.0\n",
      "[STEP]: 15150, [LOSS]: 0.0202, [EPISODE]: 155, [REWARD]: 112.0\n",
      "[STEP]: 15200, [LOSS]: 0.0348, [EPISODE]: 155, [REWARD]: 162.0\n",
      "[STEP]: 15250, [LOSS]: 0.0081, [EPISODE]: 155, [REWARD]: 212.0\n",
      "[STEP]: 15300, [LOSS]: 0.1560, [EPISODE]: 155, [REWARD]: 262.0\n",
      "[STEP]: 15350, [LOSS]: 0.0157, [EPISODE]: 155, [REWARD]: 312.0\n",
      "[STEP]: 15400, [LOSS]: 0.0192, [EPISODE]: 155, [REWARD]: 362.0\n",
      "[STEP]: 15450, [LOSS]: 0.0786, [EPISODE]: 155, [REWARD]: 412.0\n",
      "[STEP]: 15500, [LOSS]: 0.1431, [EPISODE]: 155, [REWARD]: 462.0\n",
      "[STEP]: 15550, [LOSS]: 291.8831, [EPISODE]: 156, [REWARD]: 12.0\n",
      "[STEP]: 15600, [LOSS]: 0.0116, [EPISODE]: 156, [REWARD]: 62.0\n",
      "[STEP]: 15650, [LOSS]: 0.0539, [EPISODE]: 156, [REWARD]: 112.0\n",
      "[STEP]: 15700, [LOSS]: 0.0270, [EPISODE]: 156, [REWARD]: 162.0\n",
      "[STEP]: 15750, [LOSS]: 296.1634, [EPISODE]: 156, [REWARD]: 212.0\n",
      "[STEP]: 15800, [LOSS]: 0.2197, [EPISODE]: 156, [REWARD]: 262.0\n",
      "[STEP]: 15850, [LOSS]: 295.4879, [EPISODE]: 156, [REWARD]: 312.0\n",
      "[STEP]: 15900, [LOSS]: 0.0091, [EPISODE]: 156, [REWARD]: 362.0\n",
      "[STEP]: 15950, [LOSS]: 0.0645, [EPISODE]: 156, [REWARD]: 412.0\n",
      "[STEP]: 16000, [LOSS]: 0.0234, [EPISODE]: 156, [REWARD]: 462.0\n",
      "[STEP]: 16050, [LOSS]: 0.0481, [EPISODE]: 157, [REWARD]: 12.0\n",
      "[STEP]: 16100, [LOSS]: 0.0500, [EPISODE]: 157, [REWARD]: 62.0\n",
      "[STEP]: 16150, [LOSS]: 0.0465, [EPISODE]: 157, [REWARD]: 112.0\n",
      "[STEP]: 16200, [LOSS]: 0.0052, [EPISODE]: 157, [REWARD]: 162.0\n",
      "[STEP]: 16250, [LOSS]: 0.0141, [EPISODE]: 157, [REWARD]: 212.0\n",
      "[STEP]: 16300, [LOSS]: 0.0175, [EPISODE]: 157, [REWARD]: 262.0\n",
      "[STEP]: 16350, [LOSS]: 0.0288, [EPISODE]: 157, [REWARD]: 312.0\n",
      "[STEP]: 16400, [LOSS]: 0.0138, [EPISODE]: 157, [REWARD]: 362.0\n",
      "[STEP]: 16450, [LOSS]: 0.1148, [EPISODE]: 157, [REWARD]: 412.0\n",
      "[STEP]: 16500, [LOSS]: 0.0402, [EPISODE]: 157, [REWARD]: 462.0\n",
      "[STEP]: 16550, [LOSS]: 0.0863, [EPISODE]: 158, [REWARD]: 12.0\n",
      "[STEP]: 16600, [LOSS]: 0.1545, [EPISODE]: 158, [REWARD]: 62.0\n",
      "[STEP]: 16650, [LOSS]: 0.0079, [EPISODE]: 158, [REWARD]: 112.0\n",
      "[STEP]: 16700, [LOSS]: 0.0343, [EPISODE]: 158, [REWARD]: 162.0\n",
      "[STEP]: 16750, [LOSS]: 0.0110, [EPISODE]: 158, [REWARD]: 212.0\n",
      "[STEP]: 16800, [LOSS]: 0.0108, [EPISODE]: 158, [REWARD]: 262.0\n",
      "[STEP]: 16850, [LOSS]: 0.0210, [EPISODE]: 158, [REWARD]: 312.0\n",
      "[STEP]: 16900, [LOSS]: 0.0116, [EPISODE]: 158, [REWARD]: 362.0\n",
      "[STEP]: 16950, [LOSS]: 0.0850, [EPISODE]: 158, [REWARD]: 412.0\n",
      "[STEP]: 17000, [LOSS]: 0.0084, [EPISODE]: 158, [REWARD]: 462.0\n",
      "[STEP]: 17050, [LOSS]: 0.0068, [EPISODE]: 159, [REWARD]: 12.0\n",
      "[STEP]: 17100, [LOSS]: 0.0481, [EPISODE]: 159, [REWARD]: 62.0\n",
      "[STEP]: 17150, [LOSS]: 0.0175, [EPISODE]: 159, [REWARD]: 112.0\n",
      "[STEP]: 17200, [LOSS]: 0.0238, [EPISODE]: 159, [REWARD]: 162.0\n",
      "[STEP]: 17250, [LOSS]: 0.0042, [EPISODE]: 159, [REWARD]: 212.0\n",
      "[STEP]: 17300, [LOSS]: 0.0106, [EPISODE]: 159, [REWARD]: 262.0\n",
      "[STEP]: 17350, [LOSS]: 0.0269, [EPISODE]: 159, [REWARD]: 312.0\n",
      "[STEP]: 17400, [LOSS]: 0.0288, [EPISODE]: 159, [REWARD]: 362.0\n",
      "[STEP]: 17450, [LOSS]: 0.0114, [EPISODE]: 159, [REWARD]: 412.0\n",
      "[STEP]: 17500, [LOSS]: 0.0341, [EPISODE]: 159, [REWARD]: 462.0\n",
      "[STEP]: 17550, [LOSS]: 310.3384, [EPISODE]: 160, [REWARD]: 12.0\n",
      "[STEP]: 17600, [LOSS]: 0.0113, [EPISODE]: 160, [REWARD]: 62.0\n",
      "[STEP]: 17650, [LOSS]: 0.0130, [EPISODE]: 160, [REWARD]: 112.0\n",
      "[STEP]: 17700, [LOSS]: 0.0100, [EPISODE]: 160, [REWARD]: 162.0\n",
      "[STEP]: 17750, [LOSS]: 0.0333, [EPISODE]: 160, [REWARD]: 212.0\n",
      "[STEP]: 17800, [LOSS]: 0.0381, [EPISODE]: 160, [REWARD]: 262.0\n",
      "[STEP]: 17850, [LOSS]: 0.0149, [EPISODE]: 160, [REWARD]: 312.0\n",
      "[STEP]: 17900, [LOSS]: 0.0203, [EPISODE]: 160, [REWARD]: 362.0\n",
      "[STEP]: 17950, [LOSS]: 0.0100, [EPISODE]: 160, [REWARD]: 412.0\n",
      "[STEP]: 18000, [LOSS]: 0.0334, [EPISODE]: 160, [REWARD]: 462.0\n",
      "[STEP]: 18050, [LOSS]: 0.0117, [EPISODE]: 161, [REWARD]: 12.0\n",
      "[STEP]: 18100, [LOSS]: 0.0255, [EPISODE]: 161, [REWARD]: 62.0\n",
      "[STEP]: 18150, [LOSS]: 0.0172, [EPISODE]: 161, [REWARD]: 112.0\n",
      "[STEP]: 18200, [LOSS]: 0.0759, [EPISODE]: 161, [REWARD]: 162.0\n",
      "[STEP]: 18250, [LOSS]: 0.0127, [EPISODE]: 161, [REWARD]: 212.0\n",
      "[STEP]: 18300, [LOSS]: 0.0153, [EPISODE]: 161, [REWARD]: 262.0\n",
      "[STEP]: 18350, [LOSS]: 0.0225, [EPISODE]: 161, [REWARD]: 312.0\n",
      "[STEP]: 18400, [LOSS]: 0.0077, [EPISODE]: 161, [REWARD]: 362.0\n",
      "[STEP]: 18450, [LOSS]: 0.0051, [EPISODE]: 161, [REWARD]: 412.0\n",
      "[STEP]: 18500, [LOSS]: 0.0063, [EPISODE]: 161, [REWARD]: 462.0\n",
      "[STEP]: 18550, [LOSS]: 0.0535, [EPISODE]: 162, [REWARD]: 12.0\n",
      "[STEP]: 18600, [LOSS]: 0.0642, [EPISODE]: 163, [REWARD]: 7.0\n",
      "[STEP]: 18650, [LOSS]: 321.1189, [EPISODE]: 164, [REWARD]: 18.0\n",
      "[STEP]: 18700, [LOSS]: 0.2123, [EPISODE]: 165, [REWARD]: 22.0\n",
      "[STEP]: 18750, [LOSS]: 314.2486, [EPISODE]: 165, [REWARD]: 72.0\n",
      "[STEP]: 18800, [LOSS]: 0.0749, [EPISODE]: 165, [REWARD]: 122.0\n",
      "[STEP]: 18850, [LOSS]: 0.0549, [EPISODE]: 165, [REWARD]: 172.0\n",
      "[STEP]: 18900, [LOSS]: 0.0189, [EPISODE]: 166, [REWARD]: 34.0\n",
      "[STEP]: 18950, [LOSS]: 0.0812, [EPISODE]: 166, [REWARD]: 84.0\n",
      "[STEP]: 19000, [LOSS]: 0.2642, [EPISODE]: 166, [REWARD]: 134.0\n",
      "[STEP]: 19050, [LOSS]: 0.0377, [EPISODE]: 166, [REWARD]: 184.0\n",
      "[STEP]: 19100, [LOSS]: 0.0495, [EPISODE]: 166, [REWARD]: 234.0\n",
      "[STEP]: 19150, [LOSS]: 0.3445, [EPISODE]: 166, [REWARD]: 284.0\n",
      "[STEP]: 19200, [LOSS]: 0.1158, [EPISODE]: 166, [REWARD]: 334.0\n",
      "[STEP]: 19250, [LOSS]: 0.0637, [EPISODE]: 166, [REWARD]: 384.0\n",
      "[STEP]: 19300, [LOSS]: 0.2442, [EPISODE]: 166, [REWARD]: 434.0\n",
      "[STEP]: 19350, [LOSS]: 0.3783, [EPISODE]: 166, [REWARD]: 484.0\n",
      "[STEP]: 19400, [LOSS]: 0.0583, [EPISODE]: 167, [REWARD]: 34.0\n",
      "[STEP]: 19450, [LOSS]: 0.3335, [EPISODE]: 167, [REWARD]: 84.0\n",
      "[STEP]: 19500, [LOSS]: 0.0312, [EPISODE]: 167, [REWARD]: 134.0\n",
      "[STEP]: 19550, [LOSS]: 292.7471, [EPISODE]: 167, [REWARD]: 184.0\n",
      "[STEP]: 19600, [LOSS]: 0.0532, [EPISODE]: 167, [REWARD]: 234.0\n",
      "[STEP]: 19650, [LOSS]: 0.0234, [EPISODE]: 167, [REWARD]: 284.0\n",
      "[STEP]: 19700, [LOSS]: 0.0221, [EPISODE]: 167, [REWARD]: 334.0\n",
      "[STEP]: 19750, [LOSS]: 0.0339, [EPISODE]: 167, [REWARD]: 384.0\n",
      "[STEP]: 19800, [LOSS]: 0.0457, [EPISODE]: 167, [REWARD]: 434.0\n",
      "[STEP]: 19850, [LOSS]: 0.0687, [EPISODE]: 167, [REWARD]: 484.0\n",
      "[STEP]: 19900, [LOSS]: 0.0315, [EPISODE]: 168, [REWARD]: 34.0\n",
      "[STEP]: 19950, [LOSS]: 0.0308, [EPISODE]: 168, [REWARD]: 84.0\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "episode = 0\n",
    "\n",
    "for step in range(N_TRAINING_STEPS):\n",
    "    epsilon = max(MIN_EPSILON, epsilon * DECAY)\n",
    "    action = select_action(\n",
    "        state=state,\n",
    "        q_net=q_net,\n",
    "        action_size=action_size,\n",
    "        epsilon=epsilon\n",
    "    )\n",
    "    \n",
    "    new_state, reward, terminated, truncated, info = env.step(action=action)\n",
    "    done = truncated or terminated\n",
    "    episode_reward += reward\n",
    "    buffer.add_sample((state, action, reward, new_state, done))\n",
    "    \n",
    "    if step % 100 == 0 and step < N_BUFFER_SIZE:\n",
    "        print(f\"Buffer filled so far: {len(buffer)}\")\n",
    "    \n",
    "    if step > N_START_LEARNING:\n",
    "        q_net.train()\n",
    "        target_q_net.eval()\n",
    "        batch = buffer.get_batch()\n",
    "        batch_len = len(batch)\n",
    "        states = torch.zeros(batch_len, state_size)\n",
    "        actions = torch.zeros(batch_len, 1, dtype=torch.int64)\n",
    "        rewards = torch.zeros(batch_len, 1)\n",
    "        next_states = torch.zeros(batch_len, state_size)\n",
    "        dones = torch.zeros(batch_len, 1)\n",
    "        \n",
    "        for idx, i in enumerate(batch):\n",
    "            states[idx] = torch.tensor(i[0], dtype=torch.float32)\n",
    "            actions[idx] = torch.tensor(i[1], dtype=torch.int64)\n",
    "            rewards[idx] = torch.tensor(i[2], dtype=torch.float32)\n",
    "            next_states[idx] = torch.tensor(i[3], dtype=torch.float32)\n",
    "            dones[idx] = torch.tensor(i[4], dtype=torch.float32)\n",
    "        \n",
    "        predicted_q_values = q_net(states)\n",
    "        extracted_q_values = predicted_q_values.gather(1, actions).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_q_net(next_states)\n",
    "            max_next_q = next_q_values.max(dim=1)[0]\n",
    "            \n",
    "        target = rewards.squeeze(1) + GAMMA * max_next_q * (1 - dones.squeeze(1))\n",
    "        \n",
    "        loss = F.mse_loss(extracted_q_values, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(q_net.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % TARGET_UPDATE_FREQUENCY == 0:\n",
    "            target_q_net.load_state_dict(q_net.state_dict())\n",
    "            print(f\"[STEP]: {step}, [LOSS]: {loss.item():.4f}, [EPISODE]: {episode}, [REWARD]: {episode_reward}\")\n",
    "    if done:\n",
    "        state, _ = env.reset()\n",
    "        episode += 1\n",
    "        episode_reward = 0\n",
    "    else:\n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "906cb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(q_net, env, num_episodes=10, render=False, record_video=False, video_folder=\"/RL/videos\"):\n",
    "    q_net.eval()  # Set to evaluation mode\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    success_count = 0  \n",
    "\n",
    "    # If recording video, wrap the environment.\n",
    "    # We need a new env instance for the wrapper to work correctly each time\n",
    "    # This ensures render_mode=\"rgb_array\" is set for video capture.\n",
    "    if record_video:\n",
    "        # Create the video folder if it doesn't exist\n",
    "        if not os.path.exists(video_folder):\n",
    "            os.makedirs(video_folder, exist_ok=True)\n",
    "            \n",
    "        # Wrap this new environment for video recording\n",
    "        env = gym.wrappers.RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: True) # Record all episodes\n",
    "        print(f\"Recording videos to: {video_folder}\")\n",
    "    elif render:\n",
    "        pass\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # The env.reset() method now returns (observation, info) in Gymnasium\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Only render for human viewing if not recording video,\n",
    "            # as RecordVideo handles its own internal rgb_array rendering.\n",
    "            if render and not record_video: \n",
    "                try:\n",
    "                    env.render()\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not render environment. Error: {e}\")\n",
    "            \n",
    "            # Select action greedily (no exploration)\n",
    "            with torch.no_grad():\n",
    "                # Ensure state is a float tensor and has a batch dimension\n",
    "                state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "                q_values = q_net(state_tensor)\n",
    "                action = torch.argmax(q_values, dim=-1).item()\n",
    "            \n",
    "            # Gymnasium step returns (observation, reward, terminated, truncated, info)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated # An episode is done if it's terminated or truncated\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        # CartPole is considered \"solved\" if it can balance for 195+ steps\n",
    "        if steps >= 195:\n",
    "            success_count += 1\n",
    "            \n",
    "        if episode % 20 == 0:\n",
    "            print(f\"Episode {episode}: Reward = {episode_reward}, Steps = {steps}\")\n",
    "    \n",
    "    # Crucially, close the environment to finalize video recording if a wrapper was used,\n",
    "    # or just to clean up resources for the original environment.\n",
    "    env.close() \n",
    "    \n",
    "    # If the environment was wrapped for video recording, the original_env still needs to be closed\n",
    "    if record_video:\n",
    "        env.close()\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'mean_reward': np.mean(episode_rewards),\n",
    "        'std_reward': np.std(episode_rewards),\n",
    "        'min_reward': np.min(episode_rewards),\n",
    "        'max_reward': np.max(episode_rewards),\n",
    "        'mean_length': np.mean(episode_lengths),\n",
    "        'std_length': np.std(episode_lengths),\n",
    "        'success_rate': success_count / num_episodes,\n",
    "        'episode_rewards': episode_rewards, # Raw list of rewards\n",
    "        'episode_lengths': episode_lengths  # Raw list of lengths\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "063bbb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\Pytorch\\venv\\lib\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at b:\\RL\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording videos to: /RL/videos\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-0.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-0.mp4\n",
      "Episode 0: Reward = 500.0, Steps = 500\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-1.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-1.mp4\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-2.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-2.mp4\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-3.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-3.mp4\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-4.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-4.mp4\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-5.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-5.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-5.mp4\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-6.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-6.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-6.mp4\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-7.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-7.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-7.mp4\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-8.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-8.mp4\n",
      "MoviePy - Building video b:\\RL\\videos\\rl-video-episode-9.mp4.\n",
      "MoviePy - Writing video b:\\RL\\videos\\rl-video-episode-9.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready b:\\RL\\videos\\rl-video-episode-9.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_reward': 500.0,\n",
       " 'std_reward': 0.0,\n",
       " 'min_reward': 500.0,\n",
       " 'max_reward': 500.0,\n",
       " 'mean_length': 500.0,\n",
       " 'std_length': 0.0,\n",
       " 'success_rate': 1.0,\n",
       " 'episode_rewards': [500.0,\n",
       "  500.0,\n",
       "  500.0,\n",
       "  500.0,\n",
       "  500.0,\n",
       "  500.0,\n",
       "  500.0,\n",
       "  500.0,\n",
       "  500.0,\n",
       "  500.0],\n",
       " 'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(q_net, env, record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict saved to B:/Pytorch/RL/models/dqn.pth\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_PATH = \"RL/models/dqn.pth\"\n",
    "torch.save(q_net.state_dict(), MODEL_FILE_PATH)\n",
    "print(f\"Model state_dict saved to {MODEL_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96097d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
