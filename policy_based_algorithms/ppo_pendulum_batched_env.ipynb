{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a620cae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Normal\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# import gym\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimageio\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch, os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "# import gym\n",
    "import imageio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530c4ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num updates:  4882\n",
      "Batch size:  1024\n",
      "Num Minibatch size: 256\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "ENV_NAME = \"Pendulum-v1\"\n",
    "GAMMA = 0.99\n",
    "MAX_ENVS = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_STEPS = 128\n",
    "TOTAL_STEPS = 5_000_000\n",
    "BATCH_SIZE = MAX_ENVS * MAX_STEPS\n",
    "NUM_UPDATES = TOTAL_STEPS // BATCH_SIZE\n",
    "NUM_MINIBATCHES = 4 \n",
    "NUM_MINIBATCHES_SIZE = BATCH_SIZE // NUM_MINIBATCHES\n",
    "PPO_EPOCHS = 5\n",
    "CLIP_VALUE = 0.2\n",
    "VALUE_COEFF = 0.5\n",
    "ENTROPY_COEFF = 0.01\n",
    "LOG_EVERY_N_STEPS = 50\n",
    "\n",
    "print(\"Num updates: \", NUM_UPDATES)\n",
    "print(\"Batch size: \", BATCH_SIZE)\n",
    "print(\"Num Minibatch size:\", NUM_MINIBATCHES_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8cb129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e51342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(idx, env_name, seed, gamma,eval_mode=None, render_mode=None):\n",
    "    env = gym.make(env_name, render_mode=render_mode)\n",
    "    env = gym.wrappers.ClipAction(env)\n",
    "\n",
    "    if not eval_mode:\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        obs_space = gym.spaces.Box(low=-10, high=10, shape=env.observation_space.shape, dtype=np.float32)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10), observation_space=obs_space)\n",
    "        env = gym.wrappers.NormalizeReward(env, gamma=gamma)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "    \n",
    "    env.action_space.seed(seed + idx)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca045f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [lambda i=i: make_env(i, ENV_NAME, SEED, GAMMA) for i in range(MAX_ENVS)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2943e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 1\n",
      "Observation Space: 3\n"
     ]
    }
   ],
   "source": [
    "observation_space = envs.single_observation_space.shape[0]\n",
    "action_space = envs.single_action_space.shape[0]\n",
    "print(f\"Action Space: {action_space}\")\n",
    "print(f\"Observation Space: {observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9ff446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super().__init__()\n",
    "        print(f\"State space: {state_space}, action_space: {action_space}\")\n",
    "        self.fc1 = nn.Linear(state_space, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.sigma = nn.Parameter(torch.zeros(1, action_space))\n",
    "        self.mu = nn.Linear(128, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.sigma.expand_as(mu)\n",
    "        return mu, logvar.exp()\n",
    "    \n",
    "    def get_action(self, x):\n",
    "        mu, sigma= self.forward(x)\n",
    "        dist = Normal(mu, sigma)  \n",
    "        action = dist.rsample() \n",
    "        log_prob = dist.log_prob(action).sum(-1)\n",
    "        entropy = dist.entropy().sum(-1)\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "    def evaluate_get_action(self, x, act):\n",
    "        mu, sigma= self.forward(x)\n",
    "        dist = Normal(mu, sigma)\n",
    "        log_probs = dist.log_prob(act).sum(-1)\n",
    "        entropy = dist.entropy().sum(-1)\n",
    "        return log_probs, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2651732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(CriticNet, self).__init__()\n",
    "        print(f\"State space: {state_space}, Action space: {action_space}\")\n",
    "        self.fc1 = nn.Linear(state_space, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.value = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.tanh(self.fc1(x))\n",
    "        x = torch.nn.functional.tanh(self.fc2(x))\n",
    "        x = torch.nn.functional.tanh(self.fc3(x))\n",
    "        return self.value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5cc443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: 3, action_space: 1\n",
      "State space: 3, Action space: 1\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "actor_network = ActorNet(observation_space, action_space).to(device)\n",
    "critic_network = CriticNet(observation_space, action_space).to(device)\n",
    "optimizer = optim.Adam(list(actor_network.parameters()) + list(critic_network.parameters()), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(MAX_ENVS, observation_space, device=device, dtype=torch.float32)\n",
    "    test_val = critic_network(x)\n",
    "    test_act, test_log_probs, test_entropy = actor_network.get_action(x)\n",
    "    print(test_act.shape)\n",
    "    print(test_log_probs.shape)\n",
    "    print(test_entropy.shape)\n",
    "    print(test_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc36c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(envs, model, device, gamma, num_eval_eps=10, record=False, render_mode=None):\n",
    "    eval_env = make_env(0, ENV_NAME, SEED, gamma, eval_mode=True, render_mode=render_mode)\n",
    "    eval_env.action_space.seed(SEED)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    returns = []\n",
    "    frames = []\n",
    "\n",
    "    obs_rms = envs.get_attr(\"obs_rms\")[0] \n",
    "\n",
    "    model.eval()\n",
    "    for eps in range(num_eval_eps):\n",
    "        obs, _ = eval_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "   \n",
    "        while not done:\n",
    "            if record:\n",
    "                frame = eval_env.render()\n",
    "                frames.append(frame)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                norm_obs = np.clip((obs - obs_rms.mean) / np.sqrt(obs_rms.var + 1e-8), -10, 10)\n",
    "                \n",
    "                act, _, _ = model.get_action(torch.tensor(norm_obs, device=device, dtype=torch.float32).unsqueeze(0))\n",
    "                obs, reward, terminated, truncated, _ = eval_env.step(act.cpu().numpy().flatten())\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "        returns.append(episode_reward)\n",
    "    \n",
    "    eval_env.close()\n",
    "    model.train()\n",
    "    return returns, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c089dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_storage = torch.zeros((MAX_STEPS, MAX_ENVS, observation_space)).to(device)\n",
    "actions_storage = torch.zeros((MAX_STEPS, MAX_ENVS, action_space)).to(device)\n",
    "logprobs_storage = torch.zeros((MAX_STEPS, MAX_ENVS)).to(device)\n",
    "rewards_storage = torch.zeros((MAX_STEPS, MAX_ENVS)).to(device)\n",
    "dones_storage = torch.zeros((MAX_STEPS, MAX_ENVS)).to(device)\n",
    "values_storage = torch.zeros((MAX_STEPS, MAX_ENVS)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9eb8797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_obs, _ = envs.reset(seed=SEED)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(MAX_ENVS).to(device)\n",
    "next_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4acc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP]: 50, [ACTOR_LOSS]: -0.01650993525981903, [CRITIC_LOSS]: 0.14201116561889648, [TOTAL_LOSS]: 0.11158302426338196, [REWARDS]: -0.05665622651576996\n",
      "[STEP]: 100, [ACTOR_LOSS]: -0.0015056682750582695, [CRITIC_LOSS]: 0.1322791874408722, [TOTAL_LOSS]: 0.11683674156665802, [REWARDS]: -0.053673453629016876\n",
      "[STEP]: 150, [ACTOR_LOSS]: 0.009807728230953217, [CRITIC_LOSS]: 0.23466598987579346, [TOTAL_LOSS]: 0.23061688244342804, [REWARDS]: -0.05489141494035721\n",
      "[STEP]: 200, [ACTOR_LOSS]: 0.04063758999109268, [CRITIC_LOSS]: 0.23723389208316803, [TOTAL_LOSS]: 0.26442521810531616, [REWARDS]: -0.05328790098428726\n",
      "[STEP]: 250, [ACTOR_LOSS]: 0.12823787331581116, [CRITIC_LOSS]: 0.21798241138458252, [TOTAL_LOSS]: 0.3329823911190033, [REWARDS]: -0.0516984760761261\n",
      "[STEP]: 300, [ACTOR_LOSS]: 0.011882031336426735, [CRITIC_LOSS]: 0.16546915471553802, [TOTAL_LOSS]: 0.1644914597272873, [REWARDS]: -0.04353591799736023\n",
      "[STEP]: 350, [ACTOR_LOSS]: 0.11027967929840088, [CRITIC_LOSS]: 0.04031980782747269, [TOTAL_LOSS]: 0.13840514421463013, [REWARDS]: -0.0064045097678899765\n",
      "[STEP]: 400, [ACTOR_LOSS]: 0.056772083044052124, [CRITIC_LOSS]: 0.043097443878650665, [TOTAL_LOSS]: 0.08745215088129044, [REWARDS]: -0.01612679287791252\n",
      "[STEP]: 450, [ACTOR_LOSS]: -0.0019103661179542542, [CRITIC_LOSS]: 0.011268806643784046, [TOTAL_LOSS]: -0.0028237923979759216, [REWARDS]: -0.011574126780033112\n",
      "[STEP]: 500, [ACTOR_LOSS]: 0.019212834537029266, [CRITIC_LOSS]: 0.04895550012588501, [TOTAL_LOSS]: 0.056003421545028687, [REWARDS]: -0.010157957673072815\n",
      "[STEP]: 550, [ACTOR_LOSS]: -0.07331532984972, [CRITIC_LOSS]: 0.060193829238414764, [TOTAL_LOSS]: -0.025507718324661255, [REWARDS]: -0.012741192243993282\n",
      "[STEP]: 600, [ACTOR_LOSS]: 0.013574683107435703, [CRITIC_LOSS]: 0.009927167557179928, [TOTAL_LOSS]: 0.011442168615758419, [REWARDS]: -0.007731940597295761\n",
      "[STEP]: 650, [ACTOR_LOSS]: -0.0516357421875, [CRITIC_LOSS]: 0.009924519807100296, [TOTAL_LOSS]: -0.05408932641148567, [REWARDS]: -0.00590834254398942\n",
      "[STEP]: 700, [ACTOR_LOSS]: 0.0394412986934185, [CRITIC_LOSS]: 0.017984386533498764, [TOTAL_LOSS]: 0.04478299245238304, [REWARDS]: -0.006661730352789164\n",
      "[STEP]: 750, [ACTOR_LOSS]: 0.11182082444429398, [CRITIC_LOSS]: 0.009877417236566544, [TOTAL_LOSS]: 0.10867127776145935, [REWARDS]: -0.01140979677438736\n",
      "[STEP]: 800, [ACTOR_LOSS]: 0.031620707362890244, [CRITIC_LOSS]: 0.08683450520038605, [TOTAL_LOSS]: 0.10525217652320862, [REWARDS]: -0.019424231722950935\n",
      "[STEP]: 850, [ACTOR_LOSS]: -0.005518842488527298, [CRITIC_LOSS]: 0.019841579720377922, [TOTAL_LOSS]: 0.0012132469564676285, [REWARDS]: -0.012519991025328636\n",
      "[STEP]: 900, [ACTOR_LOSS]: 0.176847442984581, [CRITIC_LOSS]: 0.021088913083076477, [TOTAL_LOSS]: 0.184583380818367, [REWARDS]: -0.008146528154611588\n",
      "[STEP]: 950, [ACTOR_LOSS]: -0.0913543775677681, [CRITIC_LOSS]: 0.00895780324935913, [TOTAL_LOSS]: -0.09546259045600891, [REWARDS]: -0.004132886882871389\n",
      "[STEP]: 1000, [ACTOR_LOSS]: -0.02094350755214691, [CRITIC_LOSS]: 0.007162436842918396, [TOTAL_LOSS]: -0.026949092745780945, [REWARDS]: -0.0038459934294223785\n",
      "[STEP]: 1050, [ACTOR_LOSS]: 0.0052837198600173, [CRITIC_LOSS]: 0.006175052374601364, [TOTAL_LOSS]: -0.0015776390209794044, [REWARDS]: -0.013468293473124504\n",
      "[STEP]: 1100, [ACTOR_LOSS]: -0.005034862086176872, [CRITIC_LOSS]: 0.031261641532182693, [TOTAL_LOSS]: 0.012985430657863617, [REWARDS]: -0.015044275671243668\n",
      "[STEP]: 1150, [ACTOR_LOSS]: 0.008710136637091637, [CRITIC_LOSS]: 0.01565995253622532, [TOTAL_LOSS]: 0.01135774701833725, [REWARDS]: -0.01365489698946476\n",
      "[STEP]: 1200, [ACTOR_LOSS]: 0.11557790637016296, [CRITIC_LOSS]: 0.024504421278834343, [TOTAL_LOSS]: 0.12710009515285492, [REWARDS]: -0.008357853628695011\n",
      "[STEP]: 1250, [ACTOR_LOSS]: 0.08534406125545502, [CRITIC_LOSS]: 0.0219552144408226, [TOTAL_LOSS]: 0.09433108568191528, [REWARDS]: -0.0058064088225364685\n",
      "[STEP]: 1300, [ACTOR_LOSS]: 0.014711597934365273, [CRITIC_LOSS]: 0.025961995124816895, [TOTAL_LOSS]: 0.02812224254012108, [REWARDS]: -0.010771704837679863\n",
      "[STEP]: 1350, [ACTOR_LOSS]: -0.04463399201631546, [CRITIC_LOSS]: 0.012384822592139244, [TOTAL_LOSS]: -0.04476740211248398, [REWARDS]: -0.0067267329432070255\n",
      "[STEP]: 1400, [ACTOR_LOSS]: 0.006913220509886742, [CRITIC_LOSS]: 0.035802654922008514, [TOTAL_LOSS]: 0.03014690801501274, [REWARDS]: -0.01622028648853302\n",
      "[STEP]: 1450, [ACTOR_LOSS]: 0.009247194975614548, [CRITIC_LOSS]: 0.07647353410720825, [TOTAL_LOSS]: 0.0733107179403305, [REWARDS]: -0.018991373479366302\n",
      "[STEP]: 1500, [ACTOR_LOSS]: -0.021898090839385986, [CRITIC_LOSS]: 0.04590139910578728, [TOTAL_LOSS]: 0.011673782020807266, [REWARDS]: -0.013363542035222054\n",
      "[STEP]: 1550, [ACTOR_LOSS]: 0.006744571961462498, [CRITIC_LOSS]: 0.03378075361251831, [TOTAL_LOSS]: 0.0282912440598011, [REWARDS]: -0.010294685140252113\n",
      "[STEP]: 1600, [ACTOR_LOSS]: 0.05231724679470062, [CRITIC_LOSS]: 0.02413310669362545, [TOTAL_LOSS]: 0.06472155451774597, [REWARDS]: -0.00792059488594532\n",
      "[STEP]: 1650, [ACTOR_LOSS]: -0.1009097620844841, [CRITIC_LOSS]: 0.0266512893140316, [TOTAL_LOSS]: -0.08613502979278564, [REWARDS]: -0.014036828652024269\n",
      "[STEP]: 1700, [ACTOR_LOSS]: 0.017663247883319855, [CRITIC_LOSS]: 0.04415538161993027, [TOTAL_LOSS]: 0.05022501200437546, [REWARDS]: -0.014053440652787685\n",
      "[STEP]: 1750, [ACTOR_LOSS]: 0.04700466990470886, [CRITIC_LOSS]: 0.031371526420116425, [TOTAL_LOSS]: 0.06703880429267883, [REWARDS]: -0.0168490968644619\n",
      "[STEP]: 1800, [ACTOR_LOSS]: -0.01703454740345478, [CRITIC_LOSS]: 0.0384732186794281, [TOTAL_LOSS]: 0.0104177575558424, [REWARDS]: -0.013780834153294563\n",
      "[STEP]: 1850, [ACTOR_LOSS]: -0.03628750890493393, [CRITIC_LOSS]: 0.0343816764652729, [TOTAL_LOSS]: -0.012657713145017624, [REWARDS]: -0.010187914595007896\n",
      "[STEP]: 1900, [ACTOR_LOSS]: 0.08665468543767929, [CRITIC_LOSS]: 0.01390871312469244, [TOTAL_LOSS]: 0.09009607881307602, [REWARDS]: -0.005904523190110922\n",
      "[STEP]: 1950, [ACTOR_LOSS]: -0.008064333349466324, [CRITIC_LOSS]: 0.010188247077167034, [TOTAL_LOSS]: -0.008145331405103207, [REWARDS]: -0.009597796946763992\n",
      "[STEP]: 2000, [ACTOR_LOSS]: 0.028560586273670197, [CRITIC_LOSS]: 0.025070689618587494, [TOTAL_LOSS]: 0.04320456087589264, [REWARDS]: -0.015191501937806606\n",
      "[STEP]: 2050, [ACTOR_LOSS]: 0.04995972663164139, [CRITIC_LOSS]: 0.015637407079339027, [TOTAL_LOSS]: 0.05539499223232269, [REWARDS]: -0.012431981973350048\n",
      "[STEP]: 2100, [ACTOR_LOSS]: -0.09447455406188965, [CRITIC_LOSS]: 0.05703122913837433, [TOTAL_LOSS]: -0.0475182831287384, [REWARDS]: -0.01544015109539032\n",
      "[STEP]: 2150, [ACTOR_LOSS]: -0.10177766531705856, [CRITIC_LOSS]: 0.06716293096542358, [TOTAL_LOSS]: -0.044633880257606506, [REWARDS]: -0.013551400974392891\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m logprobs_storage[step] = logprob\n\u001b[32m     14\u001b[39m next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m done = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogical_or\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m rewards_storage[step] = torch.tensor(reward).to(device).view(-\u001b[32m1\u001b[39m)\n\u001b[32m     18\u001b[39m next_obs = torch.Tensor(next_obs).to(device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for update in range(1, NUM_UPDATES + 1):\n",
    "    for step in range(0, MAX_STEPS):\n",
    "        obs_storage[step] = next_obs\n",
    "        dones_storage[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _ = actor_network.get_action(next_obs)\n",
    "            value = critic_network(next_obs)\n",
    "        \n",
    "        values_storage[step] = value.flatten()\n",
    "        actions_storage[step] = action\n",
    "        logprobs_storage[step] = logprob\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "\n",
    "        rewards_storage[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs = torch.Tensor(next_obs).to(device)\n",
    "        next_done = torch.Tensor(done).to(device)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        returns = torch.zeros_like(rewards_storage).to(device)\n",
    "        bootstrap_value = critic_network(next_obs).squeeze()\n",
    "        gt_next_state = bootstrap_value * (1.0 - next_done)\n",
    "        for t in reversed(range(MAX_STEPS)):\n",
    "            rt = rewards_storage[t] + GAMMA * gt_next_state\n",
    "            returns[t] = rt\n",
    "\n",
    "            gt_next_state = returns[t] * (1.0 - dones_storage[t])\n",
    "\n",
    "    advantages = returns - values_storage\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    \n",
    "    b_obs = obs_storage.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs_storage.reshape(-1)\n",
    "    b_actions = actions_storage.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "\n",
    "    b_inds = np.arange(BATCH_SIZE)\n",
    "    for epoch in range(PPO_EPOCHS):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, BATCH_SIZE, NUM_MINIBATCHES_SIZE):\n",
    "            end = start + NUM_MINIBATCHES_SIZE\n",
    "            mb_inds = b_inds[start:end]\n",
    "        \n",
    "            new_log_probs, entropy = actor_network.evaluate_get_action(b_obs[mb_inds], b_actions[mb_inds])\n",
    "            ratio = torch.exp(new_log_probs - b_logprobs[mb_inds])\n",
    "            pg_loss1 = b_advantages[mb_inds] * ratio\n",
    "            pg_loss2 = b_advantages[mb_inds] * torch.clamp(ratio, 1 - CLIP_VALUE, 1 + CLIP_VALUE)\n",
    "            policy_loss = -torch.min(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            current_values = critic_network(b_obs[mb_inds]).squeeze()\n",
    "            critic_loss = VALUE_COEFF * torch.nn.functional.smooth_l1_loss(current_values, b_returns[mb_inds])\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = policy_loss - ENTROPY_COEFF *  entropy_loss + critic_loss\n",
    "\n",
    "            # actor_optim.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(list(actor_network.parameters()) + list(critic_network.parameters()), 1.0)\n",
    "            optimizer.step()\n",
    "    \n",
    "    if update % LOG_EVERY_N_STEPS == 0:\n",
    "        print(f\"[STEP]: {update}, [ACTOR_LOSS]: {policy_loss.item()}, [CRITIC_LOSS]: {critic_loss.item()}, [TOTAL_LOSS]: {loss.item()}, [REWARDS]: {rewards_storage.mean()}\")\n",
    "        train_video_path = f\"B:\\Pytorch\\RL\\eval_episodes\\ppo_pendulum_{update}.mp4\"\n",
    "        returns, frames = evaluate(envs, actor_network, device, GAMMA, record=True, num_eval_eps=1, render_mode='rgb_array')\n",
    "\n",
    "        if frames and len(frames) > 0:\n",
    "            imageio.mimsave(\n",
    "                train_video_path,\n",
    "                frames,\n",
    "                fps=30,\n",
    "                codec='libx264',\n",
    "                macro_block_size=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b6178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
