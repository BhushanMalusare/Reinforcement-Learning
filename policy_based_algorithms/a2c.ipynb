{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05686367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1f1e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num updates:  4882\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "ENV_NAME = \"LunarLander-v3\"\n",
    "GAMMA = 0.99\n",
    "MAX_ENVS = 8\n",
    "LEARNING_RATE = 5e-4\n",
    "MAX_STEPS = 256\n",
    "TOTAL_STEPS = 10_000_000\n",
    "BATCH_SIZE = MAX_ENVS * MAX_STEPS\n",
    "NUM_UPDATES = TOTAL_STEPS // BATCH_SIZE\n",
    "VALUE_COEFF = 0.5\n",
    "ENTROPY_COEFF = 0.01\n",
    "LOG_EVERY_N_STEPS = 50\n",
    "print(\"Num updates: \", NUM_UPDATES)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72314d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(idx, is_eval=False):\n",
    "    def _init():\n",
    "        render_mode = None if not is_eval else \"rgb_array\"\n",
    "        env = gym.make(id=ENV_NAME, render_mode=render_mode)\n",
    "        env.action_space.seed(SEED + idx)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60067963",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv([make_env(idx) for idx in range(MAX_ENVS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c3d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: 4\n",
      "Observation space: 8\n"
     ]
    }
   ],
   "source": [
    "action_space = envs.single_action_space.n\n",
    "observation_space = envs.single_observation_space.shape[0]\n",
    "print(f\"Action space: {action_space}\")\n",
    "print(f\"Observation space: {observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af7517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f442f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super().__init__()\n",
    "        print(f\"Observation space: {observation_space}, action space: {action_space}\")\n",
    "        self.fc1 = nn.Linear(in_features=observation_space, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=256)\n",
    "\n",
    "        self.out = nn.Linear(in_features=256, out_features=action_space)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.softmax(self.out(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "    def get_action(self, x):\n",
    "        action_probs = self.forward(x)\n",
    "        dist = Categorical(probs=action_probs)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d10379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super().__init__()\n",
    "        print(f\"Observation space: {observation_space}, action space: {action_space}\")\n",
    "        self.fc1 = nn.Linear(in_features=observation_space, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5806583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: 8, action space: 4\n",
      "Observation space: 8, action space: 4\n"
     ]
    }
   ],
   "source": [
    "actor_net = Actor(observation_space, action_space).to(device=device)\n",
    "critic_net = Critic(observation_space, action_space).to(device=device)\n",
    "\n",
    "optimizer = optim.AdamW(list(actor_net.parameters()) + list(critic_net.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff1b63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "test_x = torch.randn(size=(2, 8), device=device)\n",
    "with torch.no_grad():\n",
    "    test_action, test_action_probs, test_entropy = actor_net.get_action(test_x)\n",
    "    test_value = critic_net(test_x)\n",
    "\n",
    "print(test_action.shape)\n",
    "print(test_action_probs.shape)\n",
    "print(test_entropy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd410633",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_storage = torch.zeros(size=(MAX_STEPS, MAX_ENVS), device=device)\n",
    "dones_storage = torch.zeros(size=(MAX_STEPS, MAX_ENVS), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc55b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP]: 50, [TOTAL_LOSS]: 25.68102264404297, [ACTOR_LOSS]: -0.005055042915046215, [CRITIC_LOSS]: 25.699565887451172, [REWARDS]: -1.191617727279663, [MEAN ENTROPY]: 1.3488019704818726\n",
      "[STEP]: 100, [TOTAL_LOSS]: 10.635954856872559, [ACTOR_LOSS]: -0.006335852202028036, [CRITIC_LOSS]: 10.654838562011719, [REWARDS]: -0.348219633102417, [MEAN ENTROPY]: 1.2547186613082886\n",
      "[STEP]: 150, [TOTAL_LOSS]: 14.343056678771973, [ACTOR_LOSS]: -0.00582401268184185, [CRITIC_LOSS]: 14.360677719116211, [REWARDS]: -0.25883153080940247, [MEAN ENTROPY]: 1.1796603202819824\n",
      "[STEP]: 200, [TOTAL_LOSS]: 11.706589698791504, [ACTOR_LOSS]: -0.0021101105958223343, [CRITIC_LOSS]: 11.72041130065918, [REWARDS]: 0.058290183544158936, [MEAN ENTROPY]: 1.1711573600769043\n",
      "[STEP]: 250, [TOTAL_LOSS]: 2.3185698986053467, [ACTOR_LOSS]: 0.005786249414086342, [CRITIC_LOSS]: 2.3254294395446777, [REWARDS]: 0.024688702076673508, [MEAN ENTROPY]: 1.264561414718628\n",
      "[STEP]: 300, [TOTAL_LOSS]: 9.494268417358398, [ACTOR_LOSS]: 0.039219651371240616, [CRITIC_LOSS]: 9.466097831726074, [REWARDS]: 0.011922173202037811, [MEAN ENTROPY]: 1.1049039363861084\n",
      "[STEP]: 350, [TOTAL_LOSS]: 2.8758866786956787, [ACTOR_LOSS]: -0.02166754938662052, [CRITIC_LOSS]: 2.9095652103424072, [REWARDS]: 0.018017549067735672, [MEAN ENTROPY]: 1.2011040449142456\n",
      "[STEP]: 400, [TOTAL_LOSS]: 6.053279876708984, [ACTOR_LOSS]: -0.11809806525707245, [CRITIC_LOSS]: 6.183367729187012, [REWARDS]: 0.07554911822080612, [MEAN ENTROPY]: 1.1989706754684448\n",
      "[STEP]: 450, [TOTAL_LOSS]: 6.291283130645752, [ACTOR_LOSS]: -0.059959374368190765, [CRITIC_LOSS]: 6.362068176269531, [REWARDS]: 0.22452285885810852, [MEAN ENTROPY]: 1.0825655460357666\n",
      "[STEP]: 500, [TOTAL_LOSS]: 3.3896005153656006, [ACTOR_LOSS]: -0.07339037954807281, [CRITIC_LOSS]: 3.4739432334899902, [REWARDS]: 0.1186785101890564, [MEAN ENTROPY]: 1.0952248573303223\n",
      "[STEP]: 550, [TOTAL_LOSS]: 4.517425060272217, [ACTOR_LOSS]: -0.06556558609008789, [CRITIC_LOSS]: 4.593663692474365, [REWARDS]: 0.18631958961486816, [MEAN ENTROPY]: 1.0672941207885742\n",
      "[STEP]: 600, [TOTAL_LOSS]: 3.5527377128601074, [ACTOR_LOSS]: -0.13407698273658752, [CRITIC_LOSS]: 3.696657657623291, [REWARDS]: 0.1975822150707245, [MEAN ENTROPY]: 0.9842931032180786\n",
      "[STEP]: 650, [TOTAL_LOSS]: 6.404229164123535, [ACTOR_LOSS]: -0.10326845943927765, [CRITIC_LOSS]: 6.517223834991455, [REWARDS]: 0.2701335549354553, [MEAN ENTROPY]: 0.9725964665412903\n",
      "[STEP]: 700, [TOTAL_LOSS]: 2.999647378921509, [ACTOR_LOSS]: 0.006861918140202761, [CRITIC_LOSS]: 3.0034823417663574, [REWARDS]: 0.17229481041431427, [MEAN ENTROPY]: 1.0697001218795776\n",
      "[STEP]: 750, [TOTAL_LOSS]: 2.538381814956665, [ACTOR_LOSS]: -0.043631866574287415, [CRITIC_LOSS]: 2.592071056365967, [REWARDS]: 0.3032165765762329, [MEAN ENTROPY]: 1.0057377815246582\n",
      "[STEP]: 800, [TOTAL_LOSS]: 0.7465152144432068, [ACTOR_LOSS]: 0.0019378215074539185, [CRITIC_LOSS]: 0.7549244165420532, [REWARDS]: 0.08935114741325378, [MEAN ENTROPY]: 1.034703254699707\n",
      "[STEP]: 850, [TOTAL_LOSS]: 1.0019540786743164, [ACTOR_LOSS]: -0.05777958035469055, [CRITIC_LOSS]: 1.0699491500854492, [REWARDS]: 0.20418517291545868, [MEAN ENTROPY]: 1.0215575695037842\n",
      "[STEP]: 900, [TOTAL_LOSS]: 2.427358388900757, [ACTOR_LOSS]: 0.0021118801087141037, [CRITIC_LOSS]: 2.434453010559082, [REWARDS]: 0.2124795764684677, [MEAN ENTROPY]: 0.9206418991088867\n",
      "[STEP]: 950, [TOTAL_LOSS]: 0.6586743593215942, [ACTOR_LOSS]: 0.007294418290257454, [CRITIC_LOSS]: 0.6612416505813599, [REWARDS]: 0.11265294253826141, [MEAN ENTROPY]: 0.9861680269241333\n",
      "[STEP]: 1000, [TOTAL_LOSS]: 1.8251824378967285, [ACTOR_LOSS]: -0.007002903148531914, [CRITIC_LOSS]: 1.8423242568969727, [REWARDS]: 0.18087314069271088, [MEAN ENTROPY]: 1.013887882232666\n",
      "[STEP]: 1050, [TOTAL_LOSS]: 5.000345230102539, [ACTOR_LOSS]: 0.04739293456077576, [CRITIC_LOSS]: 4.961553573608398, [REWARDS]: 0.11853756010532379, [MEAN ENTROPY]: 0.8601171970367432\n",
      "[STEP]: 1100, [TOTAL_LOSS]: 0.9221782684326172, [ACTOR_LOSS]: -0.03890824317932129, [CRITIC_LOSS]: 0.971460223197937, [REWARDS]: 0.1821584701538086, [MEAN ENTROPY]: 1.0373692512512207\n",
      "[STEP]: 1150, [TOTAL_LOSS]: 0.9968134164810181, [ACTOR_LOSS]: 0.00022514350712299347, [CRITIC_LOSS]: 1.0067274570465088, [REWARDS]: -0.012763711623847485, [MEAN ENTROPY]: 1.013924479484558\n",
      "[STEP]: 1200, [TOTAL_LOSS]: 1.0691945552825928, [ACTOR_LOSS]: 0.00651007192209363, [CRITIC_LOSS]: 1.0726439952850342, [REWARDS]: 0.08208386600017548, [MEAN ENTROPY]: 0.9959410429000854\n",
      "[STEP]: 1250, [TOTAL_LOSS]: 1.0633562803268433, [ACTOR_LOSS]: -0.0445973165333271, [CRITIC_LOSS]: 1.1178593635559082, [REWARDS]: 0.21230444312095642, [MEAN ENTROPY]: 0.990580677986145\n",
      "[STEP]: 1300, [TOTAL_LOSS]: 6.137150287628174, [ACTOR_LOSS]: -0.07889710366725922, [CRITIC_LOSS]: 6.224164962768555, [REWARDS]: 0.25750529766082764, [MEAN ENTROPY]: 0.8117684125900269\n",
      "[STEP]: 1350, [TOTAL_LOSS]: 1.6165790557861328, [ACTOR_LOSS]: -0.054772794246673584, [CRITIC_LOSS]: 1.6808078289031982, [REWARDS]: 0.23262298107147217, [MEAN ENTROPY]: 0.9455930590629578\n",
      "[STEP]: 1400, [TOTAL_LOSS]: 2.205265760421753, [ACTOR_LOSS]: 0.06851618736982346, [CRITIC_LOSS]: 2.145427703857422, [REWARDS]: 0.11733788996934891, [MEAN ENTROPY]: 0.8678178787231445\n",
      "[STEP]: 1450, [TOTAL_LOSS]: 2.153773784637451, [ACTOR_LOSS]: -0.12043016403913498, [CRITIC_LOSS]: 2.283534049987793, [REWARDS]: 0.28182533383369446, [MEAN ENTROPY]: 0.9330152273178101\n",
      "[STEP]: 1500, [TOTAL_LOSS]: 0.7977847456932068, [ACTOR_LOSS]: -0.037769243121147156, [CRITIC_LOSS]: 0.845050036907196, [REWARDS]: 0.029694441705942154, [MEAN ENTROPY]: 0.9496045112609863\n",
      "[STEP]: 1550, [TOTAL_LOSS]: 1.2421165704727173, [ACTOR_LOSS]: -0.054988645017147064, [CRITIC_LOSS]: 1.3067829608917236, [REWARDS]: 0.1115313470363617, [MEAN ENTROPY]: 0.9677811861038208\n",
      "[STEP]: 1600, [TOTAL_LOSS]: 0.32593584060668945, [ACTOR_LOSS]: -0.04152984172105789, [CRITIC_LOSS]: 0.37682947516441345, [REWARDS]: 0.012505965307354927, [MEAN ENTROPY]: 0.9363806843757629\n",
      "[STEP]: 1650, [TOTAL_LOSS]: 12.469654083251953, [ACTOR_LOSS]: 0.0031852442771196365, [CRITIC_LOSS]: 12.472600936889648, [REWARDS]: 0.32396385073661804, [MEAN ENTROPY]: 0.6132338643074036\n",
      "[STEP]: 1700, [TOTAL_LOSS]: 14.262007713317871, [ACTOR_LOSS]: -0.05399729683995247, [CRITIC_LOSS]: 14.32181453704834, [REWARDS]: 0.8053447008132935, [MEAN ENTROPY]: 0.5809377431869507\n",
      "[STEP]: 1750, [TOTAL_LOSS]: 5.044653415679932, [ACTOR_LOSS]: -0.08535580337047577, [CRITIC_LOSS]: 5.136250972747803, [REWARDS]: 0.5640991926193237, [MEAN ENTROPY]: 0.6241776347160339\n",
      "[STEP]: 1800, [TOTAL_LOSS]: 14.601106643676758, [ACTOR_LOSS]: -0.15604272484779358, [CRITIC_LOSS]: 14.762214660644531, [REWARDS]: 1.246795654296875, [MEAN ENTROPY]: 0.5064747929573059\n",
      "[STEP]: 1850, [TOTAL_LOSS]: 3.9225809574127197, [ACTOR_LOSS]: -0.080633245408535, [CRITIC_LOSS]: 4.009476661682129, [REWARDS]: 0.37061476707458496, [MEAN ENTROPY]: 0.6262595057487488\n",
      "[STEP]: 1900, [TOTAL_LOSS]: 22.2468318939209, [ACTOR_LOSS]: -0.0745551735162735, [CRITIC_LOSS]: 22.32607650756836, [REWARDS]: 0.9037935733795166, [MEAN ENTROPY]: 0.4690250754356384\n",
      "[STEP]: 1950, [TOTAL_LOSS]: 13.429374694824219, [ACTOR_LOSS]: -0.07667534798383713, [CRITIC_LOSS]: 13.511545181274414, [REWARDS]: 0.7809799909591675, [MEAN ENTROPY]: 0.5495263338088989\n",
      "[STEP]: 2000, [TOTAL_LOSS]: 8.88115119934082, [ACTOR_LOSS]: -0.1520763635635376, [CRITIC_LOSS]: 9.039092063903809, [REWARDS]: 0.6857341527938843, [MEAN ENTROPY]: 0.5864360332489014\n",
      "[STEP]: 2050, [TOTAL_LOSS]: 6.949270725250244, [ACTOR_LOSS]: -0.1067676916718483, [CRITIC_LOSS]: 7.061983108520508, [REWARDS]: 0.5553452372550964, [MEAN ENTROPY]: 0.5944597721099854\n",
      "[STEP]: 2100, [TOTAL_LOSS]: 10.526368141174316, [ACTOR_LOSS]: -0.12194974720478058, [CRITIC_LOSS]: 10.65407943725586, [REWARDS]: 0.732610285282135, [MEAN ENTROPY]: 0.5761178731918335\n",
      "[STEP]: 2150, [TOTAL_LOSS]: 10.292470932006836, [ACTOR_LOSS]: -0.13372963666915894, [CRITIC_LOSS]: 10.431787490844727, [REWARDS]: 0.852821409702301, [MEAN ENTROPY]: 0.5586469173431396\n",
      "[STEP]: 2200, [TOTAL_LOSS]: 11.654783248901367, [ACTOR_LOSS]: -0.1435222327709198, [CRITIC_LOSS]: 11.803603172302246, [REWARDS]: 0.9937269687652588, [MEAN ENTROPY]: 0.5297268629074097\n",
      "[STEP]: 2250, [TOTAL_LOSS]: 5.796019077301025, [ACTOR_LOSS]: -0.046797510236501694, [CRITIC_LOSS]: 5.848730087280273, [REWARDS]: 0.6640859246253967, [MEAN ENTROPY]: 0.5913642048835754\n",
      "[STEP]: 2300, [TOTAL_LOSS]: 11.432690620422363, [ACTOR_LOSS]: -0.15465761721134186, [CRITIC_LOSS]: 11.593074798583984, [REWARDS]: 0.8302304148674011, [MEAN ENTROPY]: 0.5726829767227173\n",
      "[STEP]: 2350, [TOTAL_LOSS]: 13.58578109741211, [ACTOR_LOSS]: -0.1770024299621582, [CRITIC_LOSS]: 13.767841339111328, [REWARDS]: 1.2607629299163818, [MEAN ENTROPY]: 0.5058521628379822\n",
      "[STEP]: 2400, [TOTAL_LOSS]: 20.202566146850586, [ACTOR_LOSS]: -0.08839502930641174, [CRITIC_LOSS]: 20.296175003051758, [REWARDS]: 0.9940998554229736, [MEAN ENTROPY]: 0.5215061902999878\n",
      "[STEP]: 2450, [TOTAL_LOSS]: 11.34725284576416, [ACTOR_LOSS]: -0.15702421963214874, [CRITIC_LOSS]: 11.50984001159668, [REWARDS]: 0.9095922112464905, [MEAN ENTROPY]: 0.5562404990196228\n",
      "[STEP]: 2500, [TOTAL_LOSS]: 9.310566902160645, [ACTOR_LOSS]: -0.1358371376991272, [CRITIC_LOSS]: 9.452098846435547, [REWARDS]: 0.8809468150138855, [MEAN ENTROPY]: 0.5694822072982788\n",
      "[STEP]: 2550, [TOTAL_LOSS]: 6.823659896850586, [ACTOR_LOSS]: -0.10963736474514008, [CRITIC_LOSS]: 6.938580513000488, [REWARDS]: 0.7033352255821228, [MEAN ENTROPY]: 0.5283477306365967\n",
      "[STEP]: 2600, [TOTAL_LOSS]: 10.82443618774414, [ACTOR_LOSS]: -0.1908886879682541, [CRITIC_LOSS]: 11.02055549621582, [REWARDS]: 1.0332080125808716, [MEAN ENTROPY]: 0.5230947136878967\n",
      "[STEP]: 2650, [TOTAL_LOSS]: 12.06653881072998, [ACTOR_LOSS]: -0.08847209811210632, [CRITIC_LOSS]: 12.159860610961914, [REWARDS]: 0.7655513286590576, [MEAN ENTROPY]: 0.4849381744861603\n",
      "[STEP]: 2700, [TOTAL_LOSS]: 8.569043159484863, [ACTOR_LOSS]: -0.1500028371810913, [CRITIC_LOSS]: 8.724151611328125, [REWARDS]: 1.0025157928466797, [MEAN ENTROPY]: 0.5105711221694946\n",
      "[STEP]: 2750, [TOTAL_LOSS]: 9.279159545898438, [ACTOR_LOSS]: -0.13436275720596313, [CRITIC_LOSS]: 9.418502807617188, [REWARDS]: 0.9798949360847473, [MEAN ENTROPY]: 0.497961163520813\n",
      "[STEP]: 2800, [TOTAL_LOSS]: 10.197432518005371, [ACTOR_LOSS]: -0.045249924063682556, [CRITIC_LOSS]: 10.248185157775879, [REWARDS]: 0.6962653994560242, [MEAN ENTROPY]: 0.5502760410308838\n",
      "[STEP]: 2850, [TOTAL_LOSS]: 7.3435258865356445, [ACTOR_LOSS]: -0.11266312003135681, [CRITIC_LOSS]: 7.461612701416016, [REWARDS]: 0.5982962846755981, [MEAN ENTROPY]: 0.5423625111579895\n",
      "[STEP]: 2900, [TOTAL_LOSS]: 6.0929646492004395, [ACTOR_LOSS]: -0.12306484580039978, [CRITIC_LOSS]: 6.221652507781982, [REWARDS]: 0.5972970724105835, [MEAN ENTROPY]: 0.5622872114181519\n",
      "[STEP]: 2950, [TOTAL_LOSS]: 14.885001182556152, [ACTOR_LOSS]: -0.05701170861721039, [CRITIC_LOSS]: 14.946849822998047, [REWARDS]: 1.188506841659546, [MEAN ENTROPY]: 0.4836680293083191\n",
      "[STEP]: 3000, [TOTAL_LOSS]: 8.721970558166504, [ACTOR_LOSS]: -0.08558768779039383, [CRITIC_LOSS]: 8.812411308288574, [REWARDS]: 0.7088550925254822, [MEAN ENTROPY]: 0.48536384105682373\n",
      "[STEP]: 3050, [TOTAL_LOSS]: 7.956381320953369, [ACTOR_LOSS]: -0.15770819783210754, [CRITIC_LOSS]: 8.119373321533203, [REWARDS]: 0.7835334539413452, [MEAN ENTROPY]: 0.5283634662628174\n",
      "[STEP]: 3100, [TOTAL_LOSS]: 8.691189765930176, [ACTOR_LOSS]: -0.04508564621210098, [CRITIC_LOSS]: 8.741626739501953, [REWARDS]: 0.5019055008888245, [MEAN ENTROPY]: 0.5350924730300903\n",
      "[STEP]: 3150, [TOTAL_LOSS]: 12.08518123626709, [ACTOR_LOSS]: -0.09026587009429932, [CRITIC_LOSS]: 12.180908203125, [REWARDS]: 0.7085141539573669, [MEAN ENTROPY]: 0.5460383892059326\n",
      "[STEP]: 3200, [TOTAL_LOSS]: 5.619346618652344, [ACTOR_LOSS]: -0.09169398248195648, [CRITIC_LOSS]: 5.716312408447266, [REWARDS]: 0.875527560710907, [MEAN ENTROPY]: 0.527170717716217\n",
      "[STEP]: 3250, [TOTAL_LOSS]: 11.124088287353516, [ACTOR_LOSS]: -0.1304735541343689, [CRITIC_LOSS]: 11.259077072143555, [REWARDS]: 1.1415345668792725, [MEAN ENTROPY]: 0.4515511095523834\n",
      "[STEP]: 3300, [TOTAL_LOSS]: 11.837181091308594, [ACTOR_LOSS]: -0.013590706512331963, [CRITIC_LOSS]: 11.856206893920898, [REWARDS]: 0.694683849811554, [MEAN ENTROPY]: 0.5434755086898804\n",
      "[STEP]: 3350, [TOTAL_LOSS]: 5.613236904144287, [ACTOR_LOSS]: -0.12208795547485352, [CRITIC_LOSS]: 5.740696430206299, [REWARDS]: 1.0789885520935059, [MEAN ENTROPY]: 0.5371406674385071\n",
      "[STEP]: 3400, [TOTAL_LOSS]: 4.800755977630615, [ACTOR_LOSS]: -0.009753517806529999, [CRITIC_LOSS]: 4.816197395324707, [REWARDS]: 0.6224128007888794, [MEAN ENTROPY]: 0.5687761306762695\n",
      "[STEP]: 3450, [TOTAL_LOSS]: 9.859817504882812, [ACTOR_LOSS]: -0.007834740914404392, [CRITIC_LOSS]: 9.872589111328125, [REWARDS]: 0.931423544883728, [MEAN ENTROPY]: 0.49374866485595703\n",
      "[STEP]: 3500, [TOTAL_LOSS]: 8.666528701782227, [ACTOR_LOSS]: -0.1324470341205597, [CRITIC_LOSS]: 8.80436897277832, [REWARDS]: 0.805674135684967, [MEAN ENTROPY]: 0.539348840713501\n",
      "[STEP]: 3550, [TOTAL_LOSS]: 7.226975440979004, [ACTOR_LOSS]: -0.016943063586950302, [CRITIC_LOSS]: 7.249087810516357, [REWARDS]: 0.5735988616943359, [MEAN ENTROPY]: 0.5169476270675659\n",
      "[STEP]: 3600, [TOTAL_LOSS]: 14.641321182250977, [ACTOR_LOSS]: -0.07418233156204224, [CRITIC_LOSS]: 14.720836639404297, [REWARDS]: 0.853460431098938, [MEAN ENTROPY]: 0.5332971811294556\n",
      "[STEP]: 3650, [TOTAL_LOSS]: 6.57914924621582, [ACTOR_LOSS]: -0.09939257800579071, [CRITIC_LOSS]: 6.684788703918457, [REWARDS]: 0.6237928867340088, [MEAN ENTROPY]: 0.6247158050537109\n",
      "[STEP]: 3700, [TOTAL_LOSS]: 5.574902057647705, [ACTOR_LOSS]: -0.1349770724773407, [CRITIC_LOSS]: 5.715500354766846, [REWARDS]: 0.6277772188186646, [MEAN ENTROPY]: 0.562143087387085\n",
      "[STEP]: 3750, [TOTAL_LOSS]: 6.947923183441162, [ACTOR_LOSS]: -0.11757571250200272, [CRITIC_LOSS]: 7.070788860321045, [REWARDS]: 0.5850156545639038, [MEAN ENTROPY]: 0.5289912819862366\n",
      "[STEP]: 3800, [TOTAL_LOSS]: 7.861250400543213, [ACTOR_LOSS]: -0.035276271402835846, [CRITIC_LOSS]: 7.90202522277832, [REWARDS]: 0.6934085488319397, [MEAN ENTROPY]: 0.5498244166374207\n",
      "[STEP]: 3850, [TOTAL_LOSS]: 9.483296394348145, [ACTOR_LOSS]: -0.015132898464798927, [CRITIC_LOSS]: 9.503684997558594, [REWARDS]: 0.8032791614532471, [MEAN ENTROPY]: 0.525597095489502\n",
      "[STEP]: 3900, [TOTAL_LOSS]: 6.905008792877197, [ACTOR_LOSS]: -0.03845535218715668, [CRITIC_LOSS]: 6.948800086975098, [REWARDS]: 0.8238365650177002, [MEAN ENTROPY]: 0.5335959196090698\n",
      "[STEP]: 3950, [TOTAL_LOSS]: 4.037498474121094, [ACTOR_LOSS]: -0.056167636066675186, [CRITIC_LOSS]: 4.098925590515137, [REWARDS]: 0.5779653787612915, [MEAN ENTROPY]: 0.525952935218811\n",
      "[STEP]: 4000, [TOTAL_LOSS]: 7.680025577545166, [ACTOR_LOSS]: -0.047926124185323715, [CRITIC_LOSS]: 7.732970714569092, [REWARDS]: 0.9407721161842346, [MEAN ENTROPY]: 0.5019232034683228\n",
      "[STEP]: 4050, [TOTAL_LOSS]: 6.549534320831299, [ACTOR_LOSS]: -0.05282154306769371, [CRITIC_LOSS]: 6.607542037963867, [REWARDS]: 0.9444162845611572, [MEAN ENTROPY]: 0.5186225175857544\n",
      "[STEP]: 4100, [TOTAL_LOSS]: 9.581778526306152, [ACTOR_LOSS]: 0.0004392503760755062, [CRITIC_LOSS]: 9.58635425567627, [REWARDS]: 0.6158720254898071, [MEAN ENTROPY]: 0.501510739326477\n",
      "[STEP]: 4150, [TOTAL_LOSS]: 9.207013130187988, [ACTOR_LOSS]: -0.06367896497249603, [CRITIC_LOSS]: 9.275985717773438, [REWARDS]: 0.8631629943847656, [MEAN ENTROPY]: 0.529380202293396\n",
      "[STEP]: 4200, [TOTAL_LOSS]: 8.267646789550781, [ACTOR_LOSS]: -0.06149270385503769, [CRITIC_LOSS]: 8.335759162902832, [REWARDS]: 0.5170530676841736, [MEAN ENTROPY]: 0.6619088053703308\n",
      "[STEP]: 4250, [TOTAL_LOSS]: 4.967103958129883, [ACTOR_LOSS]: -0.1020168662071228, [CRITIC_LOSS]: 5.074297904968262, [REWARDS]: 1.0438342094421387, [MEAN ENTROPY]: 0.5176846385002136\n",
      "[STEP]: 4300, [TOTAL_LOSS]: 5.719757556915283, [ACTOR_LOSS]: -0.06893572211265564, [CRITIC_LOSS]: 5.793460369110107, [REWARDS]: 1.1117193698883057, [MEAN ENTROPY]: 0.47670885920524597\n",
      "[STEP]: 4350, [TOTAL_LOSS]: 8.534886360168457, [ACTOR_LOSS]: 0.003335054498165846, [CRITIC_LOSS]: 8.53681755065918, [REWARDS]: 1.000286340713501, [MEAN ENTROPY]: 0.5266103148460388\n",
      "[STEP]: 4400, [TOTAL_LOSS]: 7.613463878631592, [ACTOR_LOSS]: -0.12781338393688202, [CRITIC_LOSS]: 7.74600887298584, [REWARDS]: 0.7976271510124207, [MEAN ENTROPY]: 0.47316059470176697\n",
      "[STEP]: 4450, [TOTAL_LOSS]: 5.868860721588135, [ACTOR_LOSS]: -0.1334792673587799, [CRITIC_LOSS]: 6.007673263549805, [REWARDS]: 0.7002800703048706, [MEAN ENTROPY]: 0.5333279371261597\n",
      "[STEP]: 4500, [TOTAL_LOSS]: 5.604776859283447, [ACTOR_LOSS]: -0.10531780123710632, [CRITIC_LOSS]: 5.714308738708496, [REWARDS]: 1.0812280178070068, [MEAN ENTROPY]: 0.42140740156173706\n",
      "[STEP]: 4550, [TOTAL_LOSS]: 14.949200630187988, [ACTOR_LOSS]: -0.03448960930109024, [CRITIC_LOSS]: 14.987653732299805, [REWARDS]: 0.857490062713623, [MEAN ENTROPY]: 0.39631593227386475\n",
      "[STEP]: 4600, [TOTAL_LOSS]: 8.477792739868164, [ACTOR_LOSS]: -0.040800873190164566, [CRITIC_LOSS]: 8.523235321044922, [REWARDS]: 1.1353144645690918, [MEAN ENTROPY]: 0.46415290236473083\n",
      "[STEP]: 4650, [TOTAL_LOSS]: 10.292008399963379, [ACTOR_LOSS]: -0.050893768668174744, [CRITIC_LOSS]: 10.347997665405273, [REWARDS]: 0.9669898748397827, [MEAN ENTROPY]: 0.5095807909965515\n",
      "[STEP]: 4700, [TOTAL_LOSS]: 6.691579341888428, [ACTOR_LOSS]: -0.03895142301917076, [CRITIC_LOSS]: 6.735869884490967, [REWARDS]: 0.5354417562484741, [MEAN ENTROPY]: 0.533915102481842\n",
      "[STEP]: 4750, [TOTAL_LOSS]: 18.92744255065918, [ACTOR_LOSS]: -0.05683401972055435, [CRITIC_LOSS]: 18.988218307495117, [REWARDS]: 1.157380223274231, [MEAN ENTROPY]: 0.39433008432388306\n",
      "[STEP]: 4800, [TOTAL_LOSS]: 5.565333843231201, [ACTOR_LOSS]: -0.07511459290981293, [CRITIC_LOSS]: 5.6457414627075195, [REWARDS]: 0.6295479536056519, [MEAN ENTROPY]: 0.5293076634407043\n",
      "[STEP]: 4850, [TOTAL_LOSS]: 5.830704212188721, [ACTOR_LOSS]: -0.12093561887741089, [CRITIC_LOSS]: 5.9576640129089355, [REWARDS]: 0.6950075626373291, [MEAN ENTROPY]: 0.6024405360221863\n"
     ]
    }
   ],
   "source": [
    "obs, _ = envs.reset()\n",
    "obs = torch.Tensor(obs).to(device)\n",
    "done = torch.zeros(MAX_ENVS, device=device)\n",
    "\n",
    "for update in range(1, NUM_UPDATES + 1):\n",
    "\n",
    "    log_probs_list = []\n",
    "    entropies_list = []\n",
    "    values_list = []\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        \n",
    "        dones_storage[step] = done\n",
    "\n",
    "        action, log_probs, entropy = actor_net.get_action(obs)\n",
    "\n",
    "        values = critic_net(obs).flatten()\n",
    "        log_probs_list.append(log_probs)\n",
    "        entropies_list.append(entropy)\n",
    "        values_list.append(values)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "\n",
    "        rewards_storage[step] = torch.as_tensor(reward, dtype=torch.float32, device=device)\n",
    "        done = torch.as_tensor(np.logical_or(terminated, truncated), dtype=torch.float32, device=device)\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
    "    \n",
    "    log_probs_storage = torch.stack(log_probs_list)   # (T, N)\n",
    "    entropies = torch.stack(entropies_list)          # (T, N)\n",
    "    values_storage = torch.stack(values_list) \n",
    "\n",
    "    returns = torch.zeros_like(rewards_storage, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_value = critic_net(obs).squeeze() * (1.0 - done)\n",
    "    \n",
    "    gt_next_state = next_value\n",
    "    for t in reversed(range(MAX_STEPS)):\n",
    "        returns[t] = rewards_storage[t] + GAMMA * gt_next_state \n",
    "        gt_next_state = returns[t] * (1 - dones_storage[t])\n",
    "\n",
    "    returns_flat = returns.view(-1)\n",
    "    values_flat = values_storage.view(-1)\n",
    "    log_probs_flat = log_probs_storage.view(-1)\n",
    "    entropies_flat = entropies.view(-1)\n",
    "    \n",
    "    advantages = (returns_flat - values_flat).detach()\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    policy_loss = -(log_probs_flat * advantages).mean()\n",
    "    critic_loss = VALUE_COEFF * F.smooth_l1_loss(values_flat, returns_flat)\n",
    "    entropy_loss = -ENTROPY_COEFF * entropies_flat.mean()\n",
    "\n",
    "    total_loss = policy_loss + critic_loss + entropy_loss\n",
    "    optimizer.zero_grad()\n",
    "            \n",
    "    total_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(list(actor_net.parameters()) + list(critic_net.parameters()), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update % LOG_EVERY_N_STEPS == 0:\n",
    "        print(f\"[STEP]: {update}, [TOTAL_LOSS]: {total_loss.item()}, [ACTOR_LOSS]: {policy_loss.item()}, [CRITIC_LOSS]: {critic_loss.item()}, [REWARDS]: {rewards_storage.mean().item()}, [MEAN ENTROPY]: {entropies_flat.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d11893f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, num_eval_eps=1, record=False):\n",
    "    eval_env = make_env(99, is_eval=True)()\n",
    "    eval_env.action_space.seed(SEED)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    returns = []\n",
    "    frames = []\n",
    "\n",
    "    model.eval()\n",
    "    for eps in range(num_eval_eps):\n",
    "        obs, _ = eval_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "   \n",
    "        while not done:\n",
    "            if record:\n",
    "                frame = eval_env.render()\n",
    "                frames.append(frame)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                act, _, _ = model.get_action(torch.tensor(obs, device=device, dtype=torch.float32).unsqueeze(0))\n",
    "                # print(act.cpu().numpy().item())\n",
    "                obs, reward, terminated, truncated, _ = eval_env.step(act.cpu().numpy().item())\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "        returns.append(episode_reward)\n",
    "    \n",
    "    eval_env.close()\n",
    "    model.train()\n",
    "    return returns, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2776616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "train_video_path = rf\"B:\\Pytorch\\RL\\videos\\a2c_lunar_lander.mp4\"\n",
    "returns, frames = evaluate(actor_net, device, record=True, num_eval_eps=1)\n",
    "\n",
    "if frames and len(frames) > 0:\n",
    "    imageio.mimsave(\n",
    "        train_video_path,\n",
    "        frames,\n",
    "        fps=30,\n",
    "        codec='libx264',\n",
    "        macro_block_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c0a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
