{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b983d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import gym, random\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f714309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: [ 0.04262742 -0.02276057  0.01158579  0.04954371]\n",
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "state, _ = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a7210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_features=4, num_actions=2, hidden_features=128) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=hidden_features)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_features, out_features=hidden_features * 2)\n",
    "        self.fc3 = nn.Linear(in_features=hidden_features * 2, out_features=num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e5696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_buffer_size=10000, batch_size=16):\n",
    "        self.buffer = deque(maxlen=max_buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add_sample(self, element: tuple):\n",
    "        self.buffer.append(element)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        return random.sample(self.buffer, k=self.batch_size) if len(self.buffer) > self.batch_size else list(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d3d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BUFFER_SIZE = 10000 \n",
    "N_TRAINING_STEPS = 50000\n",
    "N_START_LEARNING = 5000\n",
    "TARGET_UPDATE_FREQUENCY = 128 \n",
    "LEARNING_RATE = 5e-4\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 128\n",
    "EPSILON = 0.95\n",
    "DECAY = 0.995 \n",
    "MIN_EPSILON = 0.01\n",
    "epsilon = 1.0\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "q_net = QNetwork(num_features=state_size, num_actions=action_size)\n",
    "target_q_net = QNetwork(num_features=state_size, num_actions=action_size)\n",
    "buffer = ReplayBuffer(batch_size=BATCH_SIZE, max_buffer_size=N_BUFFER_SIZE)\n",
    "\n",
    "target_q_net.load_state_dict(q_net.state_dict())\n",
    "optimizer = torch.optim.AdamW(q_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25df0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, q_net, action_size, epsilon=0.5):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    else:\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = q_net(state_tensor)\n",
    "            return torch.argmax(logits, dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3787fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer filled so far: 1\n",
      "Buffer filled so far: 101\n",
      "Buffer filled so far: 201\n",
      "Buffer filled so far: 301\n",
      "Buffer filled so far: 401\n",
      "Buffer filled so far: 501\n",
      "Buffer filled so far: 601\n",
      "Buffer filled so far: 701\n",
      "Buffer filled so far: 801\n",
      "Buffer filled so far: 901\n",
      "Buffer filled so far: 1001\n",
      "Buffer filled so far: 1101\n",
      "Buffer filled so far: 1201\n",
      "Buffer filled so far: 1301\n",
      "Buffer filled so far: 1401\n",
      "Buffer filled so far: 1501\n",
      "Buffer filled so far: 1601\n",
      "Buffer filled so far: 1701\n",
      "Buffer filled so far: 1801\n",
      "Buffer filled so far: 1901\n",
      "Buffer filled so far: 2001\n",
      "Buffer filled so far: 2101\n",
      "Buffer filled so far: 2201\n",
      "Buffer filled so far: 2301\n",
      "Buffer filled so far: 2401\n",
      "Buffer filled so far: 2501\n",
      "Buffer filled so far: 2601\n",
      "Buffer filled so far: 2701\n",
      "Buffer filled so far: 2801\n",
      "Buffer filled so far: 2901\n",
      "Buffer filled so far: 3001\n",
      "Buffer filled so far: 3101\n",
      "Buffer filled so far: 3201\n",
      "Buffer filled so far: 3301\n",
      "Buffer filled so far: 3401\n",
      "Buffer filled so far: 3501\n",
      "Buffer filled so far: 3601\n",
      "Buffer filled so far: 3701\n",
      "Buffer filled so far: 3801\n",
      "Buffer filled so far: 3901\n",
      "Buffer filled so far: 4001\n",
      "Buffer filled so far: 4101\n",
      "Buffer filled so far: 4201\n",
      "Buffer filled so far: 4301\n",
      "Buffer filled so far: 4401\n",
      "Buffer filled so far: 4501\n",
      "Buffer filled so far: 4601\n",
      "Buffer filled so far: 4701\n",
      "Buffer filled so far: 4801\n",
      "Buffer filled so far: 4901\n",
      "Buffer filled so far: 5001\n",
      "Buffer filled so far: 5101\n",
      "[STEP]: 5120, [LOSS]: 0.0002, [AVG_LOSS]: 0.0076, [EPISODE]: 236, [REWARD]: 2.0, [EPSILON]: 0.9416\n",
      "Buffer filled so far: 5201\n",
      "[STEP]: 5248, [LOSS]: 0.0324, [AVG_LOSS]: 0.0231, [EPISODE]: 241, [REWARD]: 18.0, [EPSILON]: 0.8867\n",
      "Buffer filled so far: 5301\n",
      "[STEP]: 5376, [LOSS]: 0.0349, [AVG_LOSS]: 0.0581, [EPISODE]: 246, [REWARD]: 43.0, [EPSILON]: 0.8307\n",
      "Buffer filled so far: 5401\n",
      "Buffer filled so far: 5501\n",
      "[STEP]: 5504, [LOSS]: 0.0664, [AVG_LOSS]: 0.0805, [EPISODE]: 249, [REWARD]: 3.0, [EPSILON]: 0.7783\n",
      "Buffer filled so far: 5601\n",
      "[STEP]: 5632, [LOSS]: 0.1128, [AVG_LOSS]: 0.1101, [EPISODE]: 250, [REWARD]: 28.0, [EPSILON]: 0.7292\n",
      "Buffer filled so far: 5701\n",
      "[STEP]: 5760, [LOSS]: 0.1199, [AVG_LOSS]: 0.1276, [EPISODE]: 252, [REWARD]: 26.0, [EPSILON]: 0.6832\n",
      "Buffer filled so far: 5801\n",
      "[STEP]: 5888, [LOSS]: 0.1282, [AVG_LOSS]: 0.1501, [EPISODE]: 252, [REWARD]: 154.0, [EPSILON]: 0.6433\n",
      "Buffer filled so far: 5901\n",
      "Buffer filled so far: 6001\n",
      "[STEP]: 6016, [LOSS]: 0.1921, [AVG_LOSS]: 0.1727, [EPISODE]: 254, [REWARD]: 3.0, [EPSILON]: 0.6027\n",
      "Buffer filled so far: 6101\n",
      "[STEP]: 6144, [LOSS]: 0.2101, [AVG_LOSS]: 0.1846, [EPISODE]: 256, [REWARD]: 99.0, [EPSILON]: 0.5647\n",
      "Buffer filled so far: 6201\n",
      "[STEP]: 6272, [LOSS]: 0.4855, [AVG_LOSS]: 0.1848, [EPISODE]: 258, [REWARD]: 67.0, [EPSILON]: 0.5291\n",
      "Buffer filled so far: 6301\n",
      "Buffer filled so far: 6401\n",
      "[STEP]: 6400, [LOSS]: 0.1244, [AVG_LOSS]: 0.2177, [EPISODE]: 259, [REWARD]: 40.0, [EPSILON]: 0.4957\n",
      "Buffer filled so far: 6501\n",
      "[STEP]: 6528, [LOSS]: 0.4903, [AVG_LOSS]: 0.2229, [EPISODE]: 259, [REWARD]: 168.0, [EPSILON]: 0.4668\n",
      "Buffer filled so far: 6601\n",
      "[STEP]: 6656, [LOSS]: 0.2171, [AVG_LOSS]: 0.2304, [EPISODE]: 262, [REWARD]: 5.0, [EPSILON]: 0.4373\n",
      "Buffer filled so far: 6701\n",
      "[STEP]: 6784, [LOSS]: 0.4284, [AVG_LOSS]: 0.2587, [EPISODE]: 263, [REWARD]: 80.0, [EPSILON]: 0.4097\n",
      "Buffer filled so far: 6801\n",
      "Buffer filled so far: 6901\n",
      "[STEP]: 6912, [LOSS]: 0.1841, [AVG_LOSS]: 0.2761, [EPISODE]: 264, [REWARD]: 111.0, [EPSILON]: 0.3839\n",
      "Buffer filled so far: 7001\n",
      "[STEP]: 7040, [LOSS]: 0.4664, [AVG_LOSS]: 0.2641, [EPISODE]: 265, [REWARD]: 18.0, [EPSILON]: 0.3597\n",
      "Buffer filled so far: 7101\n",
      "[STEP]: 7168, [LOSS]: 0.0170, [AVG_LOSS]: 0.2701, [EPISODE]: 265, [REWARD]: 146.0, [EPSILON]: 0.3387\n",
      "Buffer filled so far: 7201\n",
      "[STEP]: 7296, [LOSS]: 0.2263, [AVG_LOSS]: 0.3065, [EPISODE]: 266, [REWARD]: 106.0, [EPSILON]: 0.3173\n",
      "Buffer filled so far: 7301\n",
      "Buffer filled so far: 7401\n",
      "[STEP]: 7424, [LOSS]: 0.6475, [AVG_LOSS]: 0.2750, [EPISODE]: 267, [REWARD]: 1.0, [EPSILON]: 0.2973\n",
      "Buffer filled so far: 7501\n",
      "[STEP]: 7552, [LOSS]: 0.6361, [AVG_LOSS]: 0.3193, [EPISODE]: 267, [REWARD]: 129.0, [EPSILON]: 0.2785\n",
      "Buffer filled so far: 7601\n",
      "[STEP]: 7680, [LOSS]: 0.5079, [AVG_LOSS]: 0.3178, [EPISODE]: 268, [REWARD]: 28.0, [EPSILON]: 0.2610\n",
      "Buffer filled so far: 7701\n",
      "Buffer filled so far: 7801\n",
      "[STEP]: 7808, [LOSS]: 0.6757, [AVG_LOSS]: 0.3491, [EPISODE]: 269, [REWARD]: 85.0, [EPSILON]: 0.2457\n",
      "Buffer filled so far: 7901\n",
      "[STEP]: 7936, [LOSS]: 0.2219, [AVG_LOSS]: 0.3551, [EPISODE]: 269, [REWARD]: 213.0, [EPSILON]: 0.2302\n",
      "Buffer filled so far: 8001\n",
      "[STEP]: 8064, [LOSS]: 0.1808, [AVG_LOSS]: 0.3522, [EPISODE]: 270, [REWARD]: 81.0, [EPSILON]: 0.2157\n",
      "Buffer filled so far: 8101\n",
      "[STEP]: 8192, [LOSS]: 0.2559, [AVG_LOSS]: 0.4092, [EPISODE]: 271, [REWARD]: 52.0, [EPSILON]: 0.2021\n",
      "Buffer filled so far: 8201\n",
      "Buffer filled so far: 8301\n",
      "[STEP]: 8320, [LOSS]: 0.6442, [AVG_LOSS]: 0.4002, [EPISODE]: 272, [REWARD]: 19.0, [EPSILON]: 0.1893\n",
      "Buffer filled so far: 8401\n",
      "[STEP]: 8448, [LOSS]: 0.2499, [AVG_LOSS]: 0.3423, [EPISODE]: 273, [REWARD]: 1.0, [EPSILON]: 0.1783\n",
      "Buffer filled so far: 8501\n",
      "[STEP]: 8576, [LOSS]: 0.4707, [AVG_LOSS]: 0.3682, [EPISODE]: 273, [REWARD]: 129.0, [EPSILON]: 0.1670\n",
      "Buffer filled so far: 8601\n",
      "Buffer filled so far: 8701\n",
      "[STEP]: 8704, [LOSS]: 0.2534, [AVG_LOSS]: 0.3574, [EPISODE]: 274, [REWARD]: 54.0, [EPSILON]: 0.1565\n",
      "Buffer filled so far: 8801\n",
      "[STEP]: 8832, [LOSS]: 0.5299, [AVG_LOSS]: 0.4096, [EPISODE]: 274, [REWARD]: 182.0, [EPSILON]: 0.1466\n",
      "Buffer filled so far: 8901\n",
      "[STEP]: 8960, [LOSS]: 0.4944, [AVG_LOSS]: 0.3583, [EPISODE]: 275, [REWARD]: 80.0, [EPSILON]: 0.1374\n",
      "Buffer filled so far: 9001\n",
      "[STEP]: 9088, [LOSS]: 0.2585, [AVG_LOSS]: 0.3491, [EPISODE]: 275, [REWARD]: 208.0, [EPSILON]: 0.1294\n",
      "Buffer filled so far: 9101\n",
      "Buffer filled so far: 9201\n",
      "[STEP]: 9216, [LOSS]: 0.3112, [AVG_LOSS]: 0.3867, [EPISODE]: 277, [REWARD]: 69.0, [EPSILON]: 0.1212\n",
      "Buffer filled so far: 9301\n",
      "[STEP]: 9344, [LOSS]: 0.2296, [AVG_LOSS]: 0.3947, [EPISODE]: 277, [REWARD]: 197.0, [EPSILON]: 0.1136\n",
      "Buffer filled so far: 9401\n",
      "[STEP]: 9472, [LOSS]: 0.9601, [AVG_LOSS]: 0.3483, [EPISODE]: 278, [REWARD]: 89.0, [EPSILON]: 0.1064\n",
      "Buffer filled so far: 9501\n",
      "Buffer filled so far: 9601\n",
      "[STEP]: 9600, [LOSS]: 0.3677, [AVG_LOSS]: 0.3778, [EPISODE]: 278, [REWARD]: 217.0, [EPSILON]: 0.0997\n",
      "Buffer filled so far: 9701\n",
      "[STEP]: 9728, [LOSS]: 0.2302, [AVG_LOSS]: 0.3487, [EPISODE]: 280, [REWARD]: 74.0, [EPSILON]: 0.0939\n",
      "Buffer filled so far: 9801\n",
      "[STEP]: 9856, [LOSS]: 0.4093, [AVG_LOSS]: 0.3694, [EPISODE]: 280, [REWARD]: 202.0, [EPSILON]: 0.0879\n",
      "Buffer filled so far: 9901\n",
      "[STEP]: 9984, [LOSS]: 0.6989, [AVG_LOSS]: 0.4077, [EPISODE]: 281, [REWARD]: 71.0, [EPSILON]: 0.0824\n",
      "[STEP]: 10112, [LOSS]: 0.5221, [AVG_LOSS]: 0.3896, [EPISODE]: 282, [REWARD]: 13.0, [EPSILON]: 0.0772\n",
      "[STEP]: 10240, [LOSS]: 0.1894, [AVG_LOSS]: 0.4123, [EPISODE]: 282, [REWARD]: 141.0, [EPSILON]: 0.0723\n",
      "[STEP]: 10368, [LOSS]: 0.6421, [AVG_LOSS]: 0.4327, [EPISODE]: 283, [REWARD]: 102.0, [EPSILON]: 0.0681\n",
      "[STEP]: 10496, [LOSS]: 0.2761, [AVG_LOSS]: 0.4811, [EPISODE]: 283, [REWARD]: 230.0, [EPSILON]: 0.0638\n",
      "[STEP]: 10624, [LOSS]: 0.4316, [AVG_LOSS]: 0.4215, [EPISODE]: 283, [REWARD]: 358.0, [EPSILON]: 0.0598\n",
      "[STEP]: 10752, [LOSS]: 0.3325, [AVG_LOSS]: 0.4549, [EPISODE]: 284, [REWARD]: 70.0, [EPSILON]: 0.0560\n",
      "[STEP]: 10880, [LOSS]: 0.1492, [AVG_LOSS]: 0.4242, [EPISODE]: 284, [REWARD]: 198.0, [EPSILON]: 0.0525\n",
      "[STEP]: 11008, [LOSS]: 0.4735, [AVG_LOSS]: 0.4289, [EPISODE]: 285, [REWARD]: 5.0, [EPSILON]: 0.0494\n",
      "[STEP]: 11136, [LOSS]: 0.3557, [AVG_LOSS]: 0.4618, [EPISODE]: 285, [REWARD]: 133.0, [EPSILON]: 0.0463\n",
      "[STEP]: 11264, [LOSS]: 0.3500, [AVG_LOSS]: 0.4378, [EPISODE]: 286, [REWARD]: 49.0, [EPSILON]: 0.0434\n",
      "[STEP]: 11392, [LOSS]: 1.0502, [AVG_LOSS]: 0.4351, [EPISODE]: 287, [REWARD]: 4.0, [EPSILON]: 0.0406\n",
      "[STEP]: 11520, [LOSS]: 0.5253, [AVG_LOSS]: 0.4736, [EPISODE]: 287, [REWARD]: 132.0, [EPSILON]: 0.0381\n",
      "[STEP]: 11648, [LOSS]: 0.0714, [AVG_LOSS]: 0.4279, [EPISODE]: 287, [REWARD]: 260.0, [EPSILON]: 0.0359\n",
      "[STEP]: 11776, [LOSS]: 0.8286, [AVG_LOSS]: 0.4043, [EPISODE]: 288, [REWARD]: 20.0, [EPSILON]: 0.0336\n",
      "[STEP]: 11904, [LOSS]: 0.4097, [AVG_LOSS]: 0.4309, [EPISODE]: 288, [REWARD]: 148.0, [EPSILON]: 0.0315\n",
      "[STEP]: 12032, [LOSS]: 0.2383, [AVG_LOSS]: 0.4604, [EPISODE]: 290, [REWARD]: 4.0, [EPSILON]: 0.0295\n",
      "[STEP]: 12160, [LOSS]: 0.4362, [AVG_LOSS]: 0.5012, [EPISODE]: 290, [REWARD]: 132.0, [EPSILON]: 0.0276\n",
      "[STEP]: 12288, [LOSS]: 0.7499, [AVG_LOSS]: 0.4251, [EPISODE]: 291, [REWARD]: 108.0, [EPSILON]: 0.0260\n",
      "[STEP]: 12416, [LOSS]: 0.7011, [AVG_LOSS]: 0.4992, [EPISODE]: 292, [REWARD]: 107.0, [EPSILON]: 0.0244\n",
      "[STEP]: 12544, [LOSS]: 0.4049, [AVG_LOSS]: 0.4071, [EPISODE]: 293, [REWARD]: 119.0, [EPSILON]: 0.0228\n",
      "[STEP]: 12672, [LOSS]: 0.3557, [AVG_LOSS]: 0.4111, [EPISODE]: 294, [REWARD]: 104.0, [EPSILON]: 0.0214\n",
      "[STEP]: 12800, [LOSS]: 0.3676, [AVG_LOSS]: 0.3657, [EPISODE]: 296, [REWARD]: 8.0, [EPSILON]: 0.0200\n",
      "[STEP]: 12928, [LOSS]: 0.3850, [AVG_LOSS]: 0.4455, [EPISODE]: 297, [REWARD]: 29.0, [EPSILON]: 0.0189\n",
      "[STEP]: 13056, [LOSS]: 0.4464, [AVG_LOSS]: 0.4212, [EPISODE]: 298, [REWARD]: 53.0, [EPSILON]: 0.0177\n",
      "[STEP]: 13184, [LOSS]: 0.6408, [AVG_LOSS]: 0.3876, [EPISODE]: 299, [REWARD]: 85.0, [EPSILON]: 0.0166\n",
      "[STEP]: 13312, [LOSS]: 0.6734, [AVG_LOSS]: 0.3687, [EPISODE]: 301, [REWARD]: 16.0, [EPSILON]: 0.0155\n",
      "[STEP]: 13440, [LOSS]: 0.7223, [AVG_LOSS]: 0.3540, [EPISODE]: 302, [REWARD]: 37.0, [EPSILON]: 0.0145\n",
      "[STEP]: 13568, [LOSS]: 0.0693, [AVG_LOSS]: 0.3778, [EPISODE]: 303, [REWARD]: 55.0, [EPSILON]: 0.0137\n",
      "[STEP]: 13696, [LOSS]: 0.3250, [AVG_LOSS]: 0.4243, [EPISODE]: 304, [REWARD]: 84.0, [EPSILON]: 0.0128\n",
      "[STEP]: 13824, [LOSS]: 0.2600, [AVG_LOSS]: 0.3905, [EPISODE]: 305, [REWARD]: 106.0, [EPSILON]: 0.0120\n",
      "[STEP]: 13952, [LOSS]: 0.4928, [AVG_LOSS]: 0.3837, [EPISODE]: 307, [REWARD]: 5.0, [EPSILON]: 0.0113\n",
      "[STEP]: 14080, [LOSS]: 0.9975, [AVG_LOSS]: 0.3285, [EPISODE]: 308, [REWARD]: 19.0, [EPSILON]: 0.0106\n",
      "[STEP]: 14208, [LOSS]: 0.3301, [AVG_LOSS]: 0.3716, [EPISODE]: 309, [REWARD]: 51.0, [EPSILON]: 0.0100\n",
      "[STEP]: 14336, [LOSS]: 0.1358, [AVG_LOSS]: 0.3467, [EPISODE]: 310, [REWARD]: 62.0, [EPSILON]: 0.0100\n",
      "[STEP]: 14464, [LOSS]: 0.6555, [AVG_LOSS]: 0.3712, [EPISODE]: 311, [REWARD]: 71.0, [EPSILON]: 0.0100\n",
      "[STEP]: 14592, [LOSS]: 0.4382, [AVG_LOSS]: 0.3370, [EPISODE]: 312, [REWARD]: 90.0, [EPSILON]: 0.0100\n",
      "[STEP]: 14720, [LOSS]: 0.0281, [AVG_LOSS]: 0.3347, [EPISODE]: 313, [REWARD]: 112.0, [EPSILON]: 0.0100\n",
      "[STEP]: 14848, [LOSS]: 0.7019, [AVG_LOSS]: 0.3481, [EPISODE]: 315, [REWARD]: 7.0, [EPSILON]: 0.0100\n",
      "[STEP]: 14976, [LOSS]: 0.2861, [AVG_LOSS]: 0.3481, [EPISODE]: 316, [REWARD]: 22.0, [EPSILON]: 0.0100\n",
      "[STEP]: 15104, [LOSS]: 0.0127, [AVG_LOSS]: 0.3577, [EPISODE]: 317, [REWARD]: 12.0, [EPSILON]: 0.0100\n",
      "[STEP]: 15232, [LOSS]: 0.2936, [AVG_LOSS]: 0.3199, [EPISODE]: 318, [REWARD]: 25.0, [EPSILON]: 0.0100\n",
      "[STEP]: 15360, [LOSS]: 0.3758, [AVG_LOSS]: 0.3061, [EPISODE]: 318, [REWARD]: 153.0, [EPSILON]: 0.0100\n",
      "[STEP]: 15488, [LOSS]: 0.0117, [AVG_LOSS]: 0.2938, [EPISODE]: 320, [REWARD]: 12.0, [EPSILON]: 0.0100\n",
      "[STEP]: 15616, [LOSS]: 0.3048, [AVG_LOSS]: 0.3281, [EPISODE]: 321, [REWARD]: 33.0, [EPSILON]: 0.0100\n",
      "[STEP]: 15744, [LOSS]: 0.7054, [AVG_LOSS]: 0.2959, [EPISODE]: 322, [REWARD]: 63.0, [EPSILON]: 0.0100\n",
      "[STEP]: 15872, [LOSS]: 0.7423, [AVG_LOSS]: 0.2589, [EPISODE]: 323, [REWARD]: 84.0, [EPSILON]: 0.0100\n",
      "[STEP]: 16000, [LOSS]: 0.0233, [AVG_LOSS]: 0.3277, [EPISODE]: 324, [REWARD]: 99.0, [EPSILON]: 0.0100\n",
      "[STEP]: 16128, [LOSS]: 0.0223, [AVG_LOSS]: 0.2710, [EPISODE]: 326, [REWARD]: 7.0, [EPSILON]: 0.0100\n",
      "[STEP]: 16256, [LOSS]: 0.0628, [AVG_LOSS]: 0.3012, [EPISODE]: 327, [REWARD]: 30.0, [EPSILON]: 0.0100\n",
      "[STEP]: 16384, [LOSS]: 0.0122, [AVG_LOSS]: 0.2744, [EPISODE]: 328, [REWARD]: 46.0, [EPSILON]: 0.0100\n",
      "[STEP]: 16512, [LOSS]: 0.0128, [AVG_LOSS]: 0.2980, [EPISODE]: 329, [REWARD]: 68.0, [EPSILON]: 0.0100\n",
      "[STEP]: 16640, [LOSS]: 0.6390, [AVG_LOSS]: 0.2542, [EPISODE]: 330, [REWARD]: 88.0, [EPSILON]: 0.0100\n",
      "[STEP]: 16768, [LOSS]: 0.2912, [AVG_LOSS]: 0.2753, [EPISODE]: 331, [REWARD]: 97.0, [EPSILON]: 0.0100\n",
      "[STEP]: 16896, [LOSS]: 0.0160, [AVG_LOSS]: 0.2518, [EPISODE]: 333, [REWARD]: 4.0, [EPSILON]: 0.0100\n",
      "[STEP]: 17024, [LOSS]: 0.6559, [AVG_LOSS]: 0.2655, [EPISODE]: 334, [REWARD]: 25.0, [EPSILON]: 0.0100\n",
      "[STEP]: 17152, [LOSS]: 0.2603, [AVG_LOSS]: 0.2644, [EPISODE]: 335, [REWARD]: 47.0, [EPSILON]: 0.0100\n",
      "[STEP]: 17280, [LOSS]: 0.2705, [AVG_LOSS]: 0.2731, [EPISODE]: 336, [REWARD]: 66.0, [EPSILON]: 0.0100\n",
      "[STEP]: 17408, [LOSS]: 0.0090, [AVG_LOSS]: 0.3046, [EPISODE]: 337, [REWARD]: 89.0, [EPSILON]: 0.0100\n",
      "[STEP]: 17536, [LOSS]: 0.2573, [AVG_LOSS]: 0.2471, [EPISODE]: 338, [REWARD]: 109.0, [EPSILON]: 0.0100\n",
      "[STEP]: 17664, [LOSS]: 1.3470, [AVG_LOSS]: 0.2709, [EPISODE]: 340, [REWARD]: 11.0, [EPSILON]: 0.0100\n",
      "[STEP]: 17792, [LOSS]: 0.0110, [AVG_LOSS]: 0.3133, [EPISODE]: 341, [REWARD]: 29.0, [EPSILON]: 0.0100\n",
      "[STEP]: 17920, [LOSS]: 0.2517, [AVG_LOSS]: 0.2380, [EPISODE]: 342, [REWARD]: 41.0, [EPSILON]: 0.0100\n",
      "[STEP]: 18048, [LOSS]: 0.9526, [AVG_LOSS]: 0.3060, [EPISODE]: 343, [REWARD]: 61.0, [EPSILON]: 0.0100\n",
      "[STEP]: 18176, [LOSS]: 0.0151, [AVG_LOSS]: 0.2275, [EPISODE]: 344, [REWARD]: 76.0, [EPSILON]: 0.0100\n",
      "[STEP]: 18304, [LOSS]: 0.2517, [AVG_LOSS]: 0.2927, [EPISODE]: 347, [REWARD]: 72.0, [EPSILON]: 0.0100\n",
      "[STEP]: 18432, [LOSS]: 0.2274, [AVG_LOSS]: 0.2097, [EPISODE]: 348, [REWARD]: 90.0, [EPSILON]: 0.0100\n",
      "[STEP]: 18560, [LOSS]: 0.3566, [AVG_LOSS]: 0.2755, [EPISODE]: 350, [REWARD]: 2.0, [EPSILON]: 0.0100\n",
      "[STEP]: 18688, [LOSS]: 0.2806, [AVG_LOSS]: 0.2538, [EPISODE]: 351, [REWARD]: 16.0, [EPSILON]: 0.0100\n",
      "[STEP]: 18816, [LOSS]: 0.0246, [AVG_LOSS]: 0.2639, [EPISODE]: 352, [REWARD]: 31.0, [EPSILON]: 0.0100\n",
      "[STEP]: 18944, [LOSS]: 0.6548, [AVG_LOSS]: 0.2202, [EPISODE]: 353, [REWARD]: 47.0, [EPSILON]: 0.0100\n",
      "[STEP]: 19072, [LOSS]: 0.2053, [AVG_LOSS]: 0.2540, [EPISODE]: 354, [REWARD]: 38.0, [EPSILON]: 0.0100\n",
      "[STEP]: 19200, [LOSS]: 0.0130, [AVG_LOSS]: 0.2396, [EPISODE]: 355, [REWARD]: 37.0, [EPSILON]: 0.0100\n",
      "[STEP]: 19328, [LOSS]: 0.3498, [AVG_LOSS]: 0.1984, [EPISODE]: 356, [REWARD]: 42.0, [EPSILON]: 0.0100\n",
      "[STEP]: 19456, [LOSS]: 0.0158, [AVG_LOSS]: 0.2107, [EPISODE]: 357, [REWARD]: 34.0, [EPSILON]: 0.0100\n",
      "[STEP]: 19584, [LOSS]: 0.5204, [AVG_LOSS]: 0.2419, [EPISODE]: 358, [REWARD]: 36.0, [EPSILON]: 0.0100\n",
      "[STEP]: 19712, [LOSS]: 0.0104, [AVG_LOSS]: 0.2130, [EPISODE]: 358, [REWARD]: 164.0, [EPSILON]: 0.0100\n",
      "[STEP]: 19840, [LOSS]: 0.0253, [AVG_LOSS]: 0.2025, [EPISODE]: 359, [REWARD]: 103.0, [EPSILON]: 0.0100\n",
      "[STEP]: 19968, [LOSS]: 0.4756, [AVG_LOSS]: 0.2269, [EPISODE]: 360, [REWARD]: 111.0, [EPSILON]: 0.0100\n",
      "[STEP]: 20096, [LOSS]: 0.2705, [AVG_LOSS]: 0.2237, [EPISODE]: 361, [REWARD]: 67.0, [EPSILON]: 0.0100\n",
      "[STEP]: 20224, [LOSS]: 0.1691, [AVG_LOSS]: 0.1976, [EPISODE]: 362, [REWARD]: 68.0, [EPSILON]: 0.0100\n",
      "[STEP]: 20352, [LOSS]: 0.0109, [AVG_LOSS]: 0.2235, [EPISODE]: 363, [REWARD]: 64.0, [EPSILON]: 0.0100\n",
      "[STEP]: 20480, [LOSS]: 0.2525, [AVG_LOSS]: 0.1911, [EPISODE]: 364, [REWARD]: 28.0, [EPSILON]: 0.0100\n",
      "[STEP]: 20608, [LOSS]: 0.3181, [AVG_LOSS]: 0.1742, [EPISODE]: 364, [REWARD]: 156.0, [EPSILON]: 0.0100\n",
      "[STEP]: 20736, [LOSS]: 0.5783, [AVG_LOSS]: 0.2058, [EPISODE]: 365, [REWARD]: 126.0, [EPSILON]: 0.0100\n",
      "[STEP]: 20864, [LOSS]: 0.0161, [AVG_LOSS]: 0.1958, [EPISODE]: 366, [REWARD]: 115.0, [EPSILON]: 0.0100\n",
      "[STEP]: 20992, [LOSS]: 0.5877, [AVG_LOSS]: 0.1575, [EPISODE]: 367, [REWARD]: 95.0, [EPSILON]: 0.0100\n",
      "[STEP]: 21120, [LOSS]: 0.0597, [AVG_LOSS]: 0.1787, [EPISODE]: 368, [REWARD]: 66.0, [EPSILON]: 0.0100\n",
      "[STEP]: 21248, [LOSS]: 0.1111, [AVG_LOSS]: 0.1620, [EPISODE]: 369, [REWARD]: 66.0, [EPSILON]: 0.0100\n",
      "[STEP]: 21376, [LOSS]: 0.0230, [AVG_LOSS]: 0.1748, [EPISODE]: 370, [REWARD]: 64.0, [EPSILON]: 0.0100\n",
      "[STEP]: 21504, [LOSS]: 0.3453, [AVG_LOSS]: 0.1756, [EPISODE]: 371, [REWARD]: 72.0, [EPSILON]: 0.0100\n",
      "[STEP]: 21632, [LOSS]: 0.4775, [AVG_LOSS]: 0.1983, [EPISODE]: 372, [REWARD]: 75.0, [EPSILON]: 0.0100\n",
      "[STEP]: 21760, [LOSS]: 0.3677, [AVG_LOSS]: 0.1995, [EPISODE]: 373, [REWARD]: 67.0, [EPSILON]: 0.0100\n",
      "[STEP]: 21888, [LOSS]: 0.3284, [AVG_LOSS]: 0.2053, [EPISODE]: 374, [REWARD]: 56.0, [EPSILON]: 0.0100\n",
      "[STEP]: 22016, [LOSS]: 0.0774, [AVG_LOSS]: 0.1318, [EPISODE]: 375, [REWARD]: 49.0, [EPSILON]: 0.0100\n",
      "[STEP]: 22144, [LOSS]: 0.0221, [AVG_LOSS]: 0.1495, [EPISODE]: 376, [REWARD]: 41.0, [EPSILON]: 0.0100\n",
      "[STEP]: 22272, [LOSS]: 0.3306, [AVG_LOSS]: 0.1408, [EPISODE]: 377, [REWARD]: 32.0, [EPSILON]: 0.0100\n",
      "[STEP]: 22400, [LOSS]: 0.2714, [AVG_LOSS]: 0.1325, [EPISODE]: 378, [REWARD]: 20.0, [EPSILON]: 0.0100\n",
      "[STEP]: 22528, [LOSS]: 0.0177, [AVG_LOSS]: 0.1673, [EPISODE]: 379, [REWARD]: 14.0, [EPSILON]: 0.0100\n",
      "[STEP]: 22656, [LOSS]: 0.2611, [AVG_LOSS]: 0.1457, [EPISODE]: 379, [REWARD]: 142.0, [EPSILON]: 0.0100\n",
      "[STEP]: 22784, [LOSS]: 0.0276, [AVG_LOSS]: 0.1352, [EPISODE]: 380, [REWARD]: 74.0, [EPSILON]: 0.0100\n",
      "[STEP]: 22912, [LOSS]: 0.0295, [AVG_LOSS]: 0.1643, [EPISODE]: 381, [REWARD]: 36.0, [EPSILON]: 0.0100\n",
      "[STEP]: 23040, [LOSS]: 0.0174, [AVG_LOSS]: 0.1196, [EPISODE]: 381, [REWARD]: 164.0, [EPSILON]: 0.0100\n",
      "[STEP]: 23168, [LOSS]: 0.0468, [AVG_LOSS]: 0.1682, [EPISODE]: 382, [REWARD]: 56.0, [EPSILON]: 0.0100\n",
      "[STEP]: 23296, [LOSS]: 0.0278, [AVG_LOSS]: 0.1619, [EPISODE]: 382, [REWARD]: 184.0, [EPSILON]: 0.0100\n",
      "[STEP]: 23424, [LOSS]: 0.5189, [AVG_LOSS]: 0.1724, [EPISODE]: 382, [REWARD]: 312.0, [EPSILON]: 0.0100\n",
      "[STEP]: 23552, [LOSS]: 0.3601, [AVG_LOSS]: 0.1563, [EPISODE]: 382, [REWARD]: 440.0, [EPSILON]: 0.0100\n",
      "[STEP]: 23680, [LOSS]: 0.6393, [AVG_LOSS]: 0.1869, [EPISODE]: 383, [REWARD]: 68.0, [EPSILON]: 0.0100\n",
      "[STEP]: 23808, [LOSS]: 0.4709, [AVG_LOSS]: 0.1709, [EPISODE]: 383, [REWARD]: 196.0, [EPSILON]: 0.0100\n",
      "[STEP]: 23936, [LOSS]: 0.2178, [AVG_LOSS]: 0.1886, [EPISODE]: 383, [REWARD]: 324.0, [EPSILON]: 0.0100\n",
      "[STEP]: 24064, [LOSS]: 0.0170, [AVG_LOSS]: 0.1598, [EPISODE]: 383, [REWARD]: 452.0, [EPSILON]: 0.0100\n",
      "[STEP]: 24192, [LOSS]: 0.3890, [AVG_LOSS]: 0.1471, [EPISODE]: 384, [REWARD]: 80.0, [EPSILON]: 0.0100\n",
      "[STEP]: 24320, [LOSS]: 0.1042, [AVG_LOSS]: 0.1903, [EPISODE]: 384, [REWARD]: 208.0, [EPSILON]: 0.0100\n",
      "[STEP]: 24448, [LOSS]: 0.0471, [AVG_LOSS]: 0.1966, [EPISODE]: 384, [REWARD]: 336.0, [EPSILON]: 0.0100\n",
      "[STEP]: 24576, [LOSS]: 0.3426, [AVG_LOSS]: 0.1168, [EPISODE]: 385, [REWARD]: 91.0, [EPSILON]: 0.0100\n",
      "[STEP]: 24704, [LOSS]: 0.0279, [AVG_LOSS]: 0.2106, [EPISODE]: 385, [REWARD]: 219.0, [EPSILON]: 0.0100\n",
      "[STEP]: 24832, [LOSS]: 0.1625, [AVG_LOSS]: 0.1791, [EPISODE]: 386, [REWARD]: 112.0, [EPSILON]: 0.0100\n",
      "[STEP]: 24960, [LOSS]: 0.5219, [AVG_LOSS]: 0.1759, [EPISODE]: 387, [REWARD]: 101.0, [EPSILON]: 0.0100\n",
      "[STEP]: 25088, [LOSS]: 0.1940, [AVG_LOSS]: 0.1703, [EPISODE]: 388, [REWARD]: 50.0, [EPSILON]: 0.0100\n",
      "[STEP]: 25216, [LOSS]: 0.1245, [AVG_LOSS]: 0.1526, [EPISODE]: 389, [REWARD]: 31.0, [EPSILON]: 0.0100\n",
      "[STEP]: 25344, [LOSS]: 0.3677, [AVG_LOSS]: 0.1900, [EPISODE]: 390, [REWARD]: 15.0, [EPSILON]: 0.0100\n",
      "[STEP]: 25472, [LOSS]: 0.1874, [AVG_LOSS]: 0.1810, [EPISODE]: 391, [REWARD]: 4.0, [EPSILON]: 0.0100\n",
      "[STEP]: 25600, [LOSS]: 0.5074, [AVG_LOSS]: 0.1987, [EPISODE]: 391, [REWARD]: 132.0, [EPSILON]: 0.0100\n",
      "[STEP]: 25728, [LOSS]: 0.2233, [AVG_LOSS]: 0.1800, [EPISODE]: 391, [REWARD]: 260.0, [EPSILON]: 0.0100\n",
      "[STEP]: 25856, [LOSS]: 0.2096, [AVG_LOSS]: 0.1605, [EPISODE]: 392, [REWARD]: 57.0, [EPSILON]: 0.0100\n",
      "[STEP]: 25984, [LOSS]: 0.1974, [AVG_LOSS]: 0.1673, [EPISODE]: 393, [REWARD]: 33.0, [EPSILON]: 0.0100\n",
      "[STEP]: 26112, [LOSS]: 0.6868, [AVG_LOSS]: 0.1843, [EPISODE]: 393, [REWARD]: 161.0, [EPSILON]: 0.0100\n",
      "[STEP]: 26240, [LOSS]: 0.0319, [AVG_LOSS]: 0.1891, [EPISODE]: 393, [REWARD]: 289.0, [EPSILON]: 0.0100\n",
      "[STEP]: 26368, [LOSS]: 0.0148, [AVG_LOSS]: 0.1776, [EPISODE]: 394, [REWARD]: 18.0, [EPSILON]: 0.0100\n",
      "[STEP]: 26496, [LOSS]: 0.1524, [AVG_LOSS]: 0.1555, [EPISODE]: 394, [REWARD]: 146.0, [EPSILON]: 0.0100\n",
      "[STEP]: 26624, [LOSS]: 0.1459, [AVG_LOSS]: 0.1790, [EPISODE]: 395, [REWARD]: 4.0, [EPSILON]: 0.0100\n",
      "[STEP]: 26752, [LOSS]: 0.3114, [AVG_LOSS]: 0.1921, [EPISODE]: 395, [REWARD]: 132.0, [EPSILON]: 0.0100\n",
      "[STEP]: 26880, [LOSS]: 0.0269, [AVG_LOSS]: 0.2106, [EPISODE]: 396, [REWARD]: 8.0, [EPSILON]: 0.0100\n",
      "[STEP]: 27008, [LOSS]: 0.2614, [AVG_LOSS]: 0.1874, [EPISODE]: 396, [REWARD]: 136.0, [EPSILON]: 0.0100\n",
      "[STEP]: 27136, [LOSS]: 0.0276, [AVG_LOSS]: 0.1657, [EPISODE]: 397, [REWARD]: 48.0, [EPSILON]: 0.0100\n",
      "[STEP]: 27264, [LOSS]: 0.0180, [AVG_LOSS]: 0.1923, [EPISODE]: 397, [REWARD]: 176.0, [EPSILON]: 0.0100\n",
      "[STEP]: 27392, [LOSS]: 0.3064, [AVG_LOSS]: 0.1429, [EPISODE]: 398, [REWARD]: 85.0, [EPSILON]: 0.0100\n",
      "[STEP]: 27520, [LOSS]: 0.0241, [AVG_LOSS]: 0.1816, [EPISODE]: 398, [REWARD]: 213.0, [EPSILON]: 0.0100\n",
      "[STEP]: 27648, [LOSS]: 0.6895, [AVG_LOSS]: 0.1676, [EPISODE]: 399, [REWARD]: 123.0, [EPSILON]: 0.0100\n",
      "[STEP]: 27776, [LOSS]: 0.0243, [AVG_LOSS]: 0.1699, [EPISODE]: 400, [REWARD]: 5.0, [EPSILON]: 0.0100\n",
      "[STEP]: 27904, [LOSS]: 0.1287, [AVG_LOSS]: 0.2139, [EPISODE]: 400, [REWARD]: 133.0, [EPSILON]: 0.0100\n",
      "[STEP]: 28032, [LOSS]: 0.3045, [AVG_LOSS]: 0.1824, [EPISODE]: 401, [REWARD]: 3.0, [EPSILON]: 0.0100\n",
      "[STEP]: 28160, [LOSS]: 0.0286, [AVG_LOSS]: 0.1721, [EPISODE]: 401, [REWARD]: 131.0, [EPSILON]: 0.0100\n",
      "[STEP]: 28288, [LOSS]: 0.0191, [AVG_LOSS]: 0.1834, [EPISODE]: 402, [REWARD]: 7.0, [EPSILON]: 0.0100\n",
      "[STEP]: 28416, [LOSS]: 0.3885, [AVG_LOSS]: 0.1942, [EPISODE]: 402, [REWARD]: 135.0, [EPSILON]: 0.0100\n",
      "[STEP]: 28544, [LOSS]: 0.0186, [AVG_LOSS]: 0.1792, [EPISODE]: 402, [REWARD]: 263.0, [EPSILON]: 0.0100\n",
      "[STEP]: 28672, [LOSS]: 0.3287, [AVG_LOSS]: 0.2182, [EPISODE]: 403, [REWARD]: 120.0, [EPSILON]: 0.0100\n",
      "[STEP]: 28800, [LOSS]: 0.1245, [AVG_LOSS]: 0.1734, [EPISODE]: 403, [REWARD]: 248.0, [EPSILON]: 0.0100\n",
      "[STEP]: 28928, [LOSS]: 0.4319, [AVG_LOSS]: 0.1744, [EPISODE]: 404, [REWARD]: 48.0, [EPSILON]: 0.0100\n",
      "[STEP]: 29056, [LOSS]: 0.3936, [AVG_LOSS]: 0.1792, [EPISODE]: 404, [REWARD]: 176.0, [EPSILON]: 0.0100\n",
      "[STEP]: 29184, [LOSS]: 0.2870, [AVG_LOSS]: 0.1837, [EPISODE]: 404, [REWARD]: 304.0, [EPSILON]: 0.0100\n",
      "[STEP]: 29312, [LOSS]: 0.3995, [AVG_LOSS]: 0.1775, [EPISODE]: 404, [REWARD]: 432.0, [EPSILON]: 0.0100\n",
      "[STEP]: 29440, [LOSS]: 0.0175, [AVG_LOSS]: 0.1421, [EPISODE]: 405, [REWARD]: 92.0, [EPSILON]: 0.0100\n",
      "[STEP]: 29568, [LOSS]: 0.2660, [AVG_LOSS]: 0.1406, [EPISODE]: 405, [REWARD]: 220.0, [EPSILON]: 0.0100\n",
      "[STEP]: 29696, [LOSS]: 0.0128, [AVG_LOSS]: 0.1850, [EPISODE]: 405, [REWARD]: 348.0, [EPSILON]: 0.0100\n",
      "[STEP]: 29824, [LOSS]: 0.0131, [AVG_LOSS]: 0.1396, [EPISODE]: 405, [REWARD]: 476.0, [EPSILON]: 0.0100\n",
      "[STEP]: 29952, [LOSS]: 0.0166, [AVG_LOSS]: 0.1500, [EPISODE]: 406, [REWARD]: 104.0, [EPSILON]: 0.0100\n",
      "[STEP]: 30080, [LOSS]: 0.1124, [AVG_LOSS]: 0.1846, [EPISODE]: 406, [REWARD]: 232.0, [EPSILON]: 0.0100\n",
      "[STEP]: 30208, [LOSS]: 0.0210, [AVG_LOSS]: 0.1558, [EPISODE]: 406, [REWARD]: 360.0, [EPSILON]: 0.0100\n",
      "[STEP]: 30336, [LOSS]: 0.2916, [AVG_LOSS]: 0.1595, [EPISODE]: 406, [REWARD]: 488.0, [EPSILON]: 0.0100\n",
      "[STEP]: 30464, [LOSS]: 0.0259, [AVG_LOSS]: 0.1910, [EPISODE]: 407, [REWARD]: 116.0, [EPSILON]: 0.0100\n",
      "[STEP]: 30592, [LOSS]: 0.0323, [AVG_LOSS]: 0.1516, [EPISODE]: 407, [REWARD]: 244.0, [EPSILON]: 0.0100\n",
      "[STEP]: 30720, [LOSS]: 0.0096, [AVG_LOSS]: 0.2106, [EPISODE]: 407, [REWARD]: 372.0, [EPSILON]: 0.0100\n",
      "[STEP]: 30848, [LOSS]: 0.0175, [AVG_LOSS]: 0.1822, [EPISODE]: 407, [REWARD]: 500.0, [EPSILON]: 0.0100\n",
      "[STEP]: 30976, [LOSS]: 0.6764, [AVG_LOSS]: 0.1801, [EPISODE]: 408, [REWARD]: 128.0, [EPSILON]: 0.0100\n",
      "[STEP]: 31104, [LOSS]: 0.0125, [AVG_LOSS]: 0.1864, [EPISODE]: 408, [REWARD]: 256.0, [EPSILON]: 0.0100\n",
      "[STEP]: 31232, [LOSS]: 0.0136, [AVG_LOSS]: 0.1961, [EPISODE]: 408, [REWARD]: 384.0, [EPSILON]: 0.0100\n",
      "[STEP]: 31360, [LOSS]: 0.2302, [AVG_LOSS]: 0.1689, [EPISODE]: 409, [REWARD]: 12.0, [EPSILON]: 0.0100\n",
      "[STEP]: 31488, [LOSS]: 0.0122, [AVG_LOSS]: 0.1924, [EPISODE]: 409, [REWARD]: 140.0, [EPSILON]: 0.0100\n",
      "[STEP]: 31616, [LOSS]: 0.0114, [AVG_LOSS]: 0.2054, [EPISODE]: 409, [REWARD]: 268.0, [EPSILON]: 0.0100\n",
      "[STEP]: 31744, [LOSS]: 0.0091, [AVG_LOSS]: 0.1878, [EPISODE]: 409, [REWARD]: 396.0, [EPSILON]: 0.0100\n",
      "[STEP]: 31872, [LOSS]: 0.0079, [AVG_LOSS]: 0.1512, [EPISODE]: 410, [REWARD]: 24.0, [EPSILON]: 0.0100\n",
      "[STEP]: 32000, [LOSS]: 0.0144, [AVG_LOSS]: 0.1761, [EPISODE]: 410, [REWARD]: 152.0, [EPSILON]: 0.0100\n",
      "[STEP]: 32128, [LOSS]: 0.0109, [AVG_LOSS]: 0.2119, [EPISODE]: 410, [REWARD]: 280.0, [EPSILON]: 0.0100\n",
      "[STEP]: 32256, [LOSS]: 0.0120, [AVG_LOSS]: 0.1837, [EPISODE]: 410, [REWARD]: 408.0, [EPSILON]: 0.0100\n",
      "[STEP]: 32384, [LOSS]: 0.0115, [AVG_LOSS]: 0.1887, [EPISODE]: 411, [REWARD]: 36.0, [EPSILON]: 0.0100\n",
      "[STEP]: 32512, [LOSS]: 0.7425, [AVG_LOSS]: 0.1561, [EPISODE]: 411, [REWARD]: 164.0, [EPSILON]: 0.0100\n",
      "[STEP]: 32640, [LOSS]: 0.0100, [AVG_LOSS]: 0.2369, [EPISODE]: 411, [REWARD]: 292.0, [EPSILON]: 0.0100\n",
      "[STEP]: 32768, [LOSS]: 0.0126, [AVG_LOSS]: 0.1616, [EPISODE]: 411, [REWARD]: 420.0, [EPSILON]: 0.0100\n",
      "[STEP]: 32896, [LOSS]: 1.0546, [AVG_LOSS]: 0.2140, [EPISODE]: 412, [REWARD]: 48.0, [EPSILON]: 0.0100\n",
      "[STEP]: 33024, [LOSS]: 0.0103, [AVG_LOSS]: 0.1576, [EPISODE]: 412, [REWARD]: 176.0, [EPSILON]: 0.0100\n",
      "[STEP]: 33152, [LOSS]: 0.0072, [AVG_LOSS]: 0.1366, [EPISODE]: 412, [REWARD]: 304.0, [EPSILON]: 0.0100\n",
      "[STEP]: 33280, [LOSS]: 0.0081, [AVG_LOSS]: 0.2073, [EPISODE]: 412, [REWARD]: 432.0, [EPSILON]: 0.0100\n",
      "[STEP]: 33408, [LOSS]: 0.0104, [AVG_LOSS]: 0.2548, [EPISODE]: 413, [REWARD]: 60.0, [EPSILON]: 0.0100\n",
      "[STEP]: 33536, [LOSS]: 0.0080, [AVG_LOSS]: 0.1884, [EPISODE]: 413, [REWARD]: 188.0, [EPSILON]: 0.0100\n",
      "[STEP]: 33664, [LOSS]: 1.9639, [AVG_LOSS]: 0.1872, [EPISODE]: 413, [REWARD]: 316.0, [EPSILON]: 0.0100\n",
      "[STEP]: 33792, [LOSS]: 0.0195, [AVG_LOSS]: 0.2233, [EPISODE]: 413, [REWARD]: 444.0, [EPSILON]: 0.0100\n",
      "[STEP]: 33920, [LOSS]: 0.0096, [AVG_LOSS]: 0.2209, [EPISODE]: 414, [REWARD]: 72.0, [EPSILON]: 0.0100\n",
      "[STEP]: 34048, [LOSS]: 0.0071, [AVG_LOSS]: 0.1916, [EPISODE]: 414, [REWARD]: 200.0, [EPSILON]: 0.0100\n",
      "[STEP]: 34176, [LOSS]: 0.0098, [AVG_LOSS]: 0.1894, [EPISODE]: 414, [REWARD]: 328.0, [EPSILON]: 0.0100\n",
      "[STEP]: 34304, [LOSS]: 0.0064, [AVG_LOSS]: 0.2359, [EPISODE]: 414, [REWARD]: 456.0, [EPSILON]: 0.0100\n",
      "[STEP]: 34432, [LOSS]: 0.4780, [AVG_LOSS]: 0.1623, [EPISODE]: 415, [REWARD]: 84.0, [EPSILON]: 0.0100\n",
      "[STEP]: 34560, [LOSS]: 0.0074, [AVG_LOSS]: 0.2259, [EPISODE]: 415, [REWARD]: 212.0, [EPSILON]: 0.0100\n",
      "[STEP]: 34688, [LOSS]: 0.0062, [AVG_LOSS]: 0.2265, [EPISODE]: 415, [REWARD]: 340.0, [EPSILON]: 0.0100\n",
      "[STEP]: 34816, [LOSS]: 0.0112, [AVG_LOSS]: 0.2354, [EPISODE]: 415, [REWARD]: 468.0, [EPSILON]: 0.0100\n",
      "[STEP]: 34944, [LOSS]: 0.4873, [AVG_LOSS]: 0.1818, [EPISODE]: 416, [REWARD]: 96.0, [EPSILON]: 0.0100\n",
      "[STEP]: 35072, [LOSS]: 0.0062, [AVG_LOSS]: 0.1919, [EPISODE]: 416, [REWARD]: 224.0, [EPSILON]: 0.0100\n",
      "[STEP]: 35200, [LOSS]: 0.0073, [AVG_LOSS]: 0.2002, [EPISODE]: 416, [REWARD]: 352.0, [EPSILON]: 0.0100\n",
      "[STEP]: 35328, [LOSS]: 0.0078, [AVG_LOSS]: 0.2166, [EPISODE]: 416, [REWARD]: 480.0, [EPSILON]: 0.0100\n",
      "[STEP]: 35456, [LOSS]: 0.9720, [AVG_LOSS]: 0.2134, [EPISODE]: 418, [REWARD]: 54.0, [EPSILON]: 0.0100\n",
      "[STEP]: 35584, [LOSS]: 0.7505, [AVG_LOSS]: 0.1788, [EPISODE]: 418, [REWARD]: 182.0, [EPSILON]: 0.0100\n",
      "[STEP]: 35712, [LOSS]: 0.4910, [AVG_LOSS]: 0.2013, [EPISODE]: 418, [REWARD]: 310.0, [EPSILON]: 0.0100\n",
      "[STEP]: 35840, [LOSS]: 0.0048, [AVG_LOSS]: 0.2269, [EPISODE]: 418, [REWARD]: 438.0, [EPSILON]: 0.0100\n",
      "[STEP]: 35968, [LOSS]: 0.0058, [AVG_LOSS]: 0.2522, [EPISODE]: 419, [REWARD]: 66.0, [EPSILON]: 0.0100\n",
      "[STEP]: 36096, [LOSS]: 1.9862, [AVG_LOSS]: 0.2908, [EPISODE]: 419, [REWARD]: 194.0, [EPSILON]: 0.0100\n",
      "[STEP]: 36224, [LOSS]: 0.0285, [AVG_LOSS]: 0.2485, [EPISODE]: 419, [REWARD]: 322.0, [EPSILON]: 0.0100\n",
      "[STEP]: 36352, [LOSS]: 0.0070, [AVG_LOSS]: 0.2154, [EPISODE]: 420, [REWARD]: 99.0, [EPSILON]: 0.0100\n",
      "[STEP]: 36480, [LOSS]: 0.0065, [AVG_LOSS]: 0.1200, [EPISODE]: 420, [REWARD]: 227.0, [EPSILON]: 0.0100\n",
      "[STEP]: 36608, [LOSS]: 0.0330, [AVG_LOSS]: 0.2472, [EPISODE]: 420, [REWARD]: 355.0, [EPSILON]: 0.0100\n",
      "[STEP]: 36736, [LOSS]: 0.7634, [AVG_LOSS]: 0.2436, [EPISODE]: 420, [REWARD]: 483.0, [EPSILON]: 0.0100\n",
      "[STEP]: 36864, [LOSS]: 0.0045, [AVG_LOSS]: 0.2487, [EPISODE]: 421, [REWARD]: 111.0, [EPSILON]: 0.0100\n",
      "[STEP]: 36992, [LOSS]: 0.7580, [AVG_LOSS]: 0.1370, [EPISODE]: 421, [REWARD]: 239.0, [EPSILON]: 0.0100\n",
      "[STEP]: 37120, [LOSS]: 0.5019, [AVG_LOSS]: 0.1855, [EPISODE]: 422, [REWARD]: 14.0, [EPSILON]: 0.0100\n",
      "[STEP]: 37248, [LOSS]: 1.2623, [AVG_LOSS]: 0.1831, [EPISODE]: 422, [REWARD]: 142.0, [EPSILON]: 0.0100\n",
      "[STEP]: 37376, [LOSS]: 0.0099, [AVG_LOSS]: 0.2145, [EPISODE]: 422, [REWARD]: 270.0, [EPSILON]: 0.0100\n",
      "[STEP]: 37504, [LOSS]: 0.7571, [AVG_LOSS]: 0.1824, [EPISODE]: 422, [REWARD]: 398.0, [EPSILON]: 0.0100\n",
      "[STEP]: 37632, [LOSS]: 0.0044, [AVG_LOSS]: 0.2023, [EPISODE]: 423, [REWARD]: 26.0, [EPSILON]: 0.0100\n",
      "[STEP]: 37760, [LOSS]: 0.0041, [AVG_LOSS]: 0.2201, [EPISODE]: 423, [REWARD]: 154.0, [EPSILON]: 0.0100\n",
      "[STEP]: 37888, [LOSS]: 0.0056, [AVG_LOSS]: 0.1713, [EPISODE]: 423, [REWARD]: 282.0, [EPSILON]: 0.0100\n",
      "[STEP]: 38016, [LOSS]: 0.0096, [AVG_LOSS]: 0.1915, [EPISODE]: 423, [REWARD]: 410.0, [EPSILON]: 0.0100\n",
      "[STEP]: 38144, [LOSS]: 0.0060, [AVG_LOSS]: 0.2209, [EPISODE]: 424, [REWARD]: 38.0, [EPSILON]: 0.0100\n",
      "[STEP]: 38272, [LOSS]: 0.0064, [AVG_LOSS]: 0.2453, [EPISODE]: 427, [REWARD]: 22.0, [EPSILON]: 0.0100\n",
      "[STEP]: 38400, [LOSS]: 0.0124, [AVG_LOSS]: 0.2510, [EPISODE]: 429, [REWARD]: 35.0, [EPSILON]: 0.0100\n",
      "[STEP]: 38528, [LOSS]: 0.7619, [AVG_LOSS]: 0.2412, [EPISODE]: 432, [REWARD]: 39.0, [EPSILON]: 0.0100\n",
      "[STEP]: 38656, [LOSS]: 0.7735, [AVG_LOSS]: 0.2644, [EPISODE]: 435, [REWARD]: 31.0, [EPSILON]: 0.0100\n",
      "[STEP]: 38784, [LOSS]: 2.2665, [AVG_LOSS]: 0.2407, [EPISODE]: 436, [REWARD]: 113.0, [EPSILON]: 0.0100\n",
      "[STEP]: 38912, [LOSS]: 0.0051, [AVG_LOSS]: 0.2460, [EPISODE]: 436, [REWARD]: 241.0, [EPSILON]: 0.0100\n",
      "[STEP]: 39040, [LOSS]: 0.7686, [AVG_LOSS]: 0.3880, [EPISODE]: 436, [REWARD]: 369.0, [EPSILON]: 0.0100\n",
      "[STEP]: 39168, [LOSS]: 0.0071, [AVG_LOSS]: 0.2396, [EPISODE]: 436, [REWARD]: 497.0, [EPSILON]: 0.0100\n",
      "[STEP]: 39296, [LOSS]: 1.5328, [AVG_LOSS]: 0.2644, [EPISODE]: 437, [REWARD]: 125.0, [EPSILON]: 0.0100\n",
      "[STEP]: 39424, [LOSS]: 0.0079, [AVG_LOSS]: 0.2242, [EPISODE]: 437, [REWARD]: 253.0, [EPSILON]: 0.0100\n",
      "[STEP]: 39552, [LOSS]: 0.0065, [AVG_LOSS]: 0.2773, [EPISODE]: 437, [REWARD]: 381.0, [EPSILON]: 0.0100\n",
      "[STEP]: 39680, [LOSS]: 0.0077, [AVG_LOSS]: 0.3079, [EPISODE]: 438, [REWARD]: 9.0, [EPSILON]: 0.0100\n",
      "[STEP]: 39808, [LOSS]: 1.5033, [AVG_LOSS]: 0.3130, [EPISODE]: 438, [REWARD]: 137.0, [EPSILON]: 0.0100\n",
      "[STEP]: 39936, [LOSS]: 0.7783, [AVG_LOSS]: 0.3515, [EPISODE]: 438, [REWARD]: 265.0, [EPSILON]: 0.0100\n",
      "[STEP]: 40064, [LOSS]: 0.0086, [AVG_LOSS]: 0.3101, [EPISODE]: 438, [REWARD]: 393.0, [EPSILON]: 0.0100\n",
      "[STEP]: 40192, [LOSS]: 0.7807, [AVG_LOSS]: 0.2414, [EPISODE]: 439, [REWARD]: 21.0, [EPSILON]: 0.0100\n",
      "[STEP]: 40320, [LOSS]: 0.0066, [AVG_LOSS]: 0.2655, [EPISODE]: 439, [REWARD]: 149.0, [EPSILON]: 0.0100\n",
      "[STEP]: 40448, [LOSS]: 0.0053, [AVG_LOSS]: 0.3285, [EPISODE]: 439, [REWARD]: 277.0, [EPSILON]: 0.0100\n",
      "[STEP]: 40576, [LOSS]: 0.0055, [AVG_LOSS]: 0.2703, [EPISODE]: 439, [REWARD]: 405.0, [EPSILON]: 0.0100\n",
      "[STEP]: 40704, [LOSS]: 0.0108, [AVG_LOSS]: 0.3481, [EPISODE]: 440, [REWARD]: 33.0, [EPSILON]: 0.0100\n",
      "[STEP]: 40832, [LOSS]: 0.7755, [AVG_LOSS]: 0.2659, [EPISODE]: 440, [REWARD]: 161.0, [EPSILON]: 0.0100\n",
      "[STEP]: 40960, [LOSS]: 0.0066, [AVG_LOSS]: 0.2119, [EPISODE]: 440, [REWARD]: 289.0, [EPSILON]: 0.0100\n",
      "[STEP]: 41088, [LOSS]: 0.0051, [AVG_LOSS]: 0.3819, [EPISODE]: 440, [REWARD]: 417.0, [EPSILON]: 0.0100\n",
      "[STEP]: 41216, [LOSS]: 0.0084, [AVG_LOSS]: 0.3068, [EPISODE]: 441, [REWARD]: 45.0, [EPSILON]: 0.0100\n",
      "[STEP]: 41344, [LOSS]: 0.0079, [AVG_LOSS]: 0.3298, [EPISODE]: 441, [REWARD]: 173.0, [EPSILON]: 0.0100\n",
      "[STEP]: 41472, [LOSS]: 0.0071, [AVG_LOSS]: 0.2972, [EPISODE]: 441, [REWARD]: 301.0, [EPSILON]: 0.0100\n",
      "[STEP]: 41600, [LOSS]: 0.7783, [AVG_LOSS]: 0.3116, [EPISODE]: 441, [REWARD]: 429.0, [EPSILON]: 0.0100\n",
      "[STEP]: 41728, [LOSS]: 0.0073, [AVG_LOSS]: 0.3702, [EPISODE]: 442, [REWARD]: 57.0, [EPSILON]: 0.0100\n",
      "[STEP]: 41856, [LOSS]: 0.5532, [AVG_LOSS]: 0.3179, [EPISODE]: 442, [REWARD]: 185.0, [EPSILON]: 0.0100\n",
      "[STEP]: 41984, [LOSS]: 0.5055, [AVG_LOSS]: 0.3235, [EPISODE]: 442, [REWARD]: 313.0, [EPSILON]: 0.0100\n",
      "[STEP]: 42112, [LOSS]: 0.7633, [AVG_LOSS]: 0.2716, [EPISODE]: 442, [REWARD]: 441.0, [EPSILON]: 0.0100\n",
      "[STEP]: 42240, [LOSS]: 0.0052, [AVG_LOSS]: 0.2186, [EPISODE]: 443, [REWARD]: 69.0, [EPSILON]: 0.0100\n",
      "[STEP]: 42368, [LOSS]: 0.0066, [AVG_LOSS]: 0.3338, [EPISODE]: 443, [REWARD]: 197.0, [EPSILON]: 0.0100\n",
      "[STEP]: 42496, [LOSS]: 0.0078, [AVG_LOSS]: 0.2267, [EPISODE]: 443, [REWARD]: 325.0, [EPSILON]: 0.0100\n",
      "[STEP]: 42624, [LOSS]: 0.0059, [AVG_LOSS]: 0.3061, [EPISODE]: 443, [REWARD]: 453.0, [EPSILON]: 0.0100\n",
      "[STEP]: 42752, [LOSS]: 0.0141, [AVG_LOSS]: 0.2948, [EPISODE]: 444, [REWARD]: 81.0, [EPSILON]: 0.0100\n",
      "[STEP]: 42880, [LOSS]: 0.0087, [AVG_LOSS]: 0.3742, [EPISODE]: 444, [REWARD]: 209.0, [EPSILON]: 0.0100\n",
      "[STEP]: 43008, [LOSS]: 1.5467, [AVG_LOSS]: 0.2616, [EPISODE]: 444, [REWARD]: 337.0, [EPSILON]: 0.0100\n",
      "[STEP]: 43136, [LOSS]: 0.0057, [AVG_LOSS]: 0.2633, [EPISODE]: 444, [REWARD]: 465.0, [EPSILON]: 0.0100\n",
      "[STEP]: 43264, [LOSS]: 0.0029, [AVG_LOSS]: 0.2483, [EPISODE]: 445, [REWARD]: 93.0, [EPSILON]: 0.0100\n",
      "[STEP]: 43392, [LOSS]: 0.7688, [AVG_LOSS]: 0.3372, [EPISODE]: 445, [REWARD]: 221.0, [EPSILON]: 0.0100\n",
      "[STEP]: 43520, [LOSS]: 1.5584, [AVG_LOSS]: 0.3411, [EPISODE]: 445, [REWARD]: 349.0, [EPSILON]: 0.0100\n",
      "[STEP]: 43648, [LOSS]: 0.7867, [AVG_LOSS]: 0.3956, [EPISODE]: 445, [REWARD]: 477.0, [EPSILON]: 0.0100\n",
      "[STEP]: 43776, [LOSS]: 0.0213, [AVG_LOSS]: 0.3015, [EPISODE]: 446, [REWARD]: 105.0, [EPSILON]: 0.0100\n",
      "[STEP]: 43904, [LOSS]: 0.0061, [AVG_LOSS]: 0.3643, [EPISODE]: 446, [REWARD]: 233.0, [EPSILON]: 0.0100\n",
      "[STEP]: 44032, [LOSS]: 0.0046, [AVG_LOSS]: 0.2898, [EPISODE]: 446, [REWARD]: 361.0, [EPSILON]: 0.0100\n",
      "[STEP]: 44160, [LOSS]: 1.5255, [AVG_LOSS]: 0.3088, [EPISODE]: 446, [REWARD]: 489.0, [EPSILON]: 0.0100\n",
      "[STEP]: 44288, [LOSS]: 0.7679, [AVG_LOSS]: 0.2909, [EPISODE]: 447, [REWARD]: 117.0, [EPSILON]: 0.0100\n",
      "[STEP]: 44416, [LOSS]: 0.5883, [AVG_LOSS]: 0.3931, [EPISODE]: 447, [REWARD]: 245.0, [EPSILON]: 0.0100\n",
      "[STEP]: 44544, [LOSS]: 0.0092, [AVG_LOSS]: 0.2779, [EPISODE]: 447, [REWARD]: 373.0, [EPSILON]: 0.0100\n",
      "[STEP]: 44672, [LOSS]: 0.0048, [AVG_LOSS]: 0.2607, [EPISODE]: 448, [REWARD]: 1.0, [EPSILON]: 0.0100\n",
      "[STEP]: 44800, [LOSS]: 0.0086, [AVG_LOSS]: 0.2451, [EPISODE]: 448, [REWARD]: 129.0, [EPSILON]: 0.0100\n",
      "[STEP]: 44928, [LOSS]: 0.0102, [AVG_LOSS]: 0.2791, [EPISODE]: 448, [REWARD]: 257.0, [EPSILON]: 0.0100\n",
      "[STEP]: 45056, [LOSS]: 0.0092, [AVG_LOSS]: 0.3061, [EPISODE]: 448, [REWARD]: 385.0, [EPSILON]: 0.0100\n",
      "[STEP]: 45184, [LOSS]: 0.0056, [AVG_LOSS]: 0.2881, [EPISODE]: 449, [REWARD]: 13.0, [EPSILON]: 0.0100\n",
      "[STEP]: 45312, [LOSS]: 0.0089, [AVG_LOSS]: 0.2959, [EPISODE]: 449, [REWARD]: 141.0, [EPSILON]: 0.0100\n",
      "[STEP]: 45440, [LOSS]: 0.0052, [AVG_LOSS]: 0.3450, [EPISODE]: 449, [REWARD]: 269.0, [EPSILON]: 0.0100\n",
      "[STEP]: 45568, [LOSS]: 0.0049, [AVG_LOSS]: 0.2807, [EPISODE]: 449, [REWARD]: 397.0, [EPSILON]: 0.0100\n",
      "[STEP]: 45696, [LOSS]: 0.0055, [AVG_LOSS]: 0.2844, [EPISODE]: 450, [REWARD]: 25.0, [EPSILON]: 0.0100\n",
      "[STEP]: 45824, [LOSS]: 0.0071, [AVG_LOSS]: 0.1842, [EPISODE]: 450, [REWARD]: 153.0, [EPSILON]: 0.0100\n",
      "[STEP]: 45952, [LOSS]: 0.0049, [AVG_LOSS]: 0.2166, [EPISODE]: 450, [REWARD]: 281.0, [EPSILON]: 0.0100\n",
      "[STEP]: 46080, [LOSS]: 0.0070, [AVG_LOSS]: 0.2757, [EPISODE]: 450, [REWARD]: 409.0, [EPSILON]: 0.0100\n",
      "[STEP]: 46208, [LOSS]: 1.5514, [AVG_LOSS]: 0.3766, [EPISODE]: 451, [REWARD]: 37.0, [EPSILON]: 0.0100\n",
      "[STEP]: 46336, [LOSS]: 0.0061, [AVG_LOSS]: 0.3495, [EPISODE]: 451, [REWARD]: 165.0, [EPSILON]: 0.0100\n",
      "[STEP]: 46464, [LOSS]: 0.7712, [AVG_LOSS]: 0.3760, [EPISODE]: 451, [REWARD]: 293.0, [EPSILON]: 0.0100\n",
      "[STEP]: 46592, [LOSS]: 0.7885, [AVG_LOSS]: 0.3601, [EPISODE]: 451, [REWARD]: 421.0, [EPSILON]: 0.0100\n",
      "[STEP]: 46720, [LOSS]: 0.7890, [AVG_LOSS]: 0.3369, [EPISODE]: 452, [REWARD]: 49.0, [EPSILON]: 0.0100\n",
      "[STEP]: 46848, [LOSS]: 0.7931, [AVG_LOSS]: 0.2846, [EPISODE]: 452, [REWARD]: 177.0, [EPSILON]: 0.0100\n",
      "[STEP]: 46976, [LOSS]: 0.0026, [AVG_LOSS]: 0.2571, [EPISODE]: 452, [REWARD]: 305.0, [EPSILON]: 0.0100\n",
      "[STEP]: 47104, [LOSS]: 0.7876, [AVG_LOSS]: 0.3069, [EPISODE]: 452, [REWARD]: 433.0, [EPSILON]: 0.0100\n",
      "[STEP]: 47232, [LOSS]: 0.0083, [AVG_LOSS]: 0.3477, [EPISODE]: 453, [REWARD]: 61.0, [EPSILON]: 0.0100\n",
      "[STEP]: 47360, [LOSS]: 0.0042, [AVG_LOSS]: 0.3362, [EPISODE]: 453, [REWARD]: 189.0, [EPSILON]: 0.0100\n",
      "[STEP]: 47488, [LOSS]: 0.6590, [AVG_LOSS]: 0.4053, [EPISODE]: 453, [REWARD]: 317.0, [EPSILON]: 0.0100\n",
      "[STEP]: 47616, [LOSS]: 0.0107, [AVG_LOSS]: 0.3839, [EPISODE]: 453, [REWARD]: 445.0, [EPSILON]: 0.0100\n",
      "[STEP]: 47744, [LOSS]: 0.0037, [AVG_LOSS]: 0.2674, [EPISODE]: 454, [REWARD]: 73.0, [EPSILON]: 0.0100\n",
      "[STEP]: 47872, [LOSS]: 0.0031, [AVG_LOSS]: 0.2750, [EPISODE]: 454, [REWARD]: 201.0, [EPSILON]: 0.0100\n",
      "[STEP]: 48000, [LOSS]: 0.0053, [AVG_LOSS]: 0.3190, [EPISODE]: 454, [REWARD]: 329.0, [EPSILON]: 0.0100\n",
      "[STEP]: 48128, [LOSS]: 0.0047, [AVG_LOSS]: 0.2756, [EPISODE]: 454, [REWARD]: 457.0, [EPSILON]: 0.0100\n",
      "[STEP]: 48256, [LOSS]: 0.0032, [AVG_LOSS]: 0.2358, [EPISODE]: 455, [REWARD]: 85.0, [EPSILON]: 0.0100\n",
      "[STEP]: 48384, [LOSS]: 0.0071, [AVG_LOSS]: 0.2917, [EPISODE]: 455, [REWARD]: 213.0, [EPSILON]: 0.0100\n",
      "[STEP]: 48512, [LOSS]: 0.0039, [AVG_LOSS]: 0.1853, [EPISODE]: 455, [REWARD]: 341.0, [EPSILON]: 0.0100\n",
      "[STEP]: 48640, [LOSS]: 0.7880, [AVG_LOSS]: 0.2198, [EPISODE]: 455, [REWARD]: 469.0, [EPSILON]: 0.0100\n",
      "[STEP]: 48768, [LOSS]: 0.0021, [AVG_LOSS]: 0.2365, [EPISODE]: 456, [REWARD]: 97.0, [EPSILON]: 0.0100\n",
      "[STEP]: 48896, [LOSS]: 0.0053, [AVG_LOSS]: 0.1659, [EPISODE]: 456, [REWARD]: 225.0, [EPSILON]: 0.0100\n",
      "[STEP]: 49024, [LOSS]: 0.0020, [AVG_LOSS]: 0.2312, [EPISODE]: 456, [REWARD]: 353.0, [EPSILON]: 0.0100\n",
      "[STEP]: 49152, [LOSS]: 0.7904, [AVG_LOSS]: 0.1441, [EPISODE]: 456, [REWARD]: 481.0, [EPSILON]: 0.0100\n",
      "[STEP]: 49280, [LOSS]: 0.7862, [AVG_LOSS]: 0.3138, [EPISODE]: 457, [REWARD]: 109.0, [EPSILON]: 0.0100\n",
      "[STEP]: 49408, [LOSS]: 0.0051, [AVG_LOSS]: 0.1285, [EPISODE]: 457, [REWARD]: 237.0, [EPSILON]: 0.0100\n",
      "[STEP]: 49536, [LOSS]: 0.0016, [AVG_LOSS]: 0.2041, [EPISODE]: 457, [REWARD]: 365.0, [EPSILON]: 0.0100\n",
      "[STEP]: 49664, [LOSS]: 0.0079, [AVG_LOSS]: 0.1896, [EPISODE]: 457, [REWARD]: 493.0, [EPSILON]: 0.0100\n",
      "[STEP]: 49792, [LOSS]: 0.0170, [AVG_LOSS]: 0.1781, [EPISODE]: 458, [REWARD]: 121.0, [EPSILON]: 0.0100\n",
      "[STEP]: 49920, [LOSS]: 0.7896, [AVG_LOSS]: 0.1369, [EPISODE]: 458, [REWARD]: 249.0, [EPSILON]: 0.0100\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "episode = 0\n",
    "losses = []\n",
    "\n",
    "for step in range(N_TRAINING_STEPS):\n",
    "    action = select_action(\n",
    "        state=state,\n",
    "        q_net=q_net,\n",
    "        action_size=action_size,\n",
    "        epsilon=epsilon\n",
    "    )\n",
    "    \n",
    "    new_state, reward, terminated, truncated, info = env.step(action=action)\n",
    "    done = truncated or terminated\n",
    "    episode_reward += reward\n",
    "    buffer.add_sample((state, action, reward, new_state, done))\n",
    "    \n",
    "    if step % 100 == 0 and step < N_BUFFER_SIZE:\n",
    "        print(f\"Buffer filled so far: {len(buffer)}\")\n",
    "    \n",
    "    if step > N_START_LEARNING and len(buffer) > BATCH_SIZE:\n",
    "        if step % 10 == 0:\n",
    "            epsilon = max(MIN_EPSILON, epsilon * DECAY)\n",
    "        q_net.train()\n",
    "        target_q_net.eval()\n",
    "        batch = buffer.get_batch()\n",
    "        batch_len = len(batch)\n",
    "        states = torch.zeros(batch_len, state_size)\n",
    "        actions = torch.zeros(batch_len, 1, dtype=torch.int64)\n",
    "        rewards = torch.zeros(batch_len, 1)\n",
    "        next_states = torch.zeros(batch_len, state_size)\n",
    "        dones = torch.zeros(batch_len, 1)\n",
    "        \n",
    "        for idx, i in enumerate(batch):\n",
    "            states[idx] = torch.tensor(i[0], dtype=torch.float32)\n",
    "            actions[idx] = torch.tensor(i[1], dtype=torch.int64)\n",
    "            rewards[idx] = torch.tensor(i[2], dtype=torch.float32)\n",
    "            next_states[idx] = torch.tensor(i[3], dtype=torch.float32)\n",
    "            dones[idx] = torch.tensor(i[4], dtype=torch.float32)\n",
    "        \n",
    "        predicted_q_values = q_net(states)\n",
    "        extracted_q_values = predicted_q_values.gather(1, actions).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_q_net(next_states)\n",
    "            max_next_q = next_q_values.max(dim=1)[0]\n",
    "            \n",
    "        target = rewards.squeeze(1) + GAMMA * max_next_q * (1 - dones.squeeze(1))\n",
    "        \n",
    "        loss = F.smooth_l1_loss(extracted_q_values, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(q_net.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % TARGET_UPDATE_FREQUENCY == 0:\n",
    "            target_q_net.load_state_dict(q_net.state_dict())\n",
    "            avg_loss = np.mean(losses[-100:]) if len(losses) >= 100 else np.mean(losses)\n",
    "            print(f\"[STEP]: {step}, [LOSS]: {loss.item():.4f}, [AVG_LOSS]: {avg_loss:.4f}, [EPISODE]: {episode}, [REWARD]: {episode_reward}, [EPSILON]: {epsilon:.4f}\")\n",
    "            \n",
    "    if done:\n",
    "        state, _ = env.reset()\n",
    "        episode += 1\n",
    "        episode_reward = 0\n",
    "    else:\n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ad1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(q_net, video_folder=r\"B:\\Pytorch\\RL\\videos\", episodes=1, epsilon=0.0, env_name='CartPole-v1'):\n",
    "    env = gym.wrappers.RecordVideo(\n",
    "        gym.make(env_name, render_mode=\"rgb_array\"),\n",
    "        video_folder=video_folder,\n",
    "        name_prefix=\"cartpole_dqn\"\n",
    "    )\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done, truncated = False, False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                    q_values = q_net(state_tensor)\n",
    "                    action = q_values.argmax(dim=1).item()\n",
    "\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"[EPISODE {ep+1}] Reward: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Video saved in '{video_folder}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3922116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\Pytorch\\.venv\\lib\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at B:\\Pytorch\\RL\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video B:\\Pytorch\\RL\\videos\\cartpole_dqn-episode-0.mp4.\n",
      "MoviePy - Writing video B:\\Pytorch\\RL\\videos\\cartpole_dqn-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready B:\\Pytorch\\RL\\videos\\cartpole_dqn-episode-0.mp4\n",
      "[EPISODE 1] Reward: 500.0\n",
      "Video saved in 'B:\\Pytorch\\RL\\videos'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "record_video(q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d7af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict saved to B:\\Pytorch\\RL\\models\\dqn_cartpole.pth\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_PATH = r\"B:\\Pytorch\\RL\\models\\dqn_cartpole.pth\"\n",
    "torch.save(q_net.state_dict(), MODEL_FILE_PATH)\n",
    "print(f\"Model state_dict saved to {MODEL_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96097d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
