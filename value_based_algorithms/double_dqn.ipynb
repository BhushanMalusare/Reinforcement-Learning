{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d121ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, random, cv2, itertools\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from vizdoom import DoomGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61eb9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Then restart your kernel/notebook\n",
    "game = DoomGame()\n",
    "\n",
    "# Load configuration\n",
    "game.load_config(\"B:/Pytorch/RL/basic.cfg\")\n",
    "\n",
    "# Initialize the game\n",
    "game.init()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f683a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362e850e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Button.MOVE_LEFT: 11>, <Button.MOVE_RIGHT: 10>, <Button.ATTACK: 0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_available_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b5c7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment shape:  (3, 240, 320)\n"
     ]
    }
   ],
   "source": [
    "print(\"Environment shape: \", state.screen_buffer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82557a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions taken:  [<Button.MOVE_LEFT: 11>, <Button.MOVE_RIGHT: 10>, <Button.ATTACK: 0>]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions taken: \", game.get_available_buttons())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba236f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is episode finished: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Is episode finished:\", game.is_episode_finished())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3d9644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Button.MOVE_LEFT: 11>, <Button.MOVE_RIGHT: 10>, <Button.ATTACK: 0>]\n",
      "Number of buttons: 3\n"
     ]
    }
   ],
   "source": [
    "buttons = game.get_available_buttons()\n",
    "print(buttons)\n",
    "print(\"Number of buttons:\", len(buttons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4facf626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Button order in action vector:\n",
      "index 0: Button.MOVE_LEFT\n",
      "index 1: Button.MOVE_RIGHT\n",
      "index 2: Button.ATTACK\n",
      "\n",
      "Filtered action vectors:\n",
      "action index: 0 → [0, 0, 1] → active buttons: ['ATTACK']\n",
      "action index: 1 → [0, 1, 0] → active buttons: ['MOVE_RIGHT']\n",
      "action index: 2 → [0, 1, 1] → active buttons: ['MOVE_RIGHT', 'ATTACK']\n",
      "action index: 3 → [1, 0, 0] → active buttons: ['MOVE_LEFT']\n",
      "action index: 4 → [1, 0, 1] → active buttons: ['MOVE_LEFT', 'ATTACK']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_game_actions(game):\n",
    "    button_names = game.get_available_buttons()\n",
    "    num_buttons = len(button_names)\n",
    "    \n",
    "    # Generate all combinations\n",
    "    all_actions = [list(bits) for bits in itertools.product([0,1], repeat=num_buttons)]\n",
    "    \n",
    "    # Filter: remove do-nothing and invalid combos\n",
    "    filtered_actions = []\n",
    "    for vec in all_actions:\n",
    "        left = vec[0]\n",
    "        right = vec[1]\n",
    "        \n",
    "        # Remove:\n",
    "        # - do nothing\n",
    "        # - both move buttons together\n",
    "        # - all buttons pressed\n",
    "        if vec == [0,0,0]:\n",
    "            continue\n",
    "        if left and right:\n",
    "            continue\n",
    "        if sum(vec) == 3:\n",
    "            continue\n",
    "        \n",
    "        filtered_actions.append(vec)\n",
    "    \n",
    "    print(\"Button order in action vector:\")\n",
    "    for idx, btn in enumerate(button_names):\n",
    "        print(f\"index {idx}: {btn}\")\n",
    "        \n",
    "    print(\"\\nFiltered action vectors:\")\n",
    "    for idx, vec in enumerate(filtered_actions):\n",
    "        active = [btn.name for i, btn in enumerate(button_names) if vec[i]==1]\n",
    "        print(f\"action index: {idx} → {vec} → active buttons: {active}\")\n",
    "    \n",
    "    return filtered_actions\n",
    "\n",
    "game_actions = get_game_actions(game)\n",
    "game_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c88b926f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.make_action([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad30b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_buffer_size=10000, batch_size=16):\n",
    "        self.buffer = deque(maxlen=max_buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add_sample(self, element: tuple):\n",
    "        self.buffer.append(element)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        return random.sample(self.buffer, k=self.batch_size) if len(self.buffer) > self.batch_size else list(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513016e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, conv_features=64, num_actions=3, input_shape=(3, 120, 160)):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=conv_features, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv_features, out_channels=conv_features*2, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.flatten_size = self._get_flatten_size(input_shape)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flatten_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_actions)\n",
    "        \n",
    "    def _get_flatten_size(self, input_shape):\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *input_shape)\n",
    "            x = F.relu(self.conv1(dummy))\n",
    "            x = self.pool1(x)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = self.pool2(x)\n",
    "            flatten_size = x.numel()\n",
    "        return flatten_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11409aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=136192, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 0.95\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 5000\n",
    "EPSILON = 1.0\n",
    "DECAY = 0.995\n",
    "MIN_EPSILON = 0.01\n",
    "LEARNING_RATE = 1e-4\n",
    "TOTAL_NUM_STEPS = 20000\n",
    "N_START_TRAINING = 2000\n",
    "TARGET_UPDATE = 50\n",
    "LOG_LOSS_EVERY_N_STEPS = 100\n",
    "EPSILON_DECAY_RATE = 0.995\n",
    "MIN_EPSILON = 0.1\n",
    "\n",
    "\n",
    "q_net = QNetwork(num_actions=len(game_actions))\n",
    "target_q_net = QNetwork(num_actions=len(game_actions))\n",
    "buffer = ReplayBuffer(max_buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE)\n",
    "optimizer = optim.AdamW(q_net.parameters(), lr=LEARNING_RATE)\n",
    "target_q_net.load_state_dict(q_net.state_dict())\n",
    "q_net.to(device=device)\n",
    "target_q_net.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee782fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensored_state(game_state):\n",
    "    game_state = game_state.transpose(1, 2, 0)\n",
    "    resized_image = cv2.resize(game_state, (160, 120))\n",
    "    return (torch.from_numpy(resized_image).permute(2, 0, 1)).float().to(device) / 255.0\n",
    "\n",
    "def select_action(state_tensor, q_net, epsilon):\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, len(game_actions)-1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logits = q_net(state_tensor.unsqueeze(0))\n",
    "                action_idx = torch.argmax(logits, dim=-1).item()\n",
    "                return action_idx\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "                logits = q_net(state_tensor.unsqueeze(0))\n",
    "                action_idx = torch.argmax(logits, dim=-1).item()\n",
    "                return action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd69fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer filled so far: 1\n",
      "Buffer filled so far: 501\n",
      "Buffer filled so far: 1001\n",
      "Buffer filled so far: 1501\n",
      "[step]: 2100, [loss]: 2.6392, [epsilon]: 0.609\n",
      "[step]: 2200, [loss]: 0.9626, [epsilon]: 0.369\n",
      "[step]: 2300, [loss]: 0.8260, [epsilon]: 0.223\n",
      "[step]: 2400, [loss]: 3.0183, [epsilon]: 0.135\n",
      "[step]: 2500, [loss]: 0.1130, [epsilon]: 0.100\n",
      "[step]: 2600, [loss]: 0.1056, [epsilon]: 0.100\n",
      "[step]: 2700, [loss]: 0.4663, [epsilon]: 0.100\n",
      "[step]: 2800, [loss]: 0.9471, [epsilon]: 0.100\n",
      "[step]: 2900, [loss]: 0.2758, [epsilon]: 0.100\n",
      "[step]: 3000, [loss]: 0.3405, [epsilon]: 0.100\n",
      "[step]: 3100, [loss]: 15.1039, [epsilon]: 0.100\n",
      "[step]: 3200, [loss]: 0.3922, [epsilon]: 0.100\n",
      "[step]: 3300, [loss]: 0.3703, [epsilon]: 0.100\n",
      "[step]: 3400, [loss]: 0.6406, [epsilon]: 0.100\n",
      "[step]: 3500, [loss]: 0.4991, [epsilon]: 0.100\n",
      "[step]: 3600, [loss]: 0.7357, [epsilon]: 0.100\n",
      "[step]: 3700, [loss]: 0.4499, [epsilon]: 0.100\n",
      "[step]: 3800, [loss]: 705.7902, [epsilon]: 0.100\n",
      "[step]: 3900, [loss]: 0.3096, [epsilon]: 0.100\n",
      "[step]: 4000, [loss]: 0.7885, [epsilon]: 0.100\n",
      "[step]: 4100, [loss]: 0.4795, [epsilon]: 0.100\n",
      "[step]: 4200, [loss]: 0.6039, [epsilon]: 0.100\n",
      "[step]: 4300, [loss]: 1.4258, [epsilon]: 0.100\n",
      "[step]: 4400, [loss]: 1.1876, [epsilon]: 0.100\n",
      "[step]: 4500, [loss]: 0.5813, [epsilon]: 0.100\n",
      "[step]: 4600, [loss]: 1.0713, [epsilon]: 0.100\n",
      "[step]: 4700, [loss]: 0.7615, [epsilon]: 0.100\n",
      "[step]: 4800, [loss]: 682.5813, [epsilon]: 0.100\n",
      "[step]: 4900, [loss]: 0.9318, [epsilon]: 0.100\n",
      "[step]: 5000, [loss]: 0.7446, [epsilon]: 0.100\n",
      "[step]: 5100, [loss]: 1.3384, [epsilon]: 0.100\n",
      "[step]: 5200, [loss]: 1319.3777, [epsilon]: 0.100\n",
      "[step]: 5300, [loss]: 0.4701, [epsilon]: 0.100\n",
      "[step]: 5400, [loss]: 1.0535, [epsilon]: 0.100\n",
      "[step]: 5500, [loss]: 0.5905, [epsilon]: 0.100\n",
      "[step]: 5600, [loss]: 0.3844, [epsilon]: 0.100\n",
      "[step]: 5700, [loss]: 2.8829, [epsilon]: 0.100\n",
      "[step]: 5800, [loss]: 1.0150, [epsilon]: 0.100\n",
      "[step]: 5900, [loss]: 1.6784, [epsilon]: 0.100\n",
      "[step]: 6000, [loss]: 0.5454, [epsilon]: 0.100\n",
      "[step]: 6100, [loss]: 0.3849, [epsilon]: 0.100\n",
      "[step]: 6200, [loss]: 1.4570, [epsilon]: 0.100\n",
      "[step]: 6300, [loss]: 1.1610, [epsilon]: 0.100\n",
      "[step]: 6400, [loss]: 0.7679, [epsilon]: 0.100\n",
      "[step]: 6500, [loss]: 633.5033, [epsilon]: 0.100\n",
      "[step]: 6600, [loss]: 606.4930, [epsilon]: 0.100\n",
      "[step]: 6700, [loss]: 586.6900, [epsilon]: 0.100\n",
      "[step]: 6800, [loss]: 0.3695, [epsilon]: 0.100\n",
      "[step]: 6900, [loss]: 0.9175, [epsilon]: 0.100\n",
      "[step]: 7000, [loss]: 5.0606, [epsilon]: 0.100\n",
      "[step]: 7100, [loss]: 1.4829, [epsilon]: 0.100\n",
      "[step]: 7200, [loss]: 577.4432, [epsilon]: 0.100\n",
      "[step]: 7300, [loss]: 1.0155, [epsilon]: 0.100\n",
      "[step]: 7400, [loss]: 1.5314, [epsilon]: 0.100\n",
      "[step]: 7500, [loss]: 2.9570, [epsilon]: 0.100\n",
      "[step]: 7600, [loss]: 16.1967, [epsilon]: 0.100\n",
      "[step]: 7700, [loss]: 553.4031, [epsilon]: 0.100\n",
      "[step]: 7800, [loss]: 0.6492, [epsilon]: 0.100\n",
      "[step]: 7900, [loss]: 961.6364, [epsilon]: 0.100\n",
      "[step]: 8000, [loss]: 3.6242, [epsilon]: 0.100\n",
      "[step]: 8100, [loss]: 1.3417, [epsilon]: 0.100\n",
      "[step]: 8200, [loss]: 1.3777, [epsilon]: 0.100\n",
      "[step]: 8300, [loss]: 6.0186, [epsilon]: 0.100\n",
      "[step]: 8400, [loss]: 356.2848, [epsilon]: 0.100\n",
      "[step]: 8500, [loss]: 2.6848, [epsilon]: 0.100\n",
      "[step]: 8600, [loss]: 1.4760, [epsilon]: 0.100\n",
      "[step]: 8700, [loss]: 296.3379, [epsilon]: 0.100\n",
      "[step]: 8800, [loss]: 3.2971, [epsilon]: 0.100\n",
      "[step]: 8900, [loss]: 4.6607, [epsilon]: 0.100\n",
      "[step]: 9000, [loss]: 2.4883, [epsilon]: 0.100\n",
      "[step]: 9100, [loss]: 3.1705, [epsilon]: 0.100\n",
      "[step]: 9200, [loss]: 2.8405, [epsilon]: 0.100\n",
      "[step]: 9300, [loss]: 2.3163, [epsilon]: 0.100\n",
      "[step]: 9400, [loss]: 12.8321, [epsilon]: 0.100\n",
      "[step]: 9500, [loss]: 3.6513, [epsilon]: 0.100\n",
      "[step]: 9600, [loss]: 8.7564, [epsilon]: 0.100\n",
      "[step]: 9700, [loss]: 6.5303, [epsilon]: 0.100\n",
      "[step]: 9800, [loss]: 21.8854, [epsilon]: 0.100\n",
      "[step]: 9900, [loss]: 160.7722, [epsilon]: 0.100\n",
      "[step]: 10000, [loss]: 13.2215, [epsilon]: 0.100\n",
      "[step]: 10100, [loss]: 5.9113, [epsilon]: 0.100\n",
      "[step]: 10200, [loss]: 26.9999, [epsilon]: 0.100\n",
      "[step]: 10300, [loss]: 26.5793, [epsilon]: 0.100\n",
      "[step]: 10400, [loss]: 109.5221, [epsilon]: 0.100\n",
      "[step]: 10500, [loss]: 41.0169, [epsilon]: 0.100\n",
      "[step]: 10600, [loss]: 6.9487, [epsilon]: 0.100\n",
      "[step]: 10700, [loss]: 6.9828, [epsilon]: 0.100\n",
      "[step]: 10800, [loss]: 10.9123, [epsilon]: 0.100\n",
      "[step]: 10900, [loss]: 27.0488, [epsilon]: 0.100\n",
      "[step]: 11000, [loss]: 69.0422, [epsilon]: 0.100\n",
      "[step]: 11100, [loss]: 67.2966, [epsilon]: 0.100\n",
      "[step]: 11200, [loss]: 57.8678, [epsilon]: 0.100\n",
      "[step]: 11300, [loss]: 18.7915, [epsilon]: 0.100\n",
      "[step]: 11400, [loss]: 98.1729, [epsilon]: 0.100\n",
      "[step]: 11500, [loss]: 47.9353, [epsilon]: 0.100\n",
      "[step]: 11600, [loss]: 43.8002, [epsilon]: 0.100\n",
      "[step]: 11700, [loss]: 47.0197, [epsilon]: 0.100\n",
      "[step]: 11800, [loss]: 22.7543, [epsilon]: 0.100\n",
      "[step]: 11900, [loss]: 8.8404, [epsilon]: 0.100\n",
      "[step]: 12000, [loss]: 86.9365, [epsilon]: 0.100\n",
      "[step]: 12100, [loss]: 126.5566, [epsilon]: 0.100\n",
      "[step]: 12200, [loss]: 7.9935, [epsilon]: 0.100\n",
      "[step]: 12300, [loss]: 137.5808, [epsilon]: 0.100\n",
      "[step]: 12400, [loss]: 17.2340, [epsilon]: 0.100\n",
      "[step]: 12500, [loss]: 28.5303, [epsilon]: 0.100\n",
      "[step]: 12600, [loss]: 27.4932, [epsilon]: 0.100\n",
      "[step]: 12700, [loss]: 22.4596, [epsilon]: 0.100\n",
      "[step]: 12800, [loss]: 5.7384, [epsilon]: 0.100\n",
      "[step]: 12900, [loss]: 47.4114, [epsilon]: 0.100\n",
      "[step]: 13000, [loss]: 23.6542, [epsilon]: 0.100\n",
      "[step]: 13100, [loss]: 84.5940, [epsilon]: 0.100\n",
      "[step]: 13200, [loss]: 25.2480, [epsilon]: 0.100\n",
      "[step]: 13300, [loss]: 8.7017, [epsilon]: 0.100\n",
      "[step]: 13400, [loss]: 30.6578, [epsilon]: 0.100\n",
      "[step]: 13500, [loss]: 25.0919, [epsilon]: 0.100\n",
      "[step]: 13600, [loss]: 19.5184, [epsilon]: 0.100\n",
      "[step]: 13700, [loss]: 8.2486, [epsilon]: 0.100\n",
      "[step]: 13800, [loss]: 82.6664, [epsilon]: 0.100\n",
      "[step]: 13900, [loss]: 34.7589, [epsilon]: 0.100\n",
      "[step]: 14000, [loss]: 23.0045, [epsilon]: 0.100\n",
      "[step]: 14100, [loss]: 49.2217, [epsilon]: 0.100\n",
      "[step]: 14200, [loss]: 39.3902, [epsilon]: 0.100\n",
      "[step]: 14300, [loss]: 29.8553, [epsilon]: 0.100\n",
      "[step]: 14400, [loss]: 14.7841, [epsilon]: 0.100\n",
      "[step]: 14500, [loss]: 45.5725, [epsilon]: 0.100\n",
      "[step]: 14600, [loss]: 50.3811, [epsilon]: 0.100\n",
      "[step]: 14700, [loss]: 8.9444, [epsilon]: 0.100\n",
      "[step]: 14800, [loss]: 63.6907, [epsilon]: 0.100\n",
      "[step]: 14900, [loss]: 55.4412, [epsilon]: 0.100\n",
      "[step]: 15000, [loss]: 173.9994, [epsilon]: 0.100\n",
      "[step]: 15100, [loss]: 8.0917, [epsilon]: 0.100\n",
      "[step]: 15200, [loss]: 146.0777, [epsilon]: 0.100\n",
      "[step]: 15300, [loss]: 15.4719, [epsilon]: 0.100\n",
      "[step]: 15400, [loss]: 50.8463, [epsilon]: 0.100\n",
      "[step]: 15500, [loss]: 36.7478, [epsilon]: 0.100\n",
      "[step]: 15600, [loss]: 16.8495, [epsilon]: 0.100\n",
      "[step]: 15700, [loss]: 21.4334, [epsilon]: 0.100\n",
      "[step]: 15800, [loss]: 8.7122, [epsilon]: 0.100\n",
      "[step]: 15900, [loss]: 11.3038, [epsilon]: 0.100\n",
      "[step]: 16000, [loss]: 13.0875, [epsilon]: 0.100\n",
      "[step]: 16100, [loss]: 35.2113, [epsilon]: 0.100\n",
      "[step]: 16200, [loss]: 22.3369, [epsilon]: 0.100\n",
      "[step]: 16300, [loss]: 49.8852, [epsilon]: 0.100\n",
      "[step]: 16400, [loss]: 17.5568, [epsilon]: 0.100\n",
      "[step]: 16500, [loss]: 10.3006, [epsilon]: 0.100\n",
      "[step]: 16600, [loss]: 45.1665, [epsilon]: 0.100\n",
      "[step]: 16700, [loss]: 9.4641, [epsilon]: 0.100\n",
      "[step]: 16800, [loss]: 42.1202, [epsilon]: 0.100\n",
      "[step]: 16900, [loss]: 33.8790, [epsilon]: 0.100\n",
      "[step]: 17000, [loss]: 41.7295, [epsilon]: 0.100\n",
      "[step]: 17100, [loss]: 34.8135, [epsilon]: 0.100\n",
      "[step]: 17200, [loss]: 7.0217, [epsilon]: 0.100\n",
      "[step]: 17300, [loss]: 4.3210, [epsilon]: 0.100\n",
      "[step]: 17400, [loss]: 11.7609, [epsilon]: 0.100\n",
      "[step]: 17500, [loss]: 11.6503, [epsilon]: 0.100\n",
      "[step]: 17600, [loss]: 9.1070, [epsilon]: 0.100\n",
      "[step]: 17700, [loss]: 52.0765, [epsilon]: 0.100\n",
      "[step]: 17800, [loss]: 47.2475, [epsilon]: 0.100\n",
      "[step]: 17900, [loss]: 20.9787, [epsilon]: 0.100\n",
      "[step]: 18000, [loss]: 3.5444, [epsilon]: 0.100\n",
      "[step]: 18100, [loss]: 4.1514, [epsilon]: 0.100\n",
      "[step]: 18200, [loss]: 30.6545, [epsilon]: 0.100\n",
      "[step]: 18300, [loss]: 41.8585, [epsilon]: 0.100\n",
      "[step]: 18400, [loss]: 27.8820, [epsilon]: 0.100\n",
      "[step]: 18500, [loss]: 120.6204, [epsilon]: 0.100\n",
      "[step]: 18600, [loss]: 2.7890, [epsilon]: 0.100\n",
      "[step]: 18700, [loss]: 3.4106, [epsilon]: 0.100\n",
      "[step]: 18800, [loss]: 39.8844, [epsilon]: 0.100\n",
      "[step]: 18900, [loss]: 72.4860, [epsilon]: 0.100\n",
      "[step]: 19000, [loss]: 22.1511, [epsilon]: 0.100\n",
      "[step]: 19100, [loss]: 20.8555, [epsilon]: 0.100\n",
      "[step]: 19200, [loss]: 3.8609, [epsilon]: 0.100\n",
      "[step]: 19300, [loss]: 38.1730, [epsilon]: 0.100\n",
      "[step]: 19400, [loss]: 14.6281, [epsilon]: 0.100\n",
      "[step]: 19500, [loss]: 7.3878, [epsilon]: 0.100\n",
      "[step]: 19600, [loss]: 32.8741, [epsilon]: 0.100\n",
      "[step]: 19700, [loss]: 4.2139, [epsilon]: 0.100\n",
      "[step]: 19800, [loss]: 81.2651, [epsilon]: 0.100\n",
      "[step]: 19900, [loss]: 57.7264, [epsilon]: 0.100\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "game.new_episode()\n",
    "for step in range(TOTAL_NUM_STEPS):\n",
    "    # Get current state\n",
    "    state = game.get_state().screen_buffer\n",
    "    state_tensor = get_tensored_state(state)\n",
    "\n",
    "    # Select action (returns index)\n",
    "    action = select_action(state_tensor, q_net, EPSILON)\n",
    "    action_vector = game_actions[action]\n",
    "    \n",
    "    # Execute action\n",
    "    reward, done = game.make_action(action_vector), game.is_episode_finished()\n",
    "    did_attack = action_vector[2] == 1\n",
    "\n",
    "    # Penalize blind attack\n",
    "    if did_attack and reward == 0:\n",
    "        reward -= 10\n",
    "\n",
    "    # Get next state\n",
    "    if done:\n",
    "        next_state_tensor = torch.zeros_like(state_tensor)\n",
    "        game.new_episode()\n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state_tensor = get_tensored_state(next_state)\n",
    "\n",
    "    # Store in buffer as numpy / Python\n",
    "    buffer.add_sample((\n",
    "        state_tensor.cpu().numpy(),\n",
    "        action,          # int\n",
    "        reward,          # float\n",
    "        next_state_tensor.cpu().numpy(),\n",
    "        done             # bool\n",
    "    ))\n",
    "    \n",
    "    if step < N_START_TRAINING and step % 500 == 0:\n",
    "        print(f\"Buffer filled so far: {len(buffer)}\")\n",
    "\n",
    "    # Start training once buffer has enough samples\n",
    "    if step > N_START_TRAINING:\n",
    "        q_net.train()\n",
    "        target_q_net.eval()\n",
    "\n",
    "        # Sample minibatch\n",
    "        sampled_batches = buffer.get_batch()\n",
    "\n",
    "        # Convert to tensors on device\n",
    "        states = torch.from_numpy(np.stack([b[0] for b in sampled_batches])).float().to(device)\n",
    "        actions = torch.tensor([b[1] for b in sampled_batches], dtype=torch.int64, device=device)\n",
    "        rewards = torch.tensor([b[2] for b in sampled_batches], dtype=torch.float32, device=device)\n",
    "        next_states = torch.from_numpy(np.stack([b[3] for b in sampled_batches])).float().to(device)\n",
    "        dones = torch.tensor([b[4] for b in sampled_batches], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Double DQN target calculation\n",
    "        with torch.no_grad():\n",
    "            q_values_main = q_net(next_states)\n",
    "            next_actions = torch.argmax(q_values_main, dim=-1)\n",
    "            target_q_values = target_q_net(next_states)\n",
    "\n",
    "        target_q_values_max = target_q_values.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "        target = rewards + GAMMA * target_q_values_max * (~dones)\n",
    "\n",
    "        # Predicted Q-values for actions actually taken\n",
    "        predicted = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(predicted, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(q_net.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if step % LOG_LOSS_EVERY_N_STEPS == 0:\n",
    "            print(f\"[step]: {step}, [loss]: {loss.item():.4f}, [epsilon]: {EPSILON:.3f}\")\n",
    "\n",
    "        # Update target network\n",
    "        if step % TARGET_UPDATE == 0:\n",
    "            target_q_net.load_state_dict(q_net.state_dict())\n",
    "        \n",
    "        EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "568a24c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict saved to models/double_dqn.pth\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_PATH = \"models/double_dqn.pth\"\n",
    "torch.save(q_net.state_dict(), MODEL_FILE_PATH)\n",
    "print(f\"Model state_dict saved to {MODEL_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47cad7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 | Reward: 95.0 | Length: 6\n",
      "Episode 2/10 | Reward: 75.0 | Length: 26\n",
      "Episode 3/10 | Reward: 95.0 | Length: 6\n",
      "Episode 4/10 | Reward: 73.0 | Length: 28\n",
      "Episode 5/10 | Reward: 95.0 | Length: 6\n",
      "Episode 6/10 | Reward: 95.0 | Length: 6\n",
      "Episode 7/10 | Reward: 71.0 | Length: 30\n",
      "Episode 8/10 | Reward: 71.0 | Length: 25\n",
      "Episode 9/10 | Reward: 95.0 | Length: 6\n",
      "Episode 10/10 | Reward: 95.0 | Length: 6\n",
      "{'mean_reward': 86.0, 'std_reward': 11.072488428533127, 'min_reward': 71.0, 'max_reward': 95.0, 'mean_length': 14.5, 'std_length': 10.480935072788114, 'success_rate': 1.0, 'episode_rewards': [95.0, 75.0, 95.0, 73.0, 95.0, 95.0, 71.0, 71.0, 95.0, 95.0], 'episode_lengths': [6, 26, 6, 28, 6, 6, 30, 25, 6, 6]}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(q_net, num_episodes=10, eval_epsilon=0.0):\n",
    "    q_net.eval()\n",
    "\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    success_count = 0\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        game.init()\n",
    "        game.new_episode()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        ep_length = 0\n",
    "\n",
    "        while not done:\n",
    "            state = game.get_state().screen_buffer\n",
    "            state_tensor = get_tensored_state(state)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action_idx = select_action(state_tensor, q_net, epsilon=eval_epsilon)\n",
    "            action_vector = game_actions[action_idx]\n",
    "\n",
    "            # Execute action\n",
    "            reward, done = game.make_action(action_vector), game.is_episode_finished()\n",
    "\n",
    "            # Penalize blind attack\n",
    "            did_attack = action_vector[2] == 1\n",
    "            if did_attack and reward == 0:\n",
    "                reward -= 10\n",
    "\n",
    "            ep_reward += reward\n",
    "            ep_length += 1\n",
    "\n",
    "        print(f\"Episode {ep+1}/{num_episodes} | Reward: {ep_reward} | Length: {ep_length}\")\n",
    "        episode_rewards.append(ep_reward)\n",
    "        episode_lengths.append(ep_length)\n",
    "\n",
    "        if ep_reward > 0:\n",
    "            success_count += 1\n",
    "\n",
    "    game.close()\n",
    "\n",
    "    metrics = {\n",
    "        'mean_reward': np.mean(episode_rewards).item(),\n",
    "        'std_reward': np.std(episode_rewards).item(),\n",
    "        'min_reward': np.min(episode_rewards).item(),\n",
    "        'max_reward': np.max(episode_rewards).item(),\n",
    "        'mean_length': np.mean(episode_lengths).item(),\n",
    "        'std_length': np.std(episode_lengths).item(),\n",
    "        'success_rate': (success_count / num_episodes),\n",
    "        'episode_rewards': episode_rewards,    # keep as raw list\n",
    "        'episode_lengths': episode_lengths     # keep as raw list\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "metrics = evaluate_agent(q_net)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c57f856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Video saved as videos/doom_agent.mp4 🎥\n"
     ]
    }
   ],
   "source": [
    "def record_episode_video(q_net,filename=\"videos/doom_agent.mp4\", fps=20, max_steps=300, eval_epsilon=0.0):\n",
    "    q_net.eval()\n",
    "    frames = []\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "\n",
    "    game.init()\n",
    "    game.new_episode()\n",
    "    done = False\n",
    "\n",
    "    while not done and episode_length < max_steps:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state_tensor = get_tensored_state(state)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action_idx = select_action(state_tensor, q_net, epsilon=eval_epsilon)\n",
    "        action_vector = game_actions[action_idx]\n",
    "\n",
    "        reward, done = game.make_action(action_vector), game.is_episode_finished()\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        frame = state.transpose(1, 2, 0)   # (C,H,W) → (H,W,C)\n",
    "        frames.append(frame)\n",
    "\n",
    "    game.close()\n",
    "\n",
    "    # save video\n",
    "    if frames:\n",
    "        height, width, _ = frames[0].shape\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n",
    "\n",
    "        for f in frames:\n",
    "            video.write(cv2.cvtColor(f, cv2.COLOR_RGB2BGR))\n",
    "        video.release()\n",
    "\n",
    "        print(f\"✅ Video saved as {filename} 🎥\")\n",
    "    else:\n",
    "        print(\"⚠️ No frames captured, video not saved.\")\n",
    "\n",
    "    return episode_reward, episode_length\n",
    "\n",
    "episode_reward, episode_length = record_episode_video(q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613fbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
